{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Built around a core of OCI container packaging and Kubernetes container cluster management, OKD is also augmented by application lifecycle management functionality and DevOps tooling. OKD provides a complete open source container application platform. OKD 4 \u00b6 $ openshift-install create cluster Tons of amazing new features Automatic updates not only for OKD but also for the host OS, k8s Operators are first class citizens, a fancy UI, and much much more CodeReady Containers for OKD: local OKD 4 cluster for development CodeReady Containers brings a minimal OpenShift 4 cluster to your local laptop or desktop computer! Download it here: CodeReady Containers for OKD Images What is OKD? \u00b6 OKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment OKD embeds Kubernetes and extends it with security and other integrated concepts OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams OKD is also referred to as Origin in GitHub and in the documentation OKD is a sibling Kubernetes distribution to Red Hat OpenShift | If you are looking for enterprise-level support, or information on partner certification, Red Hat also offers Red Hat OpenShift Container Platform OKD Community \u00b6 We know you've got great ideas for improving OKD and its network of open source projects. So roll up your sleeves and come join us in the community! Get Started \u00b6 All contributions are welcome! OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the #openshift-dev on Kubernetes Slack Channel, or get involved in the OKD-WG by joining the OKD-WG google group. Get started with the Contributors Guide Read the documentation Read our charter Help Resolve an Open Issue Connect to the community \u00b6 Join the OKD Working Group Talk to Us \u00b6 Follow the public mailing lists Chat with us on the #openshift-users channel on Slack Standardization through Containerization \u00b6 Standards are powerful forces in the software industry. They can drive technology forward by bringing together the combined efforts of multiple developers, different communities, and even competing vendors. Open source container orchestration and cluster management at scale Standardized Linux container packaging for applications and their dependencies A container-focused OS that's designed for painless management in large clusters An open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator A lightweight container runtime for Kubernetes Prometheus is a systems and service monitoring toolkit that collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true OKD End User Community \u00b6 There is a large, vibrant end user community Become a part of something bigger \u00b6 OpenShift Commons is open to all community participants: users, operators, enterprises, non-profits, educational institutions, partners, and service providers as well as other open source technology initiatives utilized under the hood or to extend the OpenShift platform If you are an OpenShift Online or an OpenShift Container Platform customer or have deployed OKD on premise or on a public cloud If you have contributed to the OKD project and want to connect with your peers and end users If you simply want to stay up-to-date on the roadmap and best practices for using, deploying and operating OpenShift ... then OpenShift Commons is the right place for you","title":"Home"},{"location":"#okd-4","text":"$ openshift-install create cluster Tons of amazing new features Automatic updates not only for OKD but also for the host OS, k8s Operators are first class citizens, a fancy UI, and much much more CodeReady Containers for OKD: local OKD 4 cluster for development CodeReady Containers brings a minimal OpenShift 4 cluster to your local laptop or desktop computer! Download it here: CodeReady Containers for OKD Images","title":"OKD 4"},{"location":"#what-is-okd","text":"OKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment OKD embeds Kubernetes and extends it with security and other integrated concepts OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams OKD is also referred to as Origin in GitHub and in the documentation OKD is a sibling Kubernetes distribution to Red Hat OpenShift | If you are looking for enterprise-level support, or information on partner certification, Red Hat also offers Red Hat OpenShift Container Platform","title":"What is OKD?"},{"location":"#okd-community","text":"We know you've got great ideas for improving OKD and its network of open source projects. So roll up your sleeves and come join us in the community!","title":"OKD Community"},{"location":"#get-started","text":"All contributions are welcome! OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the #openshift-dev on Kubernetes Slack Channel, or get involved in the OKD-WG by joining the OKD-WG google group. Get started with the Contributors Guide Read the documentation Read our charter Help Resolve an Open Issue","title":"Get Started"},{"location":"#connect-to-the-community","text":"Join the OKD Working Group","title":"Connect to the community"},{"location":"#talk-to-us","text":"Follow the public mailing lists Chat with us on the #openshift-users channel on Slack","title":"Talk to Us"},{"location":"#standardization-through-containerization","text":"Standards are powerful forces in the software industry. They can drive technology forward by bringing together the combined efforts of multiple developers, different communities, and even competing vendors. Open source container orchestration and cluster management at scale Standardized Linux container packaging for applications and their dependencies A container-focused OS that's designed for painless management in large clusters An open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator A lightweight container runtime for Kubernetes Prometheus is a systems and service monitoring toolkit that collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true","title":"Standardization through Containerization"},{"location":"#okd-end-user-community","text":"There is a large, vibrant end user community","title":"OKD End User Community"},{"location":"#become-a-part-of-something-bigger","text":"OpenShift Commons is open to all community participants: users, operators, enterprises, non-profits, educational institutions, partners, and service providers as well as other open source technology initiatives utilized under the hood or to extend the OpenShift platform If you are an OpenShift Online or an OpenShift Container Platform customer or have deployed OKD on premise or on a public cloud If you have contributed to the OKD project and want to connect with your peers and end users If you simply want to stay up-to-date on the roadmap and best practices for using, deploying and operating OpenShift ... then OpenShift Commons is the right place for you","title":"Become a part of something bigger"},{"location":"about/","text":"About OKD \u00b6 OKD is the community distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams. OKD is also referred to as Origin in GitHub and in the documentation. OKD makes launching Kubernetes on any cloud or bare metal a snap, simplifies running and updating clusters, and provides all of the tools to make your containerized-applications succeed. Features \u00b6 A fully automated distribution of Kubernetes on all major clouds and bare metal, OpenStack, and other virtualization providers Easily build applications with integrated service discovery and persistent storage. Quickly and easily scale applications to handle periods of increased demand. Support for automatic high availability, load balancing, health checking, and failover. Access to the Operator Hub for extending Kubernetes with new, automated lifecycle capabilities Developer centric tooling and console for building containerized applications on Kubernetes Push source code to your Git repository and automatically deploy containerized applications. Web console and command-line client for building and monitoring applications. Centralized administration and management of an entire stack, team, or organization. Create reusable templates for components of your system, and iteratively deploy them over time. Roll out modifications to software stacks to your entire organization in a controlled fashion. Integration with your existing authentication mechanisms, including LDAP, Active Directory, and public OAuth providers such as GitHub. Multi-tenancy support, including team and user isolation of containers, builds, and network communication. Allow developers to run containers securely with fine-grained controls in production. Limit, track, and manage the developers and teams on the platform. Integrated container image registry, automatic edge load balancing, and full spectrum monitoring with Prometheus. What can I run on OKD? \u00b6 OKD is designed to run any Kubernetes workload. It also assists in building and developing containerized applications through the developer console. For an easier experience running your source code, Source-to-Image (S2I) allows developers to simply provide an application source repository containing code to build and run. It works by combining an existing S2I-enabled container image with application source to produce a new runnable image for your application. You can see the full list of Source-to-Image builder images and it's straightforward to create your own . Some of our available images include: Ruby Python Node.js PHP Perl WildFly MySQL MongoDB PostgreSQL MariaDB What sorts of security controls does OpenShift provide for containers? \u00b6 OKD runs with the following security policy by default: Containers run as a non-root unique user that is separate from other system users They cannot access host resources, run privileged, or become root They are given CPU and memory limits defined by the system administrator Any persistent storage they access will be under a unique SELinux label, which prevents others from seeing their content These settings are per project, so containers in different projects cannot see each other by default Regular users can run Docker, source, and custom builds By default, Docker builds can (and often do) run as root. You can control who can create Docker builds through the builds/docker and builds/custom policy resource. Regular users and project admins cannot change their security quotas. Many containers expect to run as root (and therefore edit all the contents of the filesystem). The Image Author's guide gives recommendations on making your image more secure by default: Don't run as root Make directories you want to write to group-writable and owned by group id 0 Set the net-bind capability on your executables if they need to bind to ports < 1024 If you are running your own cluster and want to run a container as root, you can grant that permission to the containers in your current project with the following command: # Gives the default service account in the current project access to run as UID 0 (root) oc adm add-scc-to-user anyuid -z default See the security documentation more on confining applications.","title":"About"},{"location":"about/#about-okd","text":"OKD is the community distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams. OKD is also referred to as Origin in GitHub and in the documentation. OKD makes launching Kubernetes on any cloud or bare metal a snap, simplifies running and updating clusters, and provides all of the tools to make your containerized-applications succeed.","title":"About OKD"},{"location":"about/#features","text":"A fully automated distribution of Kubernetes on all major clouds and bare metal, OpenStack, and other virtualization providers Easily build applications with integrated service discovery and persistent storage. Quickly and easily scale applications to handle periods of increased demand. Support for automatic high availability, load balancing, health checking, and failover. Access to the Operator Hub for extending Kubernetes with new, automated lifecycle capabilities Developer centric tooling and console for building containerized applications on Kubernetes Push source code to your Git repository and automatically deploy containerized applications. Web console and command-line client for building and monitoring applications. Centralized administration and management of an entire stack, team, or organization. Create reusable templates for components of your system, and iteratively deploy them over time. Roll out modifications to software stacks to your entire organization in a controlled fashion. Integration with your existing authentication mechanisms, including LDAP, Active Directory, and public OAuth providers such as GitHub. Multi-tenancy support, including team and user isolation of containers, builds, and network communication. Allow developers to run containers securely with fine-grained controls in production. Limit, track, and manage the developers and teams on the platform. Integrated container image registry, automatic edge load balancing, and full spectrum monitoring with Prometheus.","title":"Features"},{"location":"about/#what-can-i-run-on-okd","text":"OKD is designed to run any Kubernetes workload. It also assists in building and developing containerized applications through the developer console. For an easier experience running your source code, Source-to-Image (S2I) allows developers to simply provide an application source repository containing code to build and run. It works by combining an existing S2I-enabled container image with application source to produce a new runnable image for your application. You can see the full list of Source-to-Image builder images and it's straightforward to create your own . Some of our available images include: Ruby Python Node.js PHP Perl WildFly MySQL MongoDB PostgreSQL MariaDB","title":"What can I run on OKD?"},{"location":"about/#what-sorts-of-security-controls-does-openshift-provide-for-containers","text":"OKD runs with the following security policy by default: Containers run as a non-root unique user that is separate from other system users They cannot access host resources, run privileged, or become root They are given CPU and memory limits defined by the system administrator Any persistent storage they access will be under a unique SELinux label, which prevents others from seeing their content These settings are per project, so containers in different projects cannot see each other by default Regular users can run Docker, source, and custom builds By default, Docker builds can (and often do) run as root. You can control who can create Docker builds through the builds/docker and builds/custom policy resource. Regular users and project admins cannot change their security quotas. Many containers expect to run as root (and therefore edit all the contents of the filesystem). The Image Author's guide gives recommendations on making your image more secure by default: Don't run as root Make directories you want to write to group-writable and owned by group id 0 Set the net-bind capability on your executables if they need to bind to ports < 1024 If you are running your own cluster and want to run a container as root, you can grant that permission to the containers in your current project with the following command: # Gives the default service account in the current project access to run as UID 0 (root) oc adm add-scc-to-user anyuid -z default See the security documentation more on confining applications.","title":"What sorts of security controls does OpenShift provide for containers?"},{"location":"blog/","text":"okd.io Blog \u00b6 We look forward to sharing news and useful information about OKD in this blog. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io. Blogs \u00b6 2021 \u00b6 Date Title 2021-05-06 OKD Working Group Office Hours at KubeconEU on OpenShift.tv 2021-05-04 Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift 2021-03-22 Recap OKD Testing and Deployment Workshop - Videos and Additional Resources 2021-04-19 Please avoid using FCOS 33.20210301.3.1 for new OKD installs 2021-03-16 Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now! 2021-03-07 okd.io now has a blog","title":"Blog"},{"location":"blog/#okdio-blog","text":"We look forward to sharing news and useful information about OKD in this blog. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io.","title":"okd.io Blog"},{"location":"blog/#blogs","text":"","title":"Blogs"},{"location":"blog/#2021","text":"Date Title 2021-05-06 OKD Working Group Office Hours at KubeconEU on OpenShift.tv 2021-05-04 Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift 2021-03-22 Recap OKD Testing and Deployment Workshop - Videos and Additional Resources 2021-04-19 Please avoid using FCOS 33.20210301.3.1 for new OKD installs 2021-03-16 Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now! 2021-03-07 okd.io now has a blog","title":"2021"},{"location":"charter/","text":"OKD Working Group Charter \u00b6 v1.1 2019-09-21 Introduction \u00b6 The charter describes the operations of the OKD Working Group (OKD WG). OKD is the Origin Community Distribution of Kubernetes that is upstream to Red Hat\u2019s OpenShift Container Platform. It is built around a core of OCI containers and Kubernetes container cluster management. OKD is augmented by application lifecycle management functionality and DevOps tooling. The OKD Working Group's purpose is to discuss, give guidance to, and enable collaboration on current development efforts for OKD, Kubernetes, and related CNCF projects. The OKD Working Group will also include the discussion of shared community goals for OKD 4 and beyond. Additionally, the Working Group will produce supporting materials and best practices for end-users and will provide guidance and coordination for CNCF projects working within the SIG's scope. The OKD Working Group is independent of both Fedora and the Cloud Native Computing Foundation (CNCF). The OKD Working Group is a community sponsored by Red Hat. Mission \u00b6 The mission of the OKD Working Group is: To collaborate on areas related to developing, deploying, managing, and operating OKD. To develop informational resources like guides, tutorials, and white papers to give the community an understanding of best practices, trade-offs, and value adds as it relates to developing, deploying, and managing applications in cloud native environments. To identify suitable projects and gaps in the ecosystem for future collaboration and coordination with those projects. Areas considered in Scope \u00b6 The OKD Working Group focuses on the following end-user related topics of the lifecycle of cloud-native applications: Installation of OKD Integration with Fedora CoreOS. Other Linux distributions will be gated based on community interests and resource commitments and the establishment of Sub-Working Groups OKD delivery and release management OKD management and operations The Working Group will work on developing best practices, fostering collaboration between related projects, and working on improving tool interoperability. Additionally, the Working Group will propose new initiatives and projects when capability gaps in the current ecosystem are defined. The following, non-exhaustive, sample list of activities and deliverables are in-scope for the Working Group: Operator Centric Installer Definition and Documentation of requirements for running on Linux Identification of requirements Submission of code to upstream sources Development of Continuous Integration Education material to help provide guidance for the community Might include a summary of projects available in the community Definitions of implementations and patterns for best practices for delivering OKD Best practices for OKD management Areas considered out of Scope \u00b6 Anything not explicitly considered in the scope above. Example include: Development of any Linux distribution or variant Initiation of new open source projects Definition of standards for components like container images other infrastructure-level building blocks of cloud-native applications Governance \u00b6 Operations \u00b6 The OKD Working Group is run and managed by the following chairs: Red Hat Community Development Liaison - Diane Mueller (Red Hat) External to Red Hat - Jaime Magiera (UMich) Red Hat Engineering Liaison - Vadim Rutkovsky (Red Hat) Fedora CoreOS Engineering Liaison - Timoth\u00e9e Ravier (Red Hat) Note The referenced names and chair positions will be edited in-place as chairs are added, removed, or replaced. See the roles of chairs section for more information. A dedicated git repository will be the authoritative archive for membership list, code, documentation, and decisions made. The repository, along with this charter, will be hosted at github.com/openshift/community. The mailing list at groups.google.com/forum/#!forum/okd-wg will be used as a place to call for and publish group decisions, and to hold discussions in general. Working Group Membership \u00b6 All active members of the Working Group are listed in the MEMBERS.md file with their name. New members can apply for membership by creating an Issue or Pull Request on the repository on GitHub indicating their desire to join. Membership can be surrendered by creating an Issue stating this desire, or by creating a Pull Request to remove the own name from the members list. Decision Process \u00b6 This group will seek consensus decisions. After public discussion and consideration of different opinions, the Chair and/or Co-Chair will record a decision and summarize any objections. All WG members who have joined the GitHub group at least 21 days prior to the vote are eligible to vote. This is to prevent people from rallying outside supporters for their desired outcome. When the group comes to a decision in a meeting, the decision is tentative. Any group participant may object to a decision reached at a meeting within 7 days of publication of the decision on the GitHub Issue and/or mailing list. That decision must then be confirmed on the GitHub Issue via a Call for Agreement. The Call for Agreement, when a decision is required, will be posted as a GitHub Issue or Pull Request and must be announced on the mailing list. It is an instrument to reach a time-bounded lazy consensus approval and requires a voting period of no less than 7 days to be defined (including a specific date and time in UTC). Each Call for Agreement will be considered independently, except for elections of Chairs. The Chairs will submit all Calls for Agreement that are not vague, unprofessional, off-topic, or lacking sufficient detail to determine what is being agreed. In the event that a Call for Agreement falls under the delegated authority or within a chartered Sub-Working Group, the Call for Agreement must be passed through the Sub-Working Group before receiving Working Group consideration. A Call for Agreement may require quorum of Chairs under the circumstances outlined in the Charter and Governing Documents section. A Call for Agreement is mandatory when: A Chair determines that the topic requires a Call for Agreement. When petitioned by members of the Working Group and submitted to the Chairs to call a vote unless rejected for cause. Technical decisions that add, remove, or change dependencies and requirements. Revoke a previous decision made by the Call for Agreement process. Approval of a Sub-Working Group when such SWG has any delegated authority. Once the Call for Agreement voting period has elapsed, all votes are counted, with at least a 51% majority of votes needed for consensus. A Chair will then declare the agreement \u201caccepted\u201d or \u201cdeclined\u201d, ending the Call for Agreement. Once rejected, a Call for Agreement must be revised before re-submission for a subsequent vote. All rejected Calls for Agreement will be reported to the Working Group as rejected. Charter and Governing Documents \u00b6 The Working Group may, from time to time, adopt or amend its Governing Documents and Charter, using a modified Call for Agreement process: A quorum of at least 51% of active Working Group Members must vote. A quorum of 3 Chairs is needed Two-thirds of the voting quorum must approve the proposal. A majority of Chairs must approve the proposal. A public notice period of no less than 14 days from the Call for Agreement proposing the change must elapse before voting begins. Any Call for Agreement that follows this process is considered a Governing Document. For initial approval of this Charter via Call for Agreement all members are eligible to vote, even those that have been a member for less than 21 days. This Charter will be approved if there is a majority of positive votes. Organizational Roles \u00b6 Role of Chairs \u00b6 The primary role of Chairs is to run operations and the governance of the group. The Chairs are responsible for: Setting the agenda for meetings. Extending discussion via asynchronous communication to be inclusive of members who cannot attend a specific meeting time. Scheduling discussing of proposals that have been submitted. Asking for new proposals to be made to address an identified need. Oversee disciplinary action for members. The Chairs have the authority to declare a member inactive and expel members for cause. Chairs will serve for one-year revolving terms and will be approved using the Condorcet Method . Upon the expiration of a Chair\u2019s term, it will continue for another year, unless challenged. The terms for founding Chairs start on the approval of this charter. When no candidate has submitted their name for consideration, the current Chairs may appoint an acting Chair until a candidate comes forward. Chairs must be active members. Any inactivity, disability, or ineligibility results in immediate removal. Chairs may be removed by petition to the Working Group through the Call for Agreement process outlined above. Additional Chairs may be added so long as the existing number of Chairs is odd. These Chairs are added using a Call for Agreement. Extra Chairs enjoy the same rights, responsibilities, and obligations of a Chartered Chair. Upon vacancy of an Extra Chair, it may be filled by appointment by the remaining Chairs, or a majority vote of the Working Group until the term naturally expires. In the event that an even number of Chairs exist and vote situation arises, the Chairs will randomly select one chair to abstain. Role of Sub-Working Groups \u00b6 Each Sub-Working Group (SWG) must have a Chair working as an active sponsor. Under the mandate of the Working Group, each SWG will have the autonomy to establish their own charter, membership rules, meeting times, and management processes. Each SWG will also have the authority to make in-scope decisions as delegated by the Working Group. SWGs are required to submit their agreed Charter to the Working Group for information and archival. The Chairs can petition for dissolution of an inactive or hostile SWG by Call for Agreement. Once dissolved the SWG\u2019s delegated Charter and outstanding authority to make decisions is immediately revoked. The Chairs may then take any required action to restrict access to Working Group Resources. No SWG will have authority with regards to this Charter or other OKD Working Group Governing Documents.","title":"Charter"},{"location":"charter/#okd-working-group-charter","text":"v1.1 2019-09-21","title":"OKD Working Group Charter"},{"location":"charter/#introduction","text":"The charter describes the operations of the OKD Working Group (OKD WG). OKD is the Origin Community Distribution of Kubernetes that is upstream to Red Hat\u2019s OpenShift Container Platform. It is built around a core of OCI containers and Kubernetes container cluster management. OKD is augmented by application lifecycle management functionality and DevOps tooling. The OKD Working Group's purpose is to discuss, give guidance to, and enable collaboration on current development efforts for OKD, Kubernetes, and related CNCF projects. The OKD Working Group will also include the discussion of shared community goals for OKD 4 and beyond. Additionally, the Working Group will produce supporting materials and best practices for end-users and will provide guidance and coordination for CNCF projects working within the SIG's scope. The OKD Working Group is independent of both Fedora and the Cloud Native Computing Foundation (CNCF). The OKD Working Group is a community sponsored by Red Hat.","title":"Introduction"},{"location":"charter/#mission","text":"The mission of the OKD Working Group is: To collaborate on areas related to developing, deploying, managing, and operating OKD. To develop informational resources like guides, tutorials, and white papers to give the community an understanding of best practices, trade-offs, and value adds as it relates to developing, deploying, and managing applications in cloud native environments. To identify suitable projects and gaps in the ecosystem for future collaboration and coordination with those projects.","title":"Mission"},{"location":"charter/#areas-considered-in-scope","text":"The OKD Working Group focuses on the following end-user related topics of the lifecycle of cloud-native applications: Installation of OKD Integration with Fedora CoreOS. Other Linux distributions will be gated based on community interests and resource commitments and the establishment of Sub-Working Groups OKD delivery and release management OKD management and operations The Working Group will work on developing best practices, fostering collaboration between related projects, and working on improving tool interoperability. Additionally, the Working Group will propose new initiatives and projects when capability gaps in the current ecosystem are defined. The following, non-exhaustive, sample list of activities and deliverables are in-scope for the Working Group: Operator Centric Installer Definition and Documentation of requirements for running on Linux Identification of requirements Submission of code to upstream sources Development of Continuous Integration Education material to help provide guidance for the community Might include a summary of projects available in the community Definitions of implementations and patterns for best practices for delivering OKD Best practices for OKD management","title":"Areas considered in Scope"},{"location":"charter/#areas-considered-out-of-scope","text":"Anything not explicitly considered in the scope above. Example include: Development of any Linux distribution or variant Initiation of new open source projects Definition of standards for components like container images other infrastructure-level building blocks of cloud-native applications","title":"Areas considered out of Scope"},{"location":"charter/#governance","text":"","title":"Governance"},{"location":"charter/#operations","text":"The OKD Working Group is run and managed by the following chairs: Red Hat Community Development Liaison - Diane Mueller (Red Hat) External to Red Hat - Jaime Magiera (UMich) Red Hat Engineering Liaison - Vadim Rutkovsky (Red Hat) Fedora CoreOS Engineering Liaison - Timoth\u00e9e Ravier (Red Hat) Note The referenced names and chair positions will be edited in-place as chairs are added, removed, or replaced. See the roles of chairs section for more information. A dedicated git repository will be the authoritative archive for membership list, code, documentation, and decisions made. The repository, along with this charter, will be hosted at github.com/openshift/community. The mailing list at groups.google.com/forum/#!forum/okd-wg will be used as a place to call for and publish group decisions, and to hold discussions in general.","title":"Operations"},{"location":"charter/#working-group-membership","text":"All active members of the Working Group are listed in the MEMBERS.md file with their name. New members can apply for membership by creating an Issue or Pull Request on the repository on GitHub indicating their desire to join. Membership can be surrendered by creating an Issue stating this desire, or by creating a Pull Request to remove the own name from the members list.","title":"Working Group Membership"},{"location":"charter/#decision-process","text":"This group will seek consensus decisions. After public discussion and consideration of different opinions, the Chair and/or Co-Chair will record a decision and summarize any objections. All WG members who have joined the GitHub group at least 21 days prior to the vote are eligible to vote. This is to prevent people from rallying outside supporters for their desired outcome. When the group comes to a decision in a meeting, the decision is tentative. Any group participant may object to a decision reached at a meeting within 7 days of publication of the decision on the GitHub Issue and/or mailing list. That decision must then be confirmed on the GitHub Issue via a Call for Agreement. The Call for Agreement, when a decision is required, will be posted as a GitHub Issue or Pull Request and must be announced on the mailing list. It is an instrument to reach a time-bounded lazy consensus approval and requires a voting period of no less than 7 days to be defined (including a specific date and time in UTC). Each Call for Agreement will be considered independently, except for elections of Chairs. The Chairs will submit all Calls for Agreement that are not vague, unprofessional, off-topic, or lacking sufficient detail to determine what is being agreed. In the event that a Call for Agreement falls under the delegated authority or within a chartered Sub-Working Group, the Call for Agreement must be passed through the Sub-Working Group before receiving Working Group consideration. A Call for Agreement may require quorum of Chairs under the circumstances outlined in the Charter and Governing Documents section. A Call for Agreement is mandatory when: A Chair determines that the topic requires a Call for Agreement. When petitioned by members of the Working Group and submitted to the Chairs to call a vote unless rejected for cause. Technical decisions that add, remove, or change dependencies and requirements. Revoke a previous decision made by the Call for Agreement process. Approval of a Sub-Working Group when such SWG has any delegated authority. Once the Call for Agreement voting period has elapsed, all votes are counted, with at least a 51% majority of votes needed for consensus. A Chair will then declare the agreement \u201caccepted\u201d or \u201cdeclined\u201d, ending the Call for Agreement. Once rejected, a Call for Agreement must be revised before re-submission for a subsequent vote. All rejected Calls for Agreement will be reported to the Working Group as rejected.","title":"Decision Process"},{"location":"charter/#charter-and-governing-documents","text":"The Working Group may, from time to time, adopt or amend its Governing Documents and Charter, using a modified Call for Agreement process: A quorum of at least 51% of active Working Group Members must vote. A quorum of 3 Chairs is needed Two-thirds of the voting quorum must approve the proposal. A majority of Chairs must approve the proposal. A public notice period of no less than 14 days from the Call for Agreement proposing the change must elapse before voting begins. Any Call for Agreement that follows this process is considered a Governing Document. For initial approval of this Charter via Call for Agreement all members are eligible to vote, even those that have been a member for less than 21 days. This Charter will be approved if there is a majority of positive votes.","title":"Charter and Governing Documents"},{"location":"charter/#organizational-roles","text":"","title":"Organizational Roles"},{"location":"charter/#role-of-chairs","text":"The primary role of Chairs is to run operations and the governance of the group. The Chairs are responsible for: Setting the agenda for meetings. Extending discussion via asynchronous communication to be inclusive of members who cannot attend a specific meeting time. Scheduling discussing of proposals that have been submitted. Asking for new proposals to be made to address an identified need. Oversee disciplinary action for members. The Chairs have the authority to declare a member inactive and expel members for cause. Chairs will serve for one-year revolving terms and will be approved using the Condorcet Method . Upon the expiration of a Chair\u2019s term, it will continue for another year, unless challenged. The terms for founding Chairs start on the approval of this charter. When no candidate has submitted their name for consideration, the current Chairs may appoint an acting Chair until a candidate comes forward. Chairs must be active members. Any inactivity, disability, or ineligibility results in immediate removal. Chairs may be removed by petition to the Working Group through the Call for Agreement process outlined above. Additional Chairs may be added so long as the existing number of Chairs is odd. These Chairs are added using a Call for Agreement. Extra Chairs enjoy the same rights, responsibilities, and obligations of a Chartered Chair. Upon vacancy of an Extra Chair, it may be filled by appointment by the remaining Chairs, or a majority vote of the Working Group until the term naturally expires. In the event that an even number of Chairs exist and vote situation arises, the Chairs will randomly select one chair to abstain.","title":"Role of Chairs"},{"location":"charter/#role-of-sub-working-groups","text":"Each Sub-Working Group (SWG) must have a Chair working as an active sponsor. Under the mandate of the Working Group, each SWG will have the autonomy to establish their own charter, membership rules, meeting times, and management processes. Each SWG will also have the authority to make in-scope decisions as delegated by the Working Group. SWGs are required to submit their agreed Charter to the Working Group for information and archival. The Chairs can petition for dissolution of an inactive or hostile SWG by Call for Agreement. Once dissolved the SWG\u2019s delegated Charter and outstanding authority to make decisions is immediately revoked. The Chairs may then take any required action to restrict access to Working Group Resources. No SWG will have authority with regards to this Charter or other OKD Working Group Governing Documents.","title":"Role of Sub-Working Groups"},{"location":"community/","text":"End User Community \u00b6 OKD has an active community of end-users, with many different use-cases. From enterprises, academic institutions or home hobbyists. In addition to the end-user community there is a smaller community of volunteers that contribute to the OKD project by helping other users resolve issues or by participating in one of the OKD working groups to enhance the OKD project. Code of Conduct \u00b6 We want the OKD community to be a welcoming community, where everyone is treat with respect, so the link to the code of conduct should be made visible at all events Red Hat supports the Inclusive Naming Initiative and the OKD project follows the guidelines and recommendations from that project. All contributions to OKD must also follow their guidelines End-User community \u00b6 The community of OKD users is a self-supporting community. There is no official support for OKD, all help is community provided. The Help section provides details on how to get help for any issues you may be experiencing. We encourage all users to participate in discussions and to help fellow users where they can. Contributing to OKD \u00b6 The OKD project has a charter , setting out how the project is run. If you want to join the team of volunteers working on the OKD project then details of how to become a contributor are set out here .","title":"OKD Community"},{"location":"community/#end-user-community","text":"OKD has an active community of end-users, with many different use-cases. From enterprises, academic institutions or home hobbyists. In addition to the end-user community there is a smaller community of volunteers that contribute to the OKD project by helping other users resolve issues or by participating in one of the OKD working groups to enhance the OKD project.","title":"End User Community"},{"location":"community/#code-of-conduct","text":"We want the OKD community to be a welcoming community, where everyone is treat with respect, so the link to the code of conduct should be made visible at all events Red Hat supports the Inclusive Naming Initiative and the OKD project follows the guidelines and recommendations from that project. All contributions to OKD must also follow their guidelines","title":"Code of Conduct"},{"location":"community/#end-user-community_1","text":"The community of OKD users is a self-supporting community. There is no official support for OKD, all help is community provided. The Help section provides details on how to get help for any issues you may be experiencing. We encourage all users to participate in discussions and to help fellow users where they can.","title":"End-User community"},{"location":"community/#contributing-to-okd","text":"The OKD project has a charter , setting out how the project is run. If you want to join the team of volunteers working on the OKD project then details of how to become a contributor are set out here .","title":"Contributing to OKD"},{"location":"conduct/","text":"OKD Community Code of Conduct \u00b6 Every community can be strengthened by a diverse variety of viewpoints, insights, opinions, skill sets, and skill levels. However, with diversity comes the potential for disagreement and miscommunication. The purpose of this Code of Conduct is to ensure that disagreements and differences of opinion are conducted respectfully and on their own merits, without personal attacks or other behavior that might create an unsafe or unwelcoming environment. These policies are not designed to be a comprehensive set of things you cannot do. We ask that you treat your fellow community members with respect and courtesy. This Code of Conduct should be followed in spirit as much as in letter and is not exhaustive. All okd events and community members are governed by this Code of Conduct and anti-harassment policy. We expect working group chairs and organizers to enforce these guidelines throughout all events, and we expect attendees, speakers, sponsors, and volunteers to help ensure a safe environment for our whole community. For the purposes of this Code of Conduct: An event is any in-person or on-line gathering, e-mail thread, chat room, code or documentation contribution, or any situation where okd community members are invited to gather and communicate. A community member is anyone who attends okd events and/or participates in contributing or maintaining the okd project. okd community members are: Considerate : Contributions of every kind have far-ranging consequences. Just as your work depends on the work of others, decisions you make surrounding your contributions to the okd community affect your fellow community members. You are strongly encouraged to take those consequences into account while making decisions. Additionally, pinging a specific person with general questions is inconsiderate to that person. Always pose general question to the community as a whole. Patient : Asynchronous communication can come with its own frustrations, even in the most responsive of communities. Please remember that our community is largely built on volunteered time. It might take some time to receive a response to questions, contributions, and requests for support. Repeated bumps or reminders in rapid succession are not good displays of patience. Always wait patiently for a response. Respectful : Every community inevitably has disagreements, but disagreements are never an excuse for rudeness, hostility, threatening behavior, abuse (verbal or physical), or personal attacks. Always remember that it is possible to disagree respectfully and courteously. Kind : Everyone should feel welcome in the okd community, regardless of background. Please be courteous, respectful and polite to fellow community members. Additionally, you are encouraged not to make assumptions about the background or identity of your fellow community members. Always follow the xref:okd-code-conduct-harassment[Anti-harassment policy]. Inquisitive : We encourage okd community members to ask early and ask often . Rather than asking whether you can ask a question (the answer is always yes!), simply ask your question. You are encouraged to provide as many specifics as possible. Code snippets in the form of Gists or other paste site links are almost always needed in order to get the most helpful answers. Refrain from pasting multiple lines of code directly into the Google Group channel or emails. Instead use gist.github.com or another paste site to provide code snippets. Always remember that the only stupid question is the one that does not get asked. Helpful : The okd community is committed to being a welcoming environment for all users, regardless of skill level. Our community cannot grow without an environment where new users feel safe and comfortable asking questions. It can become frustrating to answer the same questions repeatedly; however, community members are expected to remain courteous and helpful to all users equally, regardless of skill or knowledge level. At the same time, everyone is expected to read the provided documentation thoroughly. We are happy to answer questions, provide strategic guidance, and suggest effective workflows, but we are not here to do your job for you. Always remember we were all beginners once upon a time, Anti-harassment policy \u00b6 Harassment includes (but is not limited to) the following behaviors: Offensive comments related to gender (including gender expression and identity), age, sexual orientation, disability, physical appearance, body size, race, and religion Derogatory terminology including words commonly known to be slurs Posting sexual images or imagery in public spaces Deliberate intimidation Stalking Posting others\u2019 personal information without explicit permission Sustained disruption of talks or other events Inappropriate physical contact Unwelcome sexual attention Community members asked to stop any harassing behavior are expected to comply immediately. In particular, community members should not use sexual images, activities, or other material. Community members should not use sexual attire or otherwise create a sexualized environment at community events. In addition to the behaviors outlined above, continuing to behave in a certain way after you have been asked to stop also constitutes harassment, even if that behavior is not specifically outlined in this policy. It is considerate and respectful to stop doing something after you have been asked to stop, and all community members are expected to comply with such requests immediately. Policy violations \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting codeofconduct@okd.io . If a community member engages in harassing behavior, organizers or working group chairs may take any action deemed appropriate. These actions may include but are not limited to warning the offender and expelling the offender from an event. The OKD working group leaders might determine that the offender should be barred from participating in the community. Event organizers and working group leaders will be happy to help community members contact security or local law enforcement, provide escorts to an alternate location, or otherwise assist those experiencing harassment to feel safe for the duration of an event. We value the safety and well-being of our community members and want everyone to feel welcome at our events, both online and in-person. We expect all community members to follow these policies during all of our events. The okd Community Code of Conduct is licensed under the Creative Commons Attribution-Share Alike 3.0 license. Our Code of Conduct was adapted from Codes of Conduct of other open source projects, including: Ansible Contributor Covenant Elastic The Fedora Project OpenStack Puppet Labs Ubuntu","title":"Code of Conduct"},{"location":"conduct/#okd-community-code-of-conduct","text":"Every community can be strengthened by a diverse variety of viewpoints, insights, opinions, skill sets, and skill levels. However, with diversity comes the potential for disagreement and miscommunication. The purpose of this Code of Conduct is to ensure that disagreements and differences of opinion are conducted respectfully and on their own merits, without personal attacks or other behavior that might create an unsafe or unwelcoming environment. These policies are not designed to be a comprehensive set of things you cannot do. We ask that you treat your fellow community members with respect and courtesy. This Code of Conduct should be followed in spirit as much as in letter and is not exhaustive. All okd events and community members are governed by this Code of Conduct and anti-harassment policy. We expect working group chairs and organizers to enforce these guidelines throughout all events, and we expect attendees, speakers, sponsors, and volunteers to help ensure a safe environment for our whole community. For the purposes of this Code of Conduct: An event is any in-person or on-line gathering, e-mail thread, chat room, code or documentation contribution, or any situation where okd community members are invited to gather and communicate. A community member is anyone who attends okd events and/or participates in contributing or maintaining the okd project. okd community members are: Considerate : Contributions of every kind have far-ranging consequences. Just as your work depends on the work of others, decisions you make surrounding your contributions to the okd community affect your fellow community members. You are strongly encouraged to take those consequences into account while making decisions. Additionally, pinging a specific person with general questions is inconsiderate to that person. Always pose general question to the community as a whole. Patient : Asynchronous communication can come with its own frustrations, even in the most responsive of communities. Please remember that our community is largely built on volunteered time. It might take some time to receive a response to questions, contributions, and requests for support. Repeated bumps or reminders in rapid succession are not good displays of patience. Always wait patiently for a response. Respectful : Every community inevitably has disagreements, but disagreements are never an excuse for rudeness, hostility, threatening behavior, abuse (verbal or physical), or personal attacks. Always remember that it is possible to disagree respectfully and courteously. Kind : Everyone should feel welcome in the okd community, regardless of background. Please be courteous, respectful and polite to fellow community members. Additionally, you are encouraged not to make assumptions about the background or identity of your fellow community members. Always follow the xref:okd-code-conduct-harassment[Anti-harassment policy]. Inquisitive : We encourage okd community members to ask early and ask often . Rather than asking whether you can ask a question (the answer is always yes!), simply ask your question. You are encouraged to provide as many specifics as possible. Code snippets in the form of Gists or other paste site links are almost always needed in order to get the most helpful answers. Refrain from pasting multiple lines of code directly into the Google Group channel or emails. Instead use gist.github.com or another paste site to provide code snippets. Always remember that the only stupid question is the one that does not get asked. Helpful : The okd community is committed to being a welcoming environment for all users, regardless of skill level. Our community cannot grow without an environment where new users feel safe and comfortable asking questions. It can become frustrating to answer the same questions repeatedly; however, community members are expected to remain courteous and helpful to all users equally, regardless of skill or knowledge level. At the same time, everyone is expected to read the provided documentation thoroughly. We are happy to answer questions, provide strategic guidance, and suggest effective workflows, but we are not here to do your job for you. Always remember we were all beginners once upon a time,","title":"OKD Community Code of Conduct"},{"location":"conduct/#anti-harassment-policy","text":"Harassment includes (but is not limited to) the following behaviors: Offensive comments related to gender (including gender expression and identity), age, sexual orientation, disability, physical appearance, body size, race, and religion Derogatory terminology including words commonly known to be slurs Posting sexual images or imagery in public spaces Deliberate intimidation Stalking Posting others\u2019 personal information without explicit permission Sustained disruption of talks or other events Inappropriate physical contact Unwelcome sexual attention Community members asked to stop any harassing behavior are expected to comply immediately. In particular, community members should not use sexual images, activities, or other material. Community members should not use sexual attire or otherwise create a sexualized environment at community events. In addition to the behaviors outlined above, continuing to behave in a certain way after you have been asked to stop also constitutes harassment, even if that behavior is not specifically outlined in this policy. It is considerate and respectful to stop doing something after you have been asked to stop, and all community members are expected to comply with such requests immediately.","title":"Anti-harassment policy"},{"location":"conduct/#policy-violations","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting codeofconduct@okd.io . If a community member engages in harassing behavior, organizers or working group chairs may take any action deemed appropriate. These actions may include but are not limited to warning the offender and expelling the offender from an event. The OKD working group leaders might determine that the offender should be barred from participating in the community. Event organizers and working group leaders will be happy to help community members contact security or local law enforcement, provide escorts to an alternate location, or otherwise assist those experiencing harassment to feel safe for the duration of an event. We value the safety and well-being of our community members and want everyone to feel welcome at our events, both online and in-person. We expect all community members to follow these policies during all of our events. The okd Community Code of Conduct is licensed under the Creative Commons Attribution-Share Alike 3.0 license. Our Code of Conduct was adapted from Codes of Conduct of other open source projects, including: Ansible Contributor Covenant Elastic The Fedora Project OpenStack Puppet Labs Ubuntu","title":"Policy violations"},{"location":"contributor/","text":"Contributor Community \u00b6 OKD is built from many different open source projects - Fedora CoreOS, the CentOS and UBI RPM ecosystems, cri-o, Kubernetes, and many different extensions to Kubernetes. The openshift organization on GitHub holds active development of components on top of Kubernetes and references projects built elsewhere. Generally, you'll want to find the component that interests you and review their README.md for the processes for contributing. Community process and questions can be raised in our community repo and issues opened in this repository (Bugzilla locations coming soon). Our unified continuous integration system tests pull requests to the ecosystem and core images, then builds and promotes them after merge. To see the latest development releases of OKD visit our continuous release page . These releases are built continuously and expire after a few days. Long lived versions are pinned and then listed on our stable release page . All contributions are welcome - OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the OKD discussion forum , or get involved in the Kubernetes project at the container runtime layer. Becoming a contributor \u00b6 The easiest way to get involved in the community is to: watch replays of previous working group meetings attend one of the working group meetings (no invite needed, just use the link in the calendar to join the BlueJeans video conference). join the OKD Working Group Google Group join the #openshift-dev channel on Slack The OKD project has a charter , setting out how the project is run. Working Groups \u00b6 The project is managed by a bi-weekly working group video call: The main working group is where are the major project decisions are made, but when a specific work item needs to be completed a sub-group may be formed, so a focussed set of volunteers can work on a specific area.","title":"Contributor"},{"location":"contributor/#contributor-community","text":"OKD is built from many different open source projects - Fedora CoreOS, the CentOS and UBI RPM ecosystems, cri-o, Kubernetes, and many different extensions to Kubernetes. The openshift organization on GitHub holds active development of components on top of Kubernetes and references projects built elsewhere. Generally, you'll want to find the component that interests you and review their README.md for the processes for contributing. Community process and questions can be raised in our community repo and issues opened in this repository (Bugzilla locations coming soon). Our unified continuous integration system tests pull requests to the ecosystem and core images, then builds and promotes them after merge. To see the latest development releases of OKD visit our continuous release page . These releases are built continuously and expire after a few days. Long lived versions are pinned and then listed on our stable release page . All contributions are welcome - OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the OKD discussion forum , or get involved in the Kubernetes project at the container runtime layer.","title":"Contributor Community"},{"location":"contributor/#becoming-a-contributor","text":"The easiest way to get involved in the community is to: watch replays of previous working group meetings attend one of the working group meetings (no invite needed, just use the link in the calendar to join the BlueJeans video conference). join the OKD Working Group Google Group join the #openshift-dev channel on Slack The OKD project has a charter , setting out how the project is run.","title":"Becoming a contributor"},{"location":"contributor/#working-groups","text":"The project is managed by a bi-weekly working group video call: The main working group is where are the major project decisions are made, but when a specific work item needs to be completed a sub-group may be formed, so a focussed set of volunteers can work on a specific area.","title":"Working Groups"},{"location":"crc/","text":"CodeReady Containers for OKD \u00b6 CodeReady Containers brings a minimal, single node OKD 4 cluster to your local computer. This cluster provides a minimal environment for development and testing purposes. CodeReady Containers is mainly targeted at running on developers' laptops and desktops. Download CodeReady Containers for OKD \u00b6 Run a developer instance of OKD4 on your local workstation with CodeReady Containers built for OKD - >No Pull Secret Required! CodeReady Containers for OKD4 - Mac OS image CodeReady Containers for OKD4 - Linux image CodeReady Containers for OKD4 - Windows image The Getting Started Guide explains how to install and use CodeReady Containers. If you encounter any problems, please open a discussion item in the OKD GitHub Community ! CRC Working group \u00b6 There is a working group looking at automating the OKD CRC build process. If you want technical details on how to build OKD CRC see the working group section of this site","title":"CRC"},{"location":"crc/#codeready-containers-for-okd","text":"CodeReady Containers brings a minimal, single node OKD 4 cluster to your local computer. This cluster provides a minimal environment for development and testing purposes. CodeReady Containers is mainly targeted at running on developers' laptops and desktops.","title":"CodeReady Containers for OKD"},{"location":"crc/#download-codeready-containers-for-okd","text":"Run a developer instance of OKD4 on your local workstation with CodeReady Containers built for OKD - >No Pull Secret Required! CodeReady Containers for OKD4 - Mac OS image CodeReady Containers for OKD4 - Linux image CodeReady Containers for OKD4 - Windows image The Getting Started Guide explains how to install and use CodeReady Containers. If you encounter any problems, please open a discussion item in the OKD GitHub Community !","title":"Download CodeReady Containers for OKD"},{"location":"crc/#crc-working-group","text":"There is a working group looking at automating the OKD CRC build process. If you want technical details on how to build OKD CRC see the working group section of this site","title":"CRC Working group"},{"location":"docs/","text":"Documentation \u00b6 There are 2 primary sources of information for OKD: community documentation - https://okd.io (this site) product documentation - https://docs.okd.io Updates and Issues \u00b6 If you encounter an issue with the documentation or have an idea to improve the content or add new content then please follow the directions below to learn how you can get changes made. The source for the documentation is managed in GitHub. There are different processes for requesting changes in the community and product documentation: Community documentation \u00b6 The OKD Documentation subgroup is responsible for the community documentation. The process for making changes is set out in the working group section of the documentation Product documentation \u00b6 The OKD docs are built off the openshift/openshift-docs repo. If you notice any problems in the OKD docs that need to be addressed, you can either create a pull request with those changes against the openshift/openshift-docs repo or create an issue to suggest the changes. Among the changes you could suggest are: errors typos missing information incorrect product name (OpenShift Container Platform instead of OKD) Incorrect operating system (RHEL or RHCOS instead of FCOS) incorrect code examples If you create an issue, please do the following: Add [OKD] to the title of the issue. Provide as much information as possible, including the problem, the exact location in the file, the versions of OKD that the error affects (if known), and the correction you would like to see. A link to the file with the problem is extremely helpful. If you have the appropriate permissions, assign the issue to Michael Burke (mburke5678) and Diane Mueller (dmueller2001) so that the issue gets our direct attention. You can assign an issue by including the following in the issue description: /assign @mburke5678 /assign @dmueller2001 If not, you can @ mention mburke5678 in a comment. If you have the permissions, add a kind/documentation label.","title":"Documentation"},{"location":"docs/#documentation","text":"There are 2 primary sources of information for OKD: community documentation - https://okd.io (this site) product documentation - https://docs.okd.io","title":"Documentation"},{"location":"docs/#updates-and-issues","text":"If you encounter an issue with the documentation or have an idea to improve the content or add new content then please follow the directions below to learn how you can get changes made. The source for the documentation is managed in GitHub. There are different processes for requesting changes in the community and product documentation:","title":"Updates and Issues"},{"location":"docs/#community-documentation","text":"The OKD Documentation subgroup is responsible for the community documentation. The process for making changes is set out in the working group section of the documentation","title":"Community documentation"},{"location":"docs/#product-documentation","text":"The OKD docs are built off the openshift/openshift-docs repo. If you notice any problems in the OKD docs that need to be addressed, you can either create a pull request with those changes against the openshift/openshift-docs repo or create an issue to suggest the changes. Among the changes you could suggest are: errors typos missing information incorrect product name (OpenShift Container Platform instead of OKD) Incorrect operating system (RHEL or RHCOS instead of FCOS) incorrect code examples If you create an issue, please do the following: Add [OKD] to the title of the issue. Provide as much information as possible, including the problem, the exact location in the file, the versions of OKD that the error affects (if known), and the correction you would like to see. A link to the file with the problem is extremely helpful. If you have the appropriate permissions, assign the issue to Michael Burke (mburke5678) and Diane Mueller (dmueller2001) so that the issue gets our direct attention. You can assign an issue by including the following in the issue description: /assign @mburke5678 /assign @dmueller2001 If not, you can @ mention mburke5678 in a comment. If you have the permissions, add a kind/documentation label.","title":"Product documentation"},{"location":"faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Below are answers to common questions regarding OKD installation and administration. If you have a suggested question or a suggested improvement to an answer, please feel free to reach out. What are the relations with OCP project? Is OKD4 an upstream of OCP? \u00b6 In 3.x release time OKD was used as an upstream project for Openshift Container Platform. OKD could be installed on Fedora/CentOS/RHEL and used CentOS based images to install the cluster. OCP, however, could be installed only on RHEL and its images were rebuilt to be RHEL-based. Universal Base Image project has enabled us to run RHEL-based images on any platform, so the full image rebuild is no longer necessary, allowing OKD4 project to reuse most images from OCP4. There is another critical part of OCP - Red Hat Enterprise Linux CoreOS. Although RHCOS is an open source project (much like RHEL8) it's not a community-driven project. As a result, OKD workgroup has made a decision to use Fedora CoreOS - open source and community-driven project - as a base for OKD4. This decision allows end-users to modify all parts of the cluster using prepared instructions. It should be noted that OKD4 is being automatically built from OCP4 ci stream , so most of the tests are happening in OCP CI and being mirrored to OKD. As a result, OKD4 CI doesn't have to run a lot of tests to ensure the release is valid. These relationships are more complex than \"upstream/downstream\", so we use \"sibling distributions\" to describe its state. How stable is OKD4? \u00b6 OKD4 builds are being automatically tested by release-controller . Release is rejected if either installation, upgrade from previous version or conformance test fails. Test results determine the upgrade graph, so for instance, if upgrade tests passed for beta5->rc edge, clusters on beta5 can be directly updated to rc release, bypassing beta6. The OKD stable version is released bi-weekly, following Fedora CoreOS schedule, client tools are uploaded to Github and images are mirrored to Quay. Can I run a single node cluster? \u00b6 Currently, single-node cluster installations cannot be deployed directly by the 4.7 installer. This is a known issue . Single-node cluster installations do work with the 4.8 nightly installer builds. As an alternative, if OKD version 4.7 is needed, you may have luck with Charro Gruver's OKD 4 Single Node Cluster instructions . You can also use Code Ready Containers (CRC) to run a single-node cluster on your desktop. What to do in case of errors? \u00b6 If you experience problems during installation you must collect the bootstrap log bundle, see instructions If you experience problems post installation, collect data of your cluster with: oc adm must-gather See documentation for more information. Upload it to a file hoster and send the link to the developers (Slack channel, ...) During installation the SSH key is required. It can be used to SSH onto the nodes later on - ssh core@<node ip> Where do I seek support? \u00b6 OKD is a community-supported distribution, Red Hat does not provide commercial support of OKD installations. Contact us on Slack: Workspace: Kubernetes, Channel: #openshift-dev (for developer communication) Workspace: Kubernetes, Channel: #openshift-users (for users ) See https://openshift.tips/ for useful Openshift tips Where can I find upgrades? \u00b6 https://amd64.origin.releases.ci.openshift.org/ Warning Nightly builds (from 4.x.0-0.okd ) are pruned every 72 hours. If your cluster uses these images, consider mirroring these files to a local registry. Builds from the stable-4 stream are not removed. How can I upgrade my cluster to a new version? \u00b6 Find a version where a tested upgrade path is available from your version for on https://amd64.origin.releases.ci.openshift.org/ Upgrade options: Preferred ways: Web Console: Home -> Overview -> Tab: Cluster, Card: Overview -> View settings -> Update Status Shell: Upgrades to latest available version oc adm upgrade Last resort : Upgrade to a certain version (will ignore the update graph!) oc adm upgrade --force --allow-explicit-upgrade = true --to-image = registry.ci.openshift.org/origin/release:4.4.0-0.okd-2020-03-16-105308 This will take a while; the upgrade may take several hours. Throughout the upgrade, kubernetes API would still be accessible and user workloads would be evicted and rescheduled as nodes are updated. Interesting commands while an upgrade runs \u00b6 Check overall upgrade status: oc get clusterversion Check the status of your cluster operators: oc get co Check the status of your nodes (cluster upgrades may include base OS updates): oc get nodes How can I find out what's inside of a (CI) release and which commit id each component has? \u00b6 This one is very helpful if you want to know if a certain commit has landed in your current version: oc adm release info registry.ci.openshift.org/origin/release:4.4 --commit-urls Name: 4.4.0-0.okd-2020-04-10-020541 Digest: sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Created: 2020-04-10T02:14:15Z OS/Arch: linux/amd64 Manifests: 413 Pull From: registry.ci.openshift.org/origin/release@sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Release Metadata: Version: 4.4.0-0.okd-2020-04-10-020541 Upgrades: <none> Component Versions: kubernetes 1.17.1 machine-os 31.20200407.20 Fedora CoreOS Images: NAME URL aws-machine-controllers https://github.com/openshift/cluster-api-provider-aws/commit/5fa82204468e71b44f65a5f24e2675dbfa0f5c29 azure-machine-controllers https://github.com/openshift/cluster-api-provider-azure/commit/832a43a30d7f00cd6774c1f5cd117aeebbe1b730 baremetal-installer https://github.com/openshift/installer/commit/a58f24b0df7e3699b39d4ae1d23c45672706934d baremetal-machine-controllers baremetal-operator baremetal-runtimecfg https://github.com/openshift/baremetal-runtimecfg/commit/09850a724d9290ffb05db3dd7f4f4c748b982759 branding https://github.com/openshift/origin-branding/commit/068fa1eac9f31ffe13089dd3de2ec49c153b2a14 cli https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cli-artifacts https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cloud-credential-operator https://github.com/openshift/cloud-credential-operator/commit/446680ed10ac938e11626409acb0c076edd3fd52 ... How to use the official installation container? \u00b6 The official installer container is part of every release. # Find out the installer image. oc adm release info quay.io/openshift/okd:4.7.0-0.okd-2021-04-24-103438 --image-for = installer # Example output # quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 # Run the container via Podman or Docker to perform tasks. e.g. create ignition configurations docker run -v $( pwd ) :/output -ti quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 create ignition-configs","title":"FAQ"},{"location":"faq/#frequently-asked-questions-faq","text":"Below are answers to common questions regarding OKD installation and administration. If you have a suggested question or a suggested improvement to an answer, please feel free to reach out.","title":"Frequently Asked Questions (FAQ)"},{"location":"faq/#what-are-the-relations-with-ocp-project-is-okd4-an-upstream-of-ocp","text":"In 3.x release time OKD was used as an upstream project for Openshift Container Platform. OKD could be installed on Fedora/CentOS/RHEL and used CentOS based images to install the cluster. OCP, however, could be installed only on RHEL and its images were rebuilt to be RHEL-based. Universal Base Image project has enabled us to run RHEL-based images on any platform, so the full image rebuild is no longer necessary, allowing OKD4 project to reuse most images from OCP4. There is another critical part of OCP - Red Hat Enterprise Linux CoreOS. Although RHCOS is an open source project (much like RHEL8) it's not a community-driven project. As a result, OKD workgroup has made a decision to use Fedora CoreOS - open source and community-driven project - as a base for OKD4. This decision allows end-users to modify all parts of the cluster using prepared instructions. It should be noted that OKD4 is being automatically built from OCP4 ci stream , so most of the tests are happening in OCP CI and being mirrored to OKD. As a result, OKD4 CI doesn't have to run a lot of tests to ensure the release is valid. These relationships are more complex than \"upstream/downstream\", so we use \"sibling distributions\" to describe its state.","title":"What are the relations with OCP project? Is OKD4 an upstream of OCP?"},{"location":"faq/#how-stable-is-okd4","text":"OKD4 builds are being automatically tested by release-controller . Release is rejected if either installation, upgrade from previous version or conformance test fails. Test results determine the upgrade graph, so for instance, if upgrade tests passed for beta5->rc edge, clusters on beta5 can be directly updated to rc release, bypassing beta6. The OKD stable version is released bi-weekly, following Fedora CoreOS schedule, client tools are uploaded to Github and images are mirrored to Quay.","title":"How stable is OKD4?"},{"location":"faq/#can-i-run-a-single-node-cluster","text":"Currently, single-node cluster installations cannot be deployed directly by the 4.7 installer. This is a known issue . Single-node cluster installations do work with the 4.8 nightly installer builds. As an alternative, if OKD version 4.7 is needed, you may have luck with Charro Gruver's OKD 4 Single Node Cluster instructions . You can also use Code Ready Containers (CRC) to run a single-node cluster on your desktop.","title":"Can I run a single node cluster?"},{"location":"faq/#what-to-do-in-case-of-errors","text":"If you experience problems during installation you must collect the bootstrap log bundle, see instructions If you experience problems post installation, collect data of your cluster with: oc adm must-gather See documentation for more information. Upload it to a file hoster and send the link to the developers (Slack channel, ...) During installation the SSH key is required. It can be used to SSH onto the nodes later on - ssh core@<node ip>","title":"What to do in case of errors?"},{"location":"faq/#where-do-i-seek-support","text":"OKD is a community-supported distribution, Red Hat does not provide commercial support of OKD installations. Contact us on Slack: Workspace: Kubernetes, Channel: #openshift-dev (for developer communication) Workspace: Kubernetes, Channel: #openshift-users (for users ) See https://openshift.tips/ for useful Openshift tips","title":"Where do I seek support?"},{"location":"faq/#where-can-i-find-upgrades","text":"https://amd64.origin.releases.ci.openshift.org/ Warning Nightly builds (from 4.x.0-0.okd ) are pruned every 72 hours. If your cluster uses these images, consider mirroring these files to a local registry. Builds from the stable-4 stream are not removed.","title":"Where can I find upgrades?"},{"location":"faq/#how-can-i-upgrade-my-cluster-to-a-new-version","text":"Find a version where a tested upgrade path is available from your version for on https://amd64.origin.releases.ci.openshift.org/ Upgrade options: Preferred ways: Web Console: Home -> Overview -> Tab: Cluster, Card: Overview -> View settings -> Update Status Shell: Upgrades to latest available version oc adm upgrade Last resort : Upgrade to a certain version (will ignore the update graph!) oc adm upgrade --force --allow-explicit-upgrade = true --to-image = registry.ci.openshift.org/origin/release:4.4.0-0.okd-2020-03-16-105308 This will take a while; the upgrade may take several hours. Throughout the upgrade, kubernetes API would still be accessible and user workloads would be evicted and rescheduled as nodes are updated.","title":"How can I upgrade my cluster to a new version?"},{"location":"faq/#interesting-commands-while-an-upgrade-runs","text":"Check overall upgrade status: oc get clusterversion Check the status of your cluster operators: oc get co Check the status of your nodes (cluster upgrades may include base OS updates): oc get nodes","title":"Interesting commands while an upgrade runs"},{"location":"faq/#how-can-i-find-out-whats-inside-of-a-ci-release-and-which-commit-id-each-component-has","text":"This one is very helpful if you want to know if a certain commit has landed in your current version: oc adm release info registry.ci.openshift.org/origin/release:4.4 --commit-urls Name: 4.4.0-0.okd-2020-04-10-020541 Digest: sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Created: 2020-04-10T02:14:15Z OS/Arch: linux/amd64 Manifests: 413 Pull From: registry.ci.openshift.org/origin/release@sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Release Metadata: Version: 4.4.0-0.okd-2020-04-10-020541 Upgrades: <none> Component Versions: kubernetes 1.17.1 machine-os 31.20200407.20 Fedora CoreOS Images: NAME URL aws-machine-controllers https://github.com/openshift/cluster-api-provider-aws/commit/5fa82204468e71b44f65a5f24e2675dbfa0f5c29 azure-machine-controllers https://github.com/openshift/cluster-api-provider-azure/commit/832a43a30d7f00cd6774c1f5cd117aeebbe1b730 baremetal-installer https://github.com/openshift/installer/commit/a58f24b0df7e3699b39d4ae1d23c45672706934d baremetal-machine-controllers baremetal-operator baremetal-runtimecfg https://github.com/openshift/baremetal-runtimecfg/commit/09850a724d9290ffb05db3dd7f4f4c748b982759 branding https://github.com/openshift/origin-branding/commit/068fa1eac9f31ffe13089dd3de2ec49c153b2a14 cli https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cli-artifacts https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cloud-credential-operator https://github.com/openshift/cloud-credential-operator/commit/446680ed10ac938e11626409acb0c076edd3fd52 ...","title":"How can I find out what's inside of a (CI) release and which commit id each component has?"},{"location":"faq/#how-to-use-the-official-installation-container","text":"The official installer container is part of every release. # Find out the installer image. oc adm release info quay.io/openshift/okd:4.7.0-0.okd-2021-04-24-103438 --image-for = installer # Example output # quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 # Run the container via Podman or Docker to perform tasks. e.g. create ignition configurations docker run -v $( pwd ) :/output -ti quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 create ignition-configs","title":"How to use the official installation container?"},{"location":"help/","text":"Help \u00b6 There is no official product support for OKD as it is a community project. All assistance is provided by volunteers from the user community. How to ask for help \u00b6 For questions or feedback, start a discussion on the discussion forum or reach us on Kubernetes Slack on #openshift-users Community Etiquette \u00b6 As all assistance is provided by the community, you are reminded of the code-of-conduct when asking a question or replying to a question. Before starting a new discussion topic, do a search on the discussion forum to see if anyone else has already raised the same issue - then contribute to the existing discussion topic rather than starting a new topic. When seeking help you should provide all the information a community volunteer may need to assist you. The easier it is for a volunteer to understand your issue, the more likely they are to provide assistance. This information should include: the version of OKD you are using the platform you are running on (and type of install - IPI / UPI) if you are following instructions and are having issues, a link to the instructions the must gather logs - these should be uploaded to a sharing site, such as Google Drive then a public link added to the discussion topic a description of the steps that can be used to recreate your issue, if applicable Please do not tag people you see answering other questions to try to get a faster answer as it is anti-social. We have an active community and it is up to individuals which questions they feel they want to respond to. Raising bugs \u00b6 We are trying to do all the diagnostic work in the discussion forum rather than using issues for the OKD project. If you are certain you have discovered a bug, then please raise an issue , but if you are not sure if you have found a bug then use the discussion forum to discuss it. If it turns out to be a bug, then the discussion topic can be converted to an issue.","title":"Getting Help"},{"location":"help/#help","text":"There is no official product support for OKD as it is a community project. All assistance is provided by volunteers from the user community.","title":"Help"},{"location":"help/#how-to-ask-for-help","text":"For questions or feedback, start a discussion on the discussion forum or reach us on Kubernetes Slack on #openshift-users","title":"How to ask for help"},{"location":"help/#community-etiquette","text":"As all assistance is provided by the community, you are reminded of the code-of-conduct when asking a question or replying to a question. Before starting a new discussion topic, do a search on the discussion forum to see if anyone else has already raised the same issue - then contribute to the existing discussion topic rather than starting a new topic. When seeking help you should provide all the information a community volunteer may need to assist you. The easier it is for a volunteer to understand your issue, the more likely they are to provide assistance. This information should include: the version of OKD you are using the platform you are running on (and type of install - IPI / UPI) if you are following instructions and are having issues, a link to the instructions the must gather logs - these should be uploaded to a sharing site, such as Google Drive then a public link added to the discussion topic a description of the steps that can be used to recreate your issue, if applicable Please do not tag people you see answering other questions to try to get a faster answer as it is anti-social. We have an active community and it is up to individuals which questions they feel they want to respond to.","title":"Community Etiquette"},{"location":"help/#raising-bugs","text":"We are trying to do all the diagnostic work in the discussion forum rather than using issues for the OKD project. If you are certain you have discovered a bug, then please raise an issue , but if you are not sure if you have found a bug then use the discussion forum to discuss it. If it turns out to be a bug, then the discussion topic can be converted to an issue.","title":"Raising bugs"},{"location":"installation/","text":"Install OKD \u00b6 Plan your installation \u00b6 OKD supports 2 types of cluster install options: Installer-provisioned infrastructure (IPI) User-provisioned infrastructure (UPI) IPI is a largely automated install process, where the installer is responsible for setting up the infrastructure, where UPI requires you to set up the base infrastructure. You can find further details in the documentation OKD support installation on bare metal hardware, a number of virtualization platforms and a number of cloud platforms, so you need to decide where you want to install OKD and that your environment has sufficient resources for the cluster to operate. The documentation has more information to help you plan your installation. If you want to install on a typical developer workstation, then Code-Ready Containers may be a better options, as that is a cut-down installation designed to run on limited compute and memory resources. You can find examples of OKD installations, setup by OKD community members in the guides section. Getting Started \u00b6 To obtain the openshift installer and client, visit releases for stable versions or https://amd64.origin.releases.ci.openshift.org/ for nightlies. You can verify the downloads using: curl https://www.okd.io/vrutkovs.pub | gpg --import Output gpg: key 3D54B6723B20C69F: public key \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" imported gpg: Total number processed: 1 gpg: imported: 1 gpg --verify sha256sum.txt.asc sha256sum.txt Output gpg: Signature made Mon May 25 18:48:22 2020 CEST gpg: using RSA key DB861D01D4D1138A993ADC1A3D54B6723B20C69F gpg: Good signature from \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" [ultimate] gpg: aka \"Vadim Rutkovsky <vrutkovs@redhat.com>\" [ultimate] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: DB86 1D01 D4D1 138A 993A DC1A 3D54 B672 3B20 C69F sha256sum -c sha256sum.txt Output release.txt: OK openshift-client-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-windows-4.4.0-0.okd-2020-05-23-055148-beta5.zip: OK openshift-install-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-install-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK Please note that each nightly release is pruned after 72 hours. If the nightly that you installed was pruned, the cluster may be unable to pull necessary images and may show errors for various functionality (including updates). Alternatively, if you have the openshift client oc already installed, you can use it to download and extract the openshift installer and client from our container image: oc adm release extract --tools quay.io/openshift/okd:4.5.0-0.okd-2020-07-14-153706-ga Note You need a 4.x version of oc to extract the installer and the latest client. You can initially use the official Openshift client (mirror) There are full instructions in the OKD documentation for each supported platform, but the main steps for an IPI install are: extract the downloaded tarballs and copy the binaries into your PATH. run the following from an empty directory: openshift-install create cluster follow the prompts to create the install config you will need to have cloud credentials set in your shell properly before installation. you must have permission to configure the appropriate cloud resources from that account (such as VPCs, instances, and DNS records). you must have already configured a public DNS zone on your chosen cloud before the install starts. you will also be prompted for a pull-secret that will be made available to all of of your machines - for OKD4 you should either paste the pull-secret you use for your registry, or paste {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} to bypass the required value check (see bug #182 ). Once the install completes successfully the console URL and an admin username and password will be printed. If your DNS records were correct, you should be able to log in to your new OKD4 cluster! To undo the installation and delete any cloud resources created by the installer, run openshift-install destroy cluster Note The OpenShift client tools for your cluster can be downloaded from the help drop down menu at the top of the web console.","title":"Installation"},{"location":"installation/#install-okd","text":"","title":"Install OKD"},{"location":"installation/#plan-your-installation","text":"OKD supports 2 types of cluster install options: Installer-provisioned infrastructure (IPI) User-provisioned infrastructure (UPI) IPI is a largely automated install process, where the installer is responsible for setting up the infrastructure, where UPI requires you to set up the base infrastructure. You can find further details in the documentation OKD support installation on bare metal hardware, a number of virtualization platforms and a number of cloud platforms, so you need to decide where you want to install OKD and that your environment has sufficient resources for the cluster to operate. The documentation has more information to help you plan your installation. If you want to install on a typical developer workstation, then Code-Ready Containers may be a better options, as that is a cut-down installation designed to run on limited compute and memory resources. You can find examples of OKD installations, setup by OKD community members in the guides section.","title":"Plan your installation"},{"location":"installation/#getting-started","text":"To obtain the openshift installer and client, visit releases for stable versions or https://amd64.origin.releases.ci.openshift.org/ for nightlies. You can verify the downloads using: curl https://www.okd.io/vrutkovs.pub | gpg --import Output gpg: key 3D54B6723B20C69F: public key \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" imported gpg: Total number processed: 1 gpg: imported: 1 gpg --verify sha256sum.txt.asc sha256sum.txt Output gpg: Signature made Mon May 25 18:48:22 2020 CEST gpg: using RSA key DB861D01D4D1138A993ADC1A3D54B6723B20C69F gpg: Good signature from \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" [ultimate] gpg: aka \"Vadim Rutkovsky <vrutkovs@redhat.com>\" [ultimate] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: DB86 1D01 D4D1 138A 993A DC1A 3D54 B672 3B20 C69F sha256sum -c sha256sum.txt Output release.txt: OK openshift-client-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-windows-4.4.0-0.okd-2020-05-23-055148-beta5.zip: OK openshift-install-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-install-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK Please note that each nightly release is pruned after 72 hours. If the nightly that you installed was pruned, the cluster may be unable to pull necessary images and may show errors for various functionality (including updates). Alternatively, if you have the openshift client oc already installed, you can use it to download and extract the openshift installer and client from our container image: oc adm release extract --tools quay.io/openshift/okd:4.5.0-0.okd-2020-07-14-153706-ga Note You need a 4.x version of oc to extract the installer and the latest client. You can initially use the official Openshift client (mirror) There are full instructions in the OKD documentation for each supported platform, but the main steps for an IPI install are: extract the downloaded tarballs and copy the binaries into your PATH. run the following from an empty directory: openshift-install create cluster follow the prompts to create the install config you will need to have cloud credentials set in your shell properly before installation. you must have permission to configure the appropriate cloud resources from that account (such as VPCs, instances, and DNS records). you must have already configured a public DNS zone on your chosen cloud before the install starts. you will also be prompted for a pull-secret that will be made available to all of of your machines - for OKD4 you should either paste the pull-secret you use for your registry, or paste {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} to bypass the required value check (see bug #182 ). Once the install completes successfully the console URL and an admin username and password will be printed. If your DNS records were correct, you should be able to log in to your new OKD4 cluster! To undo the installation and delete any cloud resources created by the installer, run openshift-install destroy cluster Note The OpenShift client tools for your cluster can be downloaded from the help drop down menu at the top of the web console.","title":"Getting Started"},{"location":"working-groups/","text":"Working Groups \u00b6 OKD is governed by working groups as set out in the OKD Working Group Charter There is a primary working group, where all the main decisions are made regarding the project. Where an area of the project needs more time or is of interest to a subset of the working group membership, then a sub-group will be formed for that specific area, The current sub groups are: Documentation working group Code-Ready Containers on OKD working group OKD virtualization working group OKD Primary Working Group \u00b6 The OKD group meets virtually every other week. Calendar link : OKD working group calendar Agenda link : OKD working group agenda and meeting nodes \u00df Previous meeting playlist : YouTube playlist You don't need an invitation to join a working group, simple join the video call, by following the link in the calendar entry, You may also want to join online discussions as set out in the contributor section","title":"About"},{"location":"working-groups/#working-groups","text":"OKD is governed by working groups as set out in the OKD Working Group Charter There is a primary working group, where all the main decisions are made regarding the project. Where an area of the project needs more time or is of interest to a subset of the working group membership, then a sub-group will be formed for that specific area, The current sub groups are: Documentation working group Code-Ready Containers on OKD working group OKD virtualization working group","title":"Working Groups"},{"location":"working-groups/#okd-primary-working-group","text":"The OKD group meets virtually every other week. Calendar link : OKD working group calendar Agenda link : OKD working group agenda and meeting nodes \u00df Previous meeting playlist : YouTube playlist You don't need an invitation to join a working group, simple join the video call, by following the link in the calendar entry, You may also want to join online discussions as set out in the contributor section","title":"OKD Primary Working Group"},{"location":"blog/2021-03-07-new-blog.html/","text":"okd.io now has a blog \u00b6 Todo This content is for the current Middleman based OKD.io site Let's share news and useful information with each other \u00b6 We look forward to sharing news and useful information about OKD in this blog in the future. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io. Here's how to do it: Fork the repository https://github.com/openshift-cs/okd.io Create a new file in the source/blog directory The filename must have this format: yyyy-mm-dd-<title of your blog article (lowercase)>.html.markdown yyyy = Year (four digits) mm = Month (two digits) dd = Day (two digits) e.g.: source/blog/2021-03-07-my-first-article.html.markdown The date must not be in the future ! Each article must contain a header section like this one: --- title : <THIS TITLE WILL BE DISPLAYED IN THE BLOG> date : 2021-03-07 <- THIS DATE MUST EXACTLY MATCH THE DATE IN THE FILENAME tags : blog <- CHOOSE ONE OR MORE TAGS (COMMA SEPARATED) --- ... GITHUB MARKDOWN SYNTAX ... Test your changes locally. This README.md file will tell you how to do that. Create a pull request, if your article is ready to be published.","title":"okd.io now has a blog"},{"location":"blog/2021-03-07-new-blog.html/#okdio-now-has-a-blog","text":"Todo This content is for the current Middleman based OKD.io site","title":"okd.io now has a blog"},{"location":"blog/2021-03-07-new-blog.html/#lets-share-news-and-useful-information-with-each-other","text":"We look forward to sharing news and useful information about OKD in this blog in the future. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io. Here's how to do it: Fork the repository https://github.com/openshift-cs/okd.io Create a new file in the source/blog directory The filename must have this format: yyyy-mm-dd-<title of your blog article (lowercase)>.html.markdown yyyy = Year (four digits) mm = Month (two digits) dd = Day (two digits) e.g.: source/blog/2021-03-07-my-first-article.html.markdown The date must not be in the future ! Each article must contain a header section like this one: --- title : <THIS TITLE WILL BE DISPLAYED IN THE BLOG> date : 2021-03-07 <- THIS DATE MUST EXACTLY MATCH THE DATE IN THE FILENAME tags : blog <- CHOOSE ONE OR MORE TAGS (COMMA SEPARATED) --- ... GITHUB MARKDOWN SYNTAX ... Test your changes locally. This README.md file will tell you how to do that. Create a pull request, if your article is ready to be published.","title":"Let's share news and useful information with each other"},{"location":"blog/2021-03-16-save-the-date-okd-testing-deployment-workshop.html/","text":"Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now! \u00b6 The OKD Working Group is hosting a virtual workshop on testing and deploying OKD4 \u00b6 On March 20th, OKD-Working Group is hosting a one day event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day will start with all attendees together in the \u2018main stage\u2019 area for 2 hours where we will give an short welcome and describe the logistics for the day, give a brief introduction to OKD4 itself then walk thru a install deployment to vSphere using UPI approach along with a few other more universal best practices such as DNS/DHCP server configuration) that apply to all deployment targets. Then we will break into tracks specific to the deployment target platforms for deep dive demos with Q/A, try and answer any questions you have about your specific deployment target's configurations, identify any missing pieces in the documentation and triage the documentation as we go. There will be 4 track break-out rooms set-up for 3 hours of deployment walk throughs and Q/A with session leads: vSphere/UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat) and Sri Ramanujam (Datto) Our goal is to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release. This is community event NOT meant as a substitute for Red Hat technical support. There is no admission or ticket charge for OKD-Working Group events. However, you are required to complete a free hopin.to platform registration and watch the hopin site for updates about registration and schedule updates. We are committed to fostering an open and welcoming environment at our working group meetings and events. We set expectations for inclusive behavior through our code of conduct and media policies, and are prepared to enforce these. You can Register for the workshop here : https://hopin.com/events/okd-testing-and-deployment-workshop","title":"Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now!"},{"location":"blog/2021-03-16-save-the-date-okd-testing-deployment-workshop.html/#save-the-date-okd-testing-and-deployment-workshop-march-20-register-now","text":"","title":"Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now!"},{"location":"blog/2021-03-16-save-the-date-okd-testing-deployment-workshop.html/#the-okd-working-group-is-hosting-a-virtual-workshop-on-testing-and-deploying-okd4","text":"On March 20th, OKD-Working Group is hosting a one day event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day will start with all attendees together in the \u2018main stage\u2019 area for 2 hours where we will give an short welcome and describe the logistics for the day, give a brief introduction to OKD4 itself then walk thru a install deployment to vSphere using UPI approach along with a few other more universal best practices such as DNS/DHCP server configuration) that apply to all deployment targets. Then we will break into tracks specific to the deployment target platforms for deep dive demos with Q/A, try and answer any questions you have about your specific deployment target's configurations, identify any missing pieces in the documentation and triage the documentation as we go. There will be 4 track break-out rooms set-up for 3 hours of deployment walk throughs and Q/A with session leads: vSphere/UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat) and Sri Ramanujam (Datto) Our goal is to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release. This is community event NOT meant as a substitute for Red Hat technical support. There is no admission or ticket charge for OKD-Working Group events. However, you are required to complete a free hopin.to platform registration and watch the hopin site for updates about registration and schedule updates. We are committed to fostering an open and welcoming environment at our working group meetings and events. We set expectations for inclusive behavior through our code of conduct and media policies, and are prepared to enforce these. You can Register for the workshop here : https://hopin.com/events/okd-testing-and-deployment-workshop","title":"The OKD Working Group is hosting a virtual workshop on testing and deploying OKD4"},{"location":"blog/2021-03-19-please-avoid-using-fcos-33.20210301.3.1.html/","text":"Please avoid using FCOS 33.20210301.3.1 for new OKD installs \u00b6 Hi, Due to several issues ([1] and [2]) fresh installations using FCOS 33.20210301.3.1 would fail. The fix is coming in Podman 3.1.0. Please use an older stable release - 33.20210217.3.0 - as a starting point instead. See download links at https://builds.coreos.fedoraproject.org/browser?stream=stable (might need some scrolling), Note, that only fresh installs are affected. Also, you won't be left with outdated packages, as OKD does update themselves to latest stable FCOS content during installation/update. https://bugzilla.redhat.com/show_bug.cgi?id=1936927 https://github.com/openshift/okd/issues/566 -- Cheers, Vadim","title":"Please avoid using FCOS 33.20210301.3.1 for new OKD installs"},{"location":"blog/2021-03-19-please-avoid-using-fcos-33.20210301.3.1.html/#please-avoid-using-fcos-332021030131-for-new-okd-installs","text":"Hi, Due to several issues ([1] and [2]) fresh installations using FCOS 33.20210301.3.1 would fail. The fix is coming in Podman 3.1.0. Please use an older stable release - 33.20210217.3.0 - as a starting point instead. See download links at https://builds.coreos.fedoraproject.org/browser?stream=stable (might need some scrolling), Note, that only fresh installs are affected. Also, you won't be left with outdated packages, as OKD does update themselves to latest stable FCOS content during installation/update. https://bugzilla.redhat.com/show_bug.cgi?id=1936927 https://github.com/openshift/okd/issues/566 -- Cheers, Vadim","title":"Please avoid using FCOS 33.20210301.3.1 for new OKD installs"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/","text":"Recap OKD Testing and Deployment Workshop - Videos and Additional Resources \u00b6 The OKD Working Group held a virtual community-hosted workshop on testing and deploying OKD4 on March 20th \u00b6 On March 20th, OKD-Working Group hosted a day-long event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day started with all attendees together in the \u2018main stage\u2019 area for 2 hours where community members gave an short welcome along with the following four presentations: What is OKD4 (with a Release Update) - by Charro Gruver (Red Hat) Walk Thru of the OKD Release and Build Processes - Vadim Rutkovsky (Red Hat) Walk Thru of the OKD Deployment and Configuration Guides - Jamie Magiera (UMich) Best Practices such as DNS/DHCP server and Load Balancer Configuration) - Josef Meier (Rohde and Schwarz) Then attendees then broke into track sessions specific to the deployment target platforms for deep dive demos with live Q/A, answered as many questions as possible about that specific deployment target's configurations, attempted to identify any missing pieces in the documentation and triage the documentation as we went along. The 4 track break-out rooms set-up for 2.5 hours of deployment walk throughs and Q/A with session leads: Automated Installation on vSphere UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat), Sri Ramanujam (Datto) and Vadim Rutkovsky(Red Hat) Our goal was to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release. Resources: \u00b6 Link to Playlist OKD Workshop Slides - Charro Gruver DNS DHCP Load Balancer Diagram - Josef Meier","title":"Recap OKD Testing and Deployment Workshop - Videos and Additional Resources"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/#recap-okd-testing-and-deployment-workshop-videos-and-additional-resources","text":"","title":"Recap OKD Testing and Deployment Workshop - Videos and Additional Resources"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/#the-okd-working-group-held-a-virtual-community-hosted-workshop-on-testing-and-deploying-okd4-on-march-20th","text":"On March 20th, OKD-Working Group hosted a day-long event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day started with all attendees together in the \u2018main stage\u2019 area for 2 hours where community members gave an short welcome along with the following four presentations: What is OKD4 (with a Release Update) - by Charro Gruver (Red Hat) Walk Thru of the OKD Release and Build Processes - Vadim Rutkovsky (Red Hat) Walk Thru of the OKD Deployment and Configuration Guides - Jamie Magiera (UMich) Best Practices such as DNS/DHCP server and Load Balancer Configuration) - Josef Meier (Rohde and Schwarz) Then attendees then broke into track sessions specific to the deployment target platforms for deep dive demos with live Q/A, answered as many questions as possible about that specific deployment target's configurations, attempted to identify any missing pieces in the documentation and triage the documentation as we went along. The 4 track break-out rooms set-up for 2.5 hours of deployment walk throughs and Q/A with session leads: Automated Installation on vSphere UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat), Sri Ramanujam (Datto) and Vadim Rutkovsky(Red Hat) Our goal was to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release.","title":"The OKD Working Group held a virtual community-hosted workshop on testing and deploying OKD4 on March 20th"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/#resources","text":"Link to Playlist OKD Workshop Slides - Charro Gruver DNS DHCP Load Balancer Diagram - Josef Meier","title":"Resources:"},{"location":"blog/2021-05-04-From-OKD-to-OpenShift-in-3-Years.html/","text":"Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift \u00b6 From OKD to OpenShift in 3 Years - talk by Josef Meier (Rohde & Schwarz) from OpenShift Commons Gathering at Kubecon \u00b6 On May 4th 2020, OKD-Working Group member Josef Meier gave a wonderful talk about Rohde & Schwarz's Journey to OpenShift 4 from OKD to ARO (Azure Red Hat OpenShift) and discussed benefits of participating in the OKD Working Group! Join the OKD-Working Group and add your voice to the conversation!","title":"Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift"},{"location":"blog/2021-05-04-From-OKD-to-OpenShift-in-3-Years.html/#rohde-schwarzs-journey-to-openshift-4-from-okd-to-azure-red-hat-openshift","text":"","title":"Rohde &amp; Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift"},{"location":"blog/2021-05-04-From-OKD-to-OpenShift-in-3-Years.html/#from-okd-to-openshift-in-3-years-talk-by-josef-meier-rohde-schwarz-from-openshift-commons-gathering-at-kubecon","text":"On May 4th 2020, OKD-Working Group member Josef Meier gave a wonderful talk about Rohde & Schwarz's Journey to OpenShift 4 from OKD to ARO (Azure Red Hat OpenShift) and discussed benefits of participating in the OKD Working Group! Join the OKD-Working Group and add your voice to the conversation!","title":"From OKD to OpenShift in 3 Years - talk by Josef Meier (Rohde &amp; Schwarz) from OpenShift Commons Gathering at Kubecon"},{"location":"blog/2021-05-06-OKD-Office-Hours-at-KubeconEU-on-OpenShiftTV.html/","text":"OKD Working Group Office Hours at KubeconEU on OpenShift.tv \u00b6 Video from OKD Working Group Office Hours at KubeconEU on OpenShift.tv \u00b6 On May 6th 2020, OKD-Working Group members hosted an hour long community led Office Hour with a brief introduction to the latest release by Red Hat's Charro Gruver then live Q/A! Join the OKD-Working Group and add your voice to the conversation!","title":"OKD Working Group Office Hours at KubeconEU on OpenShift.tv"},{"location":"blog/2021-05-06-OKD-Office-Hours-at-KubeconEU-on-OpenShiftTV.html/#okd-working-group-office-hours-at-kubeconeu-on-openshifttv","text":"","title":"OKD Working Group Office Hours at KubeconEU on OpenShift.tv"},{"location":"blog/2021-05-06-OKD-Office-Hours-at-KubeconEU-on-OpenShiftTV.html/#video-from-okd-working-group-office-hours-at-kubeconeu-on-openshifttv","text":"On May 6th 2020, OKD-Working Group members hosted an hour long community led Office Hour with a brief introduction to the latest release by Red Hat's Charro Gruver then live Q/A! Join the OKD-Working Group and add your voice to the conversation!","title":"Video from OKD Working Group Office Hours at KubeconEU on OpenShift.tv"},{"location":"guides/automated-vsphere-upi/","text":"Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI) \u00b6 Introduction \u00b6 It\u2019s possible to completely automate the process of installing OpenShift/OKD on vSphere with User Provisioned Infrastructure by chaining together the various functions of OCT via a wrapper script. Steps \u00b6 Deploy the DNS, DHCP, and load balancer infrastructure outlined in the Prerequisites section. Create an install-config.yaml.template file based on the format outlined in the section Sample install-config.yaml file for VMware vSphere of the OKD docs. Do not add a pull secret. The script will query you for one or it will insert a default one if you use the \u2013auto-secret flag. Create a wrapper script that: Installs the desired FCOS image Downloads the oc and openshift-installer binaries for your desired release version Generates and modifies the ignition files appropriately Builds the cluster nodes Triggers the installation process. Prerequisites \u00b6 DNS \u00b6 1 entry for the bootstrap node of the format bootstrap.[cluster].domain.tld 3 entries for the master nodes of the form master-[n].[cluster].domain.tld An entry for each of the desired worker nodes in the form worker-[n].[cluster].domain.tld 1 entry for the API endpoint in the form api.[cluster].domain.tld 1 entry for the API internal endpoint in the form api-int.[cluster].domain.tld 1 wildcard entry for the Ingress endpoint in the form *.apps.[cluster].domain.tld DHCP \u00b6 Load Balancer \u00b6 vSphere UPI requires the use of a load balancer. There needs to be two pools. API: This pool should contain your master nodes. Ingress: This pool should contain your worker nodes. Proxy (Optional) \u00b6 If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid Wrapper Script \u00b6 #!/bin/bash masters_count = 3 workers_count = 2 template_url = \"https://builds.coreos.fedoraproject.org/prod/streams/testing/builds/33.20210314.2.0/x86_64/fedora-coreos-33.20210314.2.0-vmware.x86_64.ova\" template_name = \"fedora-coreos-33.20210201.2.1-vmware.x86_64\" library = \"Linux ISOs\" cluster_name = \"mycluster\" cluster_folder = \"/MyVSPHERE/vm/Linux/OKD/mycluster\" network_name = \"VM Network\" install_folder = ` pwd ` # Import the template ./oct.sh --import-template --library \" ${ library } \" --template-url \" ${ template_url } \" # Install the desired OKD tools oct.sh --install-tools --release 4 .6 # Launch the prerun to generate and modify the ignition files oct.sh --prerun --auto-secret # Deploy the nodes for the cluster with the appropriate ignition data oct.sh --build --template-name \" ${ template_name } \" --library \" ${ library } \" --cluster-name \" ${ cluster_name } \" --cluster-folder \" ${ cluster_folder } \" --network-name \" ${ network_name } \" --installation-folder \" ${ install_folder } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Turn on the cluster nodes oct.sh --cluster-power on --cluster-name \" ${ cluster_name } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Run the OpenShift installer bin/openshift-install --dir = $( pwd ) wait-for bootstrap-complete --log-level = info Future Updates \u00b6 Generating the install-config template Pull directly from FCOS release feed","title":"Automated vSphere install"},{"location":"guides/automated-vsphere-upi/#implementing-an-automated-installation-solution-for-okd-on-vsphere-with-user-provisioned-infrastructure-upi","text":"","title":"Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI)"},{"location":"guides/automated-vsphere-upi/#introduction","text":"It\u2019s possible to completely automate the process of installing OpenShift/OKD on vSphere with User Provisioned Infrastructure by chaining together the various functions of OCT via a wrapper script.","title":"Introduction"},{"location":"guides/automated-vsphere-upi/#steps","text":"Deploy the DNS, DHCP, and load balancer infrastructure outlined in the Prerequisites section. Create an install-config.yaml.template file based on the format outlined in the section Sample install-config.yaml file for VMware vSphere of the OKD docs. Do not add a pull secret. The script will query you for one or it will insert a default one if you use the \u2013auto-secret flag. Create a wrapper script that: Installs the desired FCOS image Downloads the oc and openshift-installer binaries for your desired release version Generates and modifies the ignition files appropriately Builds the cluster nodes Triggers the installation process.","title":"Steps"},{"location":"guides/automated-vsphere-upi/#prerequisites","text":"","title":"Prerequisites"},{"location":"guides/automated-vsphere-upi/#dns","text":"1 entry for the bootstrap node of the format bootstrap.[cluster].domain.tld 3 entries for the master nodes of the form master-[n].[cluster].domain.tld An entry for each of the desired worker nodes in the form worker-[n].[cluster].domain.tld 1 entry for the API endpoint in the form api.[cluster].domain.tld 1 entry for the API internal endpoint in the form api-int.[cluster].domain.tld 1 wildcard entry for the Ingress endpoint in the form *.apps.[cluster].domain.tld","title":"DNS"},{"location":"guides/automated-vsphere-upi/#dhcp","text":"","title":"DHCP"},{"location":"guides/automated-vsphere-upi/#load-balancer","text":"vSphere UPI requires the use of a load balancer. There needs to be two pools. API: This pool should contain your master nodes. Ingress: This pool should contain your worker nodes.","title":"Load Balancer"},{"location":"guides/automated-vsphere-upi/#proxy-optional","text":"If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid","title":"Proxy (Optional)"},{"location":"guides/automated-vsphere-upi/#wrapper-script","text":"#!/bin/bash masters_count = 3 workers_count = 2 template_url = \"https://builds.coreos.fedoraproject.org/prod/streams/testing/builds/33.20210314.2.0/x86_64/fedora-coreos-33.20210314.2.0-vmware.x86_64.ova\" template_name = \"fedora-coreos-33.20210201.2.1-vmware.x86_64\" library = \"Linux ISOs\" cluster_name = \"mycluster\" cluster_folder = \"/MyVSPHERE/vm/Linux/OKD/mycluster\" network_name = \"VM Network\" install_folder = ` pwd ` # Import the template ./oct.sh --import-template --library \" ${ library } \" --template-url \" ${ template_url } \" # Install the desired OKD tools oct.sh --install-tools --release 4 .6 # Launch the prerun to generate and modify the ignition files oct.sh --prerun --auto-secret # Deploy the nodes for the cluster with the appropriate ignition data oct.sh --build --template-name \" ${ template_name } \" --library \" ${ library } \" --cluster-name \" ${ cluster_name } \" --cluster-folder \" ${ cluster_folder } \" --network-name \" ${ network_name } \" --installation-folder \" ${ install_folder } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Turn on the cluster nodes oct.sh --cluster-power on --cluster-name \" ${ cluster_name } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Run the OpenShift installer bin/openshift-install --dir = $( pwd ) wait-for bootstrap-complete --log-level = info","title":"Wrapper Script"},{"location":"guides/automated-vsphere-upi/#future-updates","text":"Generating the install-config template Pull directly from FCOS release feed","title":"Future Updates"},{"location":"guides/aws-ipi/","text":"AWS IPI Default Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the default options for the installer. Infrastructure \u00b6 Compute \u00b6 3 control plane nodes instance type m4.xlarge , or m5.xlarge if previous not available in the region 3 compute nodes instance type m4.large , or m5.large if previous not available in the region Networking \u00b6 1 virtual private cloud 1 public subnet per availability zone in the region 1 private subnet per availability zone in the region 1 NAT gateway per availability zone 1 elastic IP address per NAT gateway 3 elastic load balancers 1 external network load balancer for the master API server 1 internal network load balancer for the master API server 1 classic load balancer for the router 21 elastic network interfaces, plus 1 interface per availability zone 1 virtual private cloud gateway 10 distinct security groups Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"AWS IPI Default Deployment"},{"location":"guides/aws-ipi/#aws-ipi-default-deployment","text":"This describes the resources used by OpenShift after performing an installation using the default options for the installer.","title":"AWS IPI Default Deployment"},{"location":"guides/aws-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/aws-ipi/#compute","text":"3 control plane nodes instance type m4.xlarge , or m5.xlarge if previous not available in the region 3 compute nodes instance type m4.large , or m5.large if previous not available in the region","title":"Compute"},{"location":"guides/aws-ipi/#networking","text":"1 virtual private cloud 1 public subnet per availability zone in the region 1 private subnet per availability zone in the region 1 NAT gateway per availability zone 1 elastic IP address per NAT gateway 3 elastic load balancers 1 external network load balancer for the master API server 1 internal network load balancer for the master API server 1 classic load balancer for the router 21 elastic network interfaces, plus 1 interface per availability zone 1 virtual private cloud gateway 10 distinct security groups","title":"Networking"},{"location":"guides/aws-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/azure-ipi/","text":"Azure IPI Default Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the default options for the installer. Infrastructure \u00b6 Compute \u00b6 3 control plane nodes instance type Standard_D8s_v3 3 compute nodes instance type Standard_D4s_v3 Networking \u00b6 1 virtual network (VNet) containing 2 subnets 6 network interfaces -3 network load balancers 1 public for compute node access 1 private for control plane access 1 public for control plane access 2 public IP addresses 1 for the public compute load balancer 1 for the public control plane load balancer 7 private IP addresses 1 per control plane node 1 per compute node 1 for the private control plane load balancer 2 network security groups 1 for control plane allowing traffic on port 6443 from anywhere 1 for compute allowing traffic on ports 80 and 443 from the internet Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"Azure IPI Default Deployment"},{"location":"guides/azure-ipi/#azure-ipi-default-deployment","text":"This describes the resources used by OpenShift after performing an installation using the default options for the installer.","title":"Azure IPI Default Deployment"},{"location":"guides/azure-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/azure-ipi/#compute","text":"3 control plane nodes instance type Standard_D8s_v3 3 compute nodes instance type Standard_D4s_v3","title":"Compute"},{"location":"guides/azure-ipi/#networking","text":"1 virtual network (VNet) containing 2 subnets 6 network interfaces -3 network load balancers 1 public for compute node access 1 private for control plane access 1 public for control plane access 2 public IP addresses 1 for the public compute load balancer 1 for the public control plane load balancer 7 private IP addresses 1 per control plane node 1 per compute node 1 for the private control plane load balancer 2 network security groups 1 for control plane allowing traffic on port 6443 from anywhere 1 for compute allowing traffic on ports 80 and 443 from the internet","title":"Networking"},{"location":"guides/azure-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/gcp-ipi/","text":"GCP IPI Default Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the default options for the installer. Infrastructure \u00b6 Compute \u00b6 3 control plane nodes instance type n1-standard-4 3 compute nodes instance type n1-standard-2 1 image Networking \u00b6 2 networks 2 subnetworks 3 static IP addresses 1 router 2 routes 3 target pools 10 firewall rules 2 forwarding rules 3 in-use global IP addresses 3 health checks Platform \u00b6 5 IAM service accounts Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"GCP IPI Default Deployment"},{"location":"guides/gcp-ipi/#gcp-ipi-default-deployment","text":"This describes the resources used by OpenShift after performing an installation using the default options for the installer.","title":"GCP IPI Default Deployment"},{"location":"guides/gcp-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/gcp-ipi/#compute","text":"3 control plane nodes instance type n1-standard-4 3 compute nodes instance type n1-standard-2 1 image","title":"Compute"},{"location":"guides/gcp-ipi/#networking","text":"2 networks 2 subnetworks 3 static IP addresses 1 router 2 routes 3 target pools 10 firewall rules 2 forwarding rules 3 in-use global IP addresses 3 health checks","title":"Networking"},{"location":"guides/gcp-ipi/#platform","text":"5 IAM service accounts","title":"Platform"},{"location":"guides/gcp-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/overview/","text":"Deployment Guides \u00b6 The guides linked below provide some examples of how community members are using OKD and provide details of the underlying hardware and platform configurations they are using. Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI) Prerequisites for vSphere UPI vSphere Installer Provisioned Infrastructure Deployment Single Node OKD Installation Vadim's homelab Sri's homelab Azure Installer Provisioned Infrastructure Default Deployment AWS Installer Provisioned Infrastructure Default Deployment GCP Installer Provisioned Infrastructure Default Deployment Guide to installing OKD Virtualization on Bare Metal UPI","title":"Overview"},{"location":"guides/overview/#deployment-guides","text":"The guides linked below provide some examples of how community members are using OKD and provide details of the underlying hardware and platform configurations they are using. Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI) Prerequisites for vSphere UPI vSphere Installer Provisioned Infrastructure Deployment Single Node OKD Installation Vadim's homelab Sri's homelab Azure Installer Provisioned Infrastructure Default Deployment AWS Installer Provisioned Infrastructure Default Deployment GCP Installer Provisioned Infrastructure Default Deployment Guide to installing OKD Virtualization on Bare Metal UPI","title":"Deployment Guides"},{"location":"guides/sno/","text":"Single Node OKD Installation \u00b6 This document outlines how to deploy a single node OKD cluster using virt. Requirements \u00b6 Host with a minimal CentOS Stream, Fedora, or CentOS-8 installed ( do not create a /home filesystem ) Monitor, mouse, and keyboard attached to the host Static IP for the host The following packages installed: virt, wget, git, net-tools, bind, bind-utils, bash-completion, rsync, libguestfs-tools, virt-install, epel-release, libvirt-devel, httpd-tools, snf, nginx Procedure \u00b6 For the complete procedure, please see Building an OKD4 single node cluster with minimal resources","title":"Single Node OKD Installation"},{"location":"guides/sno/#single-node-okd-installation","text":"This document outlines how to deploy a single node OKD cluster using virt.","title":"Single Node OKD Installation"},{"location":"guides/sno/#requirements","text":"Host with a minimal CentOS Stream, Fedora, or CentOS-8 installed ( do not create a /home filesystem ) Monitor, mouse, and keyboard attached to the host Static IP for the host The following packages installed: virt, wget, git, net-tools, bind, bind-utils, bash-completion, rsync, libguestfs-tools, virt-install, epel-release, libvirt-devel, httpd-tools, snf, nginx","title":"Requirements"},{"location":"guides/sno/#procedure","text":"For the complete procedure, please see Building an OKD4 single node cluster with minimal resources","title":"Procedure"},{"location":"guides/sri/","text":"Sri's Overkill Homelab Setup \u00b6 This document lays out the resources used to create my completely-overkill homelab. This cluster provides all the compute and storage I think I'll need for the foreseeable future, and the CPU, RAM, and storage can all be scaled vertically independently of each other. Not that I think I'll need to do that for a while. More detail into the deployment and my homelab's Terraform configuration can be found here . Hardware \u00b6 3 hyper-converged hypervisors Ryzen 5 3600 64 GiB RAM 3x 4TiB HDD 2x 500GiB SSD in RAID1 1x 256GiB NVME for boot disk 1 NUC I had laying around gathering dust Intel Core i3-5010U 16 GiB RAM 500GiB SSD Main cluster \u00b6 My hypervisors each host an identical workload. The total size of this cluster is 3 control plane nodes, and 9 worker nodes. So it splits very nicely three ways. Each hypervisor hosts 1 control plane VM and 3 worker VMs. 3 control plane nodes 4x CPU 10 GiB RAM 50 GiB disk 9 worker nodes 8x CPU 16 GiB RAM 50 GiB root disk 4 TiB HDD for workload use 1 bootstrap node (temporary, taken down after initial setup is complete) 4 vCPU 8 GiB RAM 120 GiB root disk Supporting infrastructure \u00b6 Networking \u00b6 OKD, and especially baremetal UPI OKD, requires a very specific network setup. You will most likely need something more flexible than your ISP's router to get everything fully configured. The documentation is very clear on the various DNS records and DHCP static allocations you will need to make, so I won't go into them here. However, there are a couple extra things that you may want to set for best results. In particular, I make sure that I have PTR records set up for all my cluster nodes. This is extremely important as the nodes need a correct PTR record set up for them to auto-discover their hostname. Clusters typically do not set themselves up properly if there are hostname collisions! API load balancer \u00b6 I run a separate smaller VM on the NUC as a single-purpose load balancer appliance, running HAProxy. 1 load balancer VM 2x vCPU 256MiB RAM 10GiB disk The HAProxy config is straightforward. I adapted mine from the example config file created by the ocp4-helpernode playbook. Deployment \u00b6 I create the VMs on the hypervisors using Terraform. The Terraform Libvirt provider is very, very cool. It's also used by openshift-install for its Libvirt-based deployments, so it supports everything needed to deploy OKD nodes. Most importantly, I can use Terraform to supply the VMs with their Ignition configs, which means I don't have to worry about passing kernel args manually or setting up a PXE server to get things going like the official OKD docs would have you do. Terraform also makes it easy to tear down the cluster and reset in case something goes wrong. Post-Bootstrap One-Time Setup \u00b6 Storage with Rook and Ceph \u00b6 I deploy a Ceph cluster into OKD using Rook. The Rook configuration deploys OSDs on top of the 4TiB HDDs assigned to each worker. I deploy an erasure-coded CephFS pool (6+2) for RWX workloads and a 3x replica block pool for RWO workloads. Monitoring and Alerting \u00b6 OKD comes with a very comprehensive monitoring and alerting suite, and it would be a shame not to take advantage of it. I set up an Alertmanager webhook to send any alerts to a small program I wrote that posts the alerts to Discord . I also deploy a Prometheus + Grafana set up into the cluster that collects metrics from the various hypervisors and supporting infrastructure VMs. I use Grafana's built-in Discord alerting mechanism to post those alerts. LoadBalancer with MetalLB \u00b6 MetalLB is a piece of fantastic software that allows on-prem or otherwise non-public-cloud Kubernetes clusters to enjoy the luxury of LoadBalancer type services. It's dead simple to set up and makes you feel you're in a real datacenter. I deploy several workloads that don't use standard HTTP and so can't be deployed behind a Route . Without MetalLB, I wouldn't be able to deploy these workloads on OKD at all but with it, I can! Software I Run \u00b6 I maintain an ansible playbook that handles deploying my workloads into the cluster. I prefer Ansible over other tools like Helm because it has more robust capabilities to store secrets, I find its templating capabilities more flexible and powerful than Helm's (especially when it comes to inlining config files into config maps or creating templated Dockerfiles for BuildConfigs), and because I am already familiar with Ansible and know how it works. paperless-ng - A document organizer that uses machine learning to automatically classify and organize bitwarden_rs - Password manager Jellyfin - Media management Samba - I joined a StatefulSet to my AD domain and it serves an authenticated SMB share Netbox - Infrastructure management tool Quassel - IRC bouncer Ukulele - Bot that plays music into Discord channels RPM and deb package repos for internal packages","title":"Sri's homelab"},{"location":"guides/sri/#sris-overkill-homelab-setup","text":"This document lays out the resources used to create my completely-overkill homelab. This cluster provides all the compute and storage I think I'll need for the foreseeable future, and the CPU, RAM, and storage can all be scaled vertically independently of each other. Not that I think I'll need to do that for a while. More detail into the deployment and my homelab's Terraform configuration can be found here .","title":"Sri's Overkill Homelab Setup"},{"location":"guides/sri/#hardware","text":"3 hyper-converged hypervisors Ryzen 5 3600 64 GiB RAM 3x 4TiB HDD 2x 500GiB SSD in RAID1 1x 256GiB NVME for boot disk 1 NUC I had laying around gathering dust Intel Core i3-5010U 16 GiB RAM 500GiB SSD","title":"Hardware"},{"location":"guides/sri/#main-cluster","text":"My hypervisors each host an identical workload. The total size of this cluster is 3 control plane nodes, and 9 worker nodes. So it splits very nicely three ways. Each hypervisor hosts 1 control plane VM and 3 worker VMs. 3 control plane nodes 4x CPU 10 GiB RAM 50 GiB disk 9 worker nodes 8x CPU 16 GiB RAM 50 GiB root disk 4 TiB HDD for workload use 1 bootstrap node (temporary, taken down after initial setup is complete) 4 vCPU 8 GiB RAM 120 GiB root disk","title":"Main cluster"},{"location":"guides/sri/#supporting-infrastructure","text":"","title":"Supporting infrastructure"},{"location":"guides/sri/#networking","text":"OKD, and especially baremetal UPI OKD, requires a very specific network setup. You will most likely need something more flexible than your ISP's router to get everything fully configured. The documentation is very clear on the various DNS records and DHCP static allocations you will need to make, so I won't go into them here. However, there are a couple extra things that you may want to set for best results. In particular, I make sure that I have PTR records set up for all my cluster nodes. This is extremely important as the nodes need a correct PTR record set up for them to auto-discover their hostname. Clusters typically do not set themselves up properly if there are hostname collisions!","title":"Networking"},{"location":"guides/sri/#api-load-balancer","text":"I run a separate smaller VM on the NUC as a single-purpose load balancer appliance, running HAProxy. 1 load balancer VM 2x vCPU 256MiB RAM 10GiB disk The HAProxy config is straightforward. I adapted mine from the example config file created by the ocp4-helpernode playbook.","title":"API load balancer"},{"location":"guides/sri/#deployment","text":"I create the VMs on the hypervisors using Terraform. The Terraform Libvirt provider is very, very cool. It's also used by openshift-install for its Libvirt-based deployments, so it supports everything needed to deploy OKD nodes. Most importantly, I can use Terraform to supply the VMs with their Ignition configs, which means I don't have to worry about passing kernel args manually or setting up a PXE server to get things going like the official OKD docs would have you do. Terraform also makes it easy to tear down the cluster and reset in case something goes wrong.","title":"Deployment"},{"location":"guides/sri/#post-bootstrap-one-time-setup","text":"","title":"Post-Bootstrap One-Time Setup"},{"location":"guides/sri/#storage-with-rook-and-ceph","text":"I deploy a Ceph cluster into OKD using Rook. The Rook configuration deploys OSDs on top of the 4TiB HDDs assigned to each worker. I deploy an erasure-coded CephFS pool (6+2) for RWX workloads and a 3x replica block pool for RWO workloads.","title":"Storage with Rook and Ceph"},{"location":"guides/sri/#monitoring-and-alerting","text":"OKD comes with a very comprehensive monitoring and alerting suite, and it would be a shame not to take advantage of it. I set up an Alertmanager webhook to send any alerts to a small program I wrote that posts the alerts to Discord . I also deploy a Prometheus + Grafana set up into the cluster that collects metrics from the various hypervisors and supporting infrastructure VMs. I use Grafana's built-in Discord alerting mechanism to post those alerts.","title":"Monitoring and Alerting"},{"location":"guides/sri/#loadbalancer-with-metallb","text":"MetalLB is a piece of fantastic software that allows on-prem or otherwise non-public-cloud Kubernetes clusters to enjoy the luxury of LoadBalancer type services. It's dead simple to set up and makes you feel you're in a real datacenter. I deploy several workloads that don't use standard HTTP and so can't be deployed behind a Route . Without MetalLB, I wouldn't be able to deploy these workloads on OKD at all but with it, I can!","title":"LoadBalancer with MetalLB"},{"location":"guides/sri/#software-i-run","text":"I maintain an ansible playbook that handles deploying my workloads into the cluster. I prefer Ansible over other tools like Helm because it has more robust capabilities to store secrets, I find its templating capabilities more flexible and powerful than Helm's (especially when it comes to inlining config files into config maps or creating templated Dockerfiles for BuildConfigs), and because I am already familiar with Ansible and know how it works. paperless-ng - A document organizer that uses machine learning to automatically classify and organize bitwarden_rs - Password manager Jellyfin - Media management Samba - I joined a StatefulSet to my AD domain and it serves an authenticated SMB share Netbox - Infrastructure management tool Quassel - IRC bouncer Ukulele - Bot that plays music into Discord channels RPM and deb package repos for internal packages","title":"Software I Run"},{"location":"guides/vadim/","text":"Vadim's homelab \u00b6 This describes the resources used by OpenShift after performing an installation to make it similar to my homelab setup. Compute \u00b6 Ubiquity EdgeRouter ER-X runs DHCP (embedded), custom DNS server via AdGuard NAS/Bastion host haproxy for loadbalancer ceph cluster for PVs NFS server for shared data control plane Intel i5 CPU, 16+4 GB RAM 120 GB NVME disk compute nodes Lenovo X220 laptop Router setup \u00b6 Once nodes have booted assign static IPs using MAC pinning. EdgeRouter has dnsmasq to support custom DNS entries, but I wanted to have a network-wide ad filtering and DNS-over-TLS for free, so I followed this guide to install AdGuard Home on the router. This gives a fancy UI for DNS rewrites and gives a useful stats about the nodes on the network. NAS/Bastion setup \u00b6 HAProxy setup is fairly standard - see ocp4-helpernode for idea. Along with (fairly standard) NFS server I also run a single node Ceph cluster, so that I could benefit from CSI / autoprovision / snapshots etc. Installation \u00b6 Currently \"single node install\" requires a dedicated throwaway bootstrap node, so I used future compute node (x220 laptop) as a bootstrap node. Once master was installed, the laptop was re-provisioned to become a compute node. Upgrading \u00b6 Since I use a single master install, upgrades are bit complicated. Both nodes are labelled as workers, so upgrading those is not an issue. Upgrading single master is tricky, so I use this script to pivot the node into expected master ignition content, which runs rpm-ostree rebase <new content> . This script needs to be cancelled before it starts installing OS extensions (NetworkManager-ovs etc.) as its necessary. This issue as a class would be addressed in 4.8. Useful software \u00b6 Grafana operator is incredibly useful to setup monitoring. This operator helps me to define a configuration for various datasources (i.e. Promtail+Loki ) and control dashboard source code using CRs. SnapScheduler makes periodic snapshots of some PVs so that risky changes could be reverted. Tekton operator is helping me to run a few clean up jobs in cluster periodically. Most useful pipeline I've been using is running oc adm must-gather on this cluster, unpacking it and storing it in Git. This helps me keep track of changes in the cluster in a git repo - and, unlike gitops solution like ArgoCD - I can still tinker with things in the console. Other useful software running in my cluster: Gitea - git server HomeAssistant - controls smart home devices BitWarden_rs - password storage Minio - S3-like storage Nextcloud - file sync software Navidrome - music server MiniFlux - RSS reader Matrix Synapse - federated chat app Pleroma - federated microblogging app Wallabag - Read-It-Later app","title":"Vadim's homelab"},{"location":"guides/vadim/#vadims-homelab","text":"This describes the resources used by OpenShift after performing an installation to make it similar to my homelab setup.","title":"Vadim's homelab"},{"location":"guides/vadim/#compute","text":"Ubiquity EdgeRouter ER-X runs DHCP (embedded), custom DNS server via AdGuard NAS/Bastion host haproxy for loadbalancer ceph cluster for PVs NFS server for shared data control plane Intel i5 CPU, 16+4 GB RAM 120 GB NVME disk compute nodes Lenovo X220 laptop","title":"Compute"},{"location":"guides/vadim/#router-setup","text":"Once nodes have booted assign static IPs using MAC pinning. EdgeRouter has dnsmasq to support custom DNS entries, but I wanted to have a network-wide ad filtering and DNS-over-TLS for free, so I followed this guide to install AdGuard Home on the router. This gives a fancy UI for DNS rewrites and gives a useful stats about the nodes on the network.","title":"Router setup"},{"location":"guides/vadim/#nasbastion-setup","text":"HAProxy setup is fairly standard - see ocp4-helpernode for idea. Along with (fairly standard) NFS server I also run a single node Ceph cluster, so that I could benefit from CSI / autoprovision / snapshots etc.","title":"NAS/Bastion setup"},{"location":"guides/vadim/#installation","text":"Currently \"single node install\" requires a dedicated throwaway bootstrap node, so I used future compute node (x220 laptop) as a bootstrap node. Once master was installed, the laptop was re-provisioned to become a compute node.","title":"Installation"},{"location":"guides/vadim/#upgrading","text":"Since I use a single master install, upgrades are bit complicated. Both nodes are labelled as workers, so upgrading those is not an issue. Upgrading single master is tricky, so I use this script to pivot the node into expected master ignition content, which runs rpm-ostree rebase <new content> . This script needs to be cancelled before it starts installing OS extensions (NetworkManager-ovs etc.) as its necessary. This issue as a class would be addressed in 4.8.","title":"Upgrading"},{"location":"guides/vadim/#useful-software","text":"Grafana operator is incredibly useful to setup monitoring. This operator helps me to define a configuration for various datasources (i.e. Promtail+Loki ) and control dashboard source code using CRs. SnapScheduler makes periodic snapshots of some PVs so that risky changes could be reverted. Tekton operator is helping me to run a few clean up jobs in cluster periodically. Most useful pipeline I've been using is running oc adm must-gather on this cluster, unpacking it and storing it in Git. This helps me keep track of changes in the cluster in a git repo - and, unlike gitops solution like ArgoCD - I can still tinker with things in the console. Other useful software running in my cluster: Gitea - git server HomeAssistant - controls smart home devices BitWarden_rs - password storage Minio - S3-like storage Nextcloud - file sync software Navidrome - music server MiniFlux - RSS reader Matrix Synapse - federated chat app Pleroma - federated microblogging app Wallabag - Read-It-Later app","title":"Useful software"},{"location":"guides/vsphere-ipi/","text":"vSphere IPI Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the required options for the installer. Infrastructure \u00b6 Compute \u00b6 All vms stored within folder described above and tagged with tag created by installer. 3 control plane vms (name format: {cluster name}-{generated cluster id}-master-{0,1,2} ) 4 vCPU 16 GB RAM 120 GB storage 3 worker vms (name format: {cluster name}-{generated cluster id}-master-{generated worker id} ) 2 vCPU 8 GB RAM 120 GB storage Networking \u00b6 Should be set up by user. Installer doesn't create anything there. Network name should be provided as installer argument. Miscellaneous \u00b6 tag category with format openshift-{cluster name}-{generated cluster id} tag with format {cluster name}-{generated cluster id} folder with title format {cluster name}-{generated cluster id} disabled virtual machine with name {cluster name}-rhcos-{generated cluster id} which using as template for further scaling Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"vSphere IPI Deployment"},{"location":"guides/vsphere-ipi/#vsphere-ipi-deployment","text":"This describes the resources used by OpenShift after performing an installation using the required options for the installer.","title":"vSphere IPI Deployment"},{"location":"guides/vsphere-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/vsphere-ipi/#compute","text":"All vms stored within folder described above and tagged with tag created by installer. 3 control plane vms (name format: {cluster name}-{generated cluster id}-master-{0,1,2} ) 4 vCPU 16 GB RAM 120 GB storage 3 worker vms (name format: {cluster name}-{generated cluster id}-master-{generated worker id} ) 2 vCPU 8 GB RAM 120 GB storage","title":"Compute"},{"location":"guides/vsphere-ipi/#networking","text":"Should be set up by user. Installer doesn't create anything there. Network name should be provided as installer argument.","title":"Networking"},{"location":"guides/vsphere-ipi/#miscellaneous","text":"tag category with format openshift-{cluster name}-{generated cluster id} tag with format {cluster name}-{generated cluster id} folder with title format {cluster name}-{generated cluster id} disabled virtual machine with name {cluster name}-rhcos-{generated cluster id} which using as template for further scaling","title":"Miscellaneous"},{"location":"guides/vsphere-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/vsphere-prereqs/","text":"Prerequisites for vSphere UPI \u00b6 In this example I describe the setup of a DNS/DHCP server and a Load Balancer on a Raspberry PI microcomputer. The instructions most certainly will also work for other environments. I use Raspberry Pi OS (debian based). IP Addresses of components in this example \u00b6 Homelab subnet: 192.168.178.0/24 DSL router/gateway: 192.168.178.1 IP address of Raspberry Pi (DHCP/DNS/Load Balancer): 192.168.178.5 local domain: homelab.net local cluster (name: c1) domain: c1.homelab.net DHCP range: 192.168.178.40 \u2026 192.168.178.199 Static IPs for OKD\u2019s bootstrap, masters and workers Upgrade Raspberry Pi \u00b6 sudo apt-get update sudo apt-get upgrade sudo reboot Set static IP address on Raspberry Pi \u00b6 Add this: interface eth0 static ip_address=192.168.178.5/24 static routers=192.168.178.1 static domain_name_servers=192.168.178.5 8.8.8.8 to /etc/dhcpcd.conf DHCP \u00b6 Ensure that no other DHCP servers are activated in the network of your homelab e.g. in your internet router. The DHCP server in this example is setup with DDNS (Dynamic DNS) enabled. Install \u00b6 sudo apt-get install isc-dhcp-server Configure \u00b6 Enable DHCP server for IPv4 on eth0: /etc/default/isc-dhcp-server INTERFACESv4=\"eth0\" INTERFACESv6=\"\" /etc/dhcp/dhcpd.conf # dhcpd.conf # #################################################################################### # Configuration for Dynamic DNS (DDNS) updates # # Clients requesting an IP and sending their hostname for domain *.homelab.net # # will be auto registered in the DNS server. # #################################################################################### ddns-updates on; ddns-update-style standard; # This option points to the copy rndc.key we created for bind9. include \"/etc/bind/rndc.key\"; allow unknown-clients; use-host-decl-names on; default-lease-time 300; # 5 minutes max-lease-time 300; # 5 minutes # homelab.net DNS zones zone homelab.net. { primary 192.168.178.5; # This server is the primary DNS server for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } zone 178.168.192.in-addr.arpa. { primary 192.168.178.5; # This server is the primary reverse DNS for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } ddns-domainname \"homelab.net.\"; ddns-rev-domainname \"in-addr.arpa.\"; #################################################################################### #################################################################################### # Basic configuration # #################################################################################### # option definitions common to all supported networks... default-lease-time 300; max-lease-time 300; # If this DHCP server is the official DHCP server for the local # network, the authoritative directive should be uncommented. authoritative; # Parts of this section will be put in the /etc/resolv.conf of your hosts later option domain-name \"homelab.net\"; option routers 192.168.178.1; option subnet-mask 255.255.255.0; option domain-name-servers 192.168.178.5; subnet 192.168.178.0 netmask 255.255.255.0 { range 192.168.178.40 192.168.178.199; } #################################################################################### #################################################################################### # Static IP addresses # # (Replace the MAC addresses here with the ones you set in vsphere for your vms) # #################################################################################### group { host bootstrap { hardware ethernet 00:1c:00:00:00:00; fixed-address 192.168.178.200; } host master0 { hardware ethernet 00:1c:00:00:00:10; fixed-address 192.168.178.210; } host master1 { hardware ethernet 00:1c:00:00:00:11; fixed-address 192.168.178.211; } host master2 { hardware ethernet 00:1c:00:00:00:12; fixed-address 192.168.178.212; } host worker0 { hardware ethernet 00:1c:00:00:00:20; fixed-address 192.168.178.220; } host worker1 { hardware ethernet 00:1c:00:00:00:21; fixed-address 192.168.178.221; } host worker2 { hardware ethernet 00:1c:00:00:00:22; fixed-address 192.168.178.222; } } DNS \u00b6 Install \u00b6 sudo apt install bind9 dnsutils Basic configuration \u00b6 /etc/bind/named.conf.options include \"/etc/bind/rndc.key\"; acl internals { // lo adapter 127.0.0.1; // CIDR for your homelab network 192.168.178.0/24; }; options { directory \"/var/cache/bind\"; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. forwarders { 8.8.8.8; 8.8.4.4; }; forward only; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation no; listen-on-v6 { none; }; auth-nxdomain no; listen-on port 53 { any; }; // Allow queries from my Homelab and also from Wireguard Clients. allow-query { internals; }; allow-query-cache { internals; }; allow-update { internals; }; recursion yes; allow-recursion { internals; }; allow-transfer { internals; }; dnssec-enable no; check-names master ignore; check-names slave ignore; check-names response ignore; }; /etc/bind/named.conf.local #include \"/etc/bind/rndc.key\"; // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; # All devices that don't belong to the OKD cluster will be maintained here. zone \"homelab.net\" { type master; file \"/etc/bind/forward.homelab.net\"; allow-update { key rndc-key; }; }; zone \"c1.homelab.net\" { type master; file \"/etc/bind/forward.c1.homelab.net\"; allow-update { key rndc-key; }; }; zone \"178.168.192.in-addr.arpa\" { type master; notify no; file \"/etc/bind/178.168.192.in-addr.arpa\"; allow-update { key rndc-key; }; }; Zone file for homlab.net: /etc/bind/forward.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA homelab.net. root.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 The name of the next file depends on the subnet that is used: /etc/bind/178.168.192.in-addr.arpa $TTL 1W @ IN SOA ns1.homelab.net. root.homelab.net. ( 2019070742 ; serial 10800 ; refresh (3 hours) 1800 ; retry (30 minutes) 1209600 ; expire (2 weeks) 604800 ; minimum (1 week) ) NS ns1.homelab.net. 200 PTR bootstrap.c1.homelab.net. 210 PTR master0.c1.homelab.net. 211 PTR master1.c1.homelab.net. 212 PTR master2.c1.homelab.net. 220 PTR worker0.c1.homelab.net. 221 PTR worker1.c1.homelab.net. 222 PTR worker2.c1.homelab.net. 5 PTR api.c1.homelab.net. 5 PTR api-int.c1.homelab.net. DNS records for OKD 4 \u00b6 Zone file for c1.homelab.net (our OKD 4 cluster will be in this domain): /etc/bind/forward.c1.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA c1.homelab.net. root.c1.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS c1.homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 load-balancer IN A 192.168.178.5 bootstrap IN A 192.168.178.200 master0 IN A 192.168.178.210 master1 IN A 192.168.178.211 master2 IN A 192.168.178.212 worker0 IN A 192.168.178.220 worker1 IN A 192.168.178.221 worker2 IN A 192.168.178.222 worker3 IN A 192.168.178.223 *.apps.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api-int.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. Set file permissions \u00b6 For dynamic DNS (ddns) to work you should do this: sudo chown -R bind:bind /etc/bind Load Balancer \u00b6 Install \u00b6 sudo apt-get install haproxy Configure \u00b6 /etc/haproxy/haproxy.cfg global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from: # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 20000 timeout client 10000 timeout server 10000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # You can see the stats and observe OKD's bootstrap process by opening # http://<IP>:4321/haproxy?stats listen stats bind :4321 mode http log global maxconn 10 timeout client 100s timeout server 100s timeout connect 100s timeout queue 100s stats enable stats hide-version stats refresh 30s stats show-node stats auth admin:password stats uri /haproxy?stats frontend openshift-api-server bind *:6443 default_backend openshift-api-server mode tcp option tcplog backend openshift-api-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:6443 check server master0 master0.c1.homelab.net:6443 check server master1 master1.c1.homelab.net:6443 check server master2 master2.c1.homelab.net:6443 check frontend machine-config-server bind *:22623 default_backend machine-config-server mode tcp option tcplog backend machine-config-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:22623 check server master0 master0.c1.homelab.net:22623 check server master1 master1.c1.homelab.net:22623 check server master2 master2.c1.homelab.net:22623 check frontend ingress-http bind *:80 default_backend ingress-http mode tcp option tcplog backend ingress-http balance source mode tcp server master0 master0.c1.homelab.net:80 check server master1 master1.c1.homelab.net:80 check server master2 master2.c1.homelab.net:80 check server worker0 worker0.c1.homelab.net:80 check server worker1 worker1.c1.homelab.net:80 check server worker2 worker2.c1.homelab.net:80 check server worker3 worker3.c1.homelab.net:80 check frontend ingress-https bind *:443 default_backend ingress-https mode tcp option tcplog backend ingress-https balance source mode tcp server master0 master0.c1.homelab.net:443 check server master1 master1.c1.homelab.net:443 check server master2 master2.c1.homelab.net:443 check server worker0 worker0.c1.homelab.net:443 check server worker1 worker1.c1.homelab.net:443 check server worker2 worker2.c1.homelab.net:443 check server worker3 worker3.c1.homelab.net:443 check Reboot and check status \u00b6 Reboot Raspberry Pi: sudo reboot Check status of DNS/DHCP server and Load Balancer: sudo systemctl status haproxy.service sudo systemctl status isc-dhcp-server.service sudo systemctl status bind9 Proxy (if on a private network) \u00b6 If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid","title":"Prerequites for vSphere UPI"},{"location":"guides/vsphere-prereqs/#prerequisites-for-vsphere-upi","text":"In this example I describe the setup of a DNS/DHCP server and a Load Balancer on a Raspberry PI microcomputer. The instructions most certainly will also work for other environments. I use Raspberry Pi OS (debian based).","title":"Prerequisites for vSphere UPI"},{"location":"guides/vsphere-prereqs/#ip-addresses-of-components-in-this-example","text":"Homelab subnet: 192.168.178.0/24 DSL router/gateway: 192.168.178.1 IP address of Raspberry Pi (DHCP/DNS/Load Balancer): 192.168.178.5 local domain: homelab.net local cluster (name: c1) domain: c1.homelab.net DHCP range: 192.168.178.40 \u2026 192.168.178.199 Static IPs for OKD\u2019s bootstrap, masters and workers","title":"IP Addresses of components in this example"},{"location":"guides/vsphere-prereqs/#upgrade-raspberry-pi","text":"sudo apt-get update sudo apt-get upgrade sudo reboot","title":"Upgrade Raspberry Pi"},{"location":"guides/vsphere-prereqs/#set-static-ip-address-on-raspberry-pi","text":"Add this: interface eth0 static ip_address=192.168.178.5/24 static routers=192.168.178.1 static domain_name_servers=192.168.178.5 8.8.8.8 to /etc/dhcpcd.conf","title":"Set static IP address on Raspberry Pi"},{"location":"guides/vsphere-prereqs/#dhcp","text":"Ensure that no other DHCP servers are activated in the network of your homelab e.g. in your internet router. The DHCP server in this example is setup with DDNS (Dynamic DNS) enabled.","title":"DHCP"},{"location":"guides/vsphere-prereqs/#install","text":"sudo apt-get install isc-dhcp-server","title":"Install"},{"location":"guides/vsphere-prereqs/#configure","text":"Enable DHCP server for IPv4 on eth0: /etc/default/isc-dhcp-server INTERFACESv4=\"eth0\" INTERFACESv6=\"\" /etc/dhcp/dhcpd.conf # dhcpd.conf # #################################################################################### # Configuration for Dynamic DNS (DDNS) updates # # Clients requesting an IP and sending their hostname for domain *.homelab.net # # will be auto registered in the DNS server. # #################################################################################### ddns-updates on; ddns-update-style standard; # This option points to the copy rndc.key we created for bind9. include \"/etc/bind/rndc.key\"; allow unknown-clients; use-host-decl-names on; default-lease-time 300; # 5 minutes max-lease-time 300; # 5 minutes # homelab.net DNS zones zone homelab.net. { primary 192.168.178.5; # This server is the primary DNS server for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } zone 178.168.192.in-addr.arpa. { primary 192.168.178.5; # This server is the primary reverse DNS for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } ddns-domainname \"homelab.net.\"; ddns-rev-domainname \"in-addr.arpa.\"; #################################################################################### #################################################################################### # Basic configuration # #################################################################################### # option definitions common to all supported networks... default-lease-time 300; max-lease-time 300; # If this DHCP server is the official DHCP server for the local # network, the authoritative directive should be uncommented. authoritative; # Parts of this section will be put in the /etc/resolv.conf of your hosts later option domain-name \"homelab.net\"; option routers 192.168.178.1; option subnet-mask 255.255.255.0; option domain-name-servers 192.168.178.5; subnet 192.168.178.0 netmask 255.255.255.0 { range 192.168.178.40 192.168.178.199; } #################################################################################### #################################################################################### # Static IP addresses # # (Replace the MAC addresses here with the ones you set in vsphere for your vms) # #################################################################################### group { host bootstrap { hardware ethernet 00:1c:00:00:00:00; fixed-address 192.168.178.200; } host master0 { hardware ethernet 00:1c:00:00:00:10; fixed-address 192.168.178.210; } host master1 { hardware ethernet 00:1c:00:00:00:11; fixed-address 192.168.178.211; } host master2 { hardware ethernet 00:1c:00:00:00:12; fixed-address 192.168.178.212; } host worker0 { hardware ethernet 00:1c:00:00:00:20; fixed-address 192.168.178.220; } host worker1 { hardware ethernet 00:1c:00:00:00:21; fixed-address 192.168.178.221; } host worker2 { hardware ethernet 00:1c:00:00:00:22; fixed-address 192.168.178.222; } }","title":"Configure"},{"location":"guides/vsphere-prereqs/#dns","text":"","title":"DNS"},{"location":"guides/vsphere-prereqs/#install_1","text":"sudo apt install bind9 dnsutils","title":"Install"},{"location":"guides/vsphere-prereqs/#basic-configuration","text":"/etc/bind/named.conf.options include \"/etc/bind/rndc.key\"; acl internals { // lo adapter 127.0.0.1; // CIDR for your homelab network 192.168.178.0/24; }; options { directory \"/var/cache/bind\"; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. forwarders { 8.8.8.8; 8.8.4.4; }; forward only; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation no; listen-on-v6 { none; }; auth-nxdomain no; listen-on port 53 { any; }; // Allow queries from my Homelab and also from Wireguard Clients. allow-query { internals; }; allow-query-cache { internals; }; allow-update { internals; }; recursion yes; allow-recursion { internals; }; allow-transfer { internals; }; dnssec-enable no; check-names master ignore; check-names slave ignore; check-names response ignore; }; /etc/bind/named.conf.local #include \"/etc/bind/rndc.key\"; // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; # All devices that don't belong to the OKD cluster will be maintained here. zone \"homelab.net\" { type master; file \"/etc/bind/forward.homelab.net\"; allow-update { key rndc-key; }; }; zone \"c1.homelab.net\" { type master; file \"/etc/bind/forward.c1.homelab.net\"; allow-update { key rndc-key; }; }; zone \"178.168.192.in-addr.arpa\" { type master; notify no; file \"/etc/bind/178.168.192.in-addr.arpa\"; allow-update { key rndc-key; }; }; Zone file for homlab.net: /etc/bind/forward.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA homelab.net. root.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 The name of the next file depends on the subnet that is used: /etc/bind/178.168.192.in-addr.arpa $TTL 1W @ IN SOA ns1.homelab.net. root.homelab.net. ( 2019070742 ; serial 10800 ; refresh (3 hours) 1800 ; retry (30 minutes) 1209600 ; expire (2 weeks) 604800 ; minimum (1 week) ) NS ns1.homelab.net. 200 PTR bootstrap.c1.homelab.net. 210 PTR master0.c1.homelab.net. 211 PTR master1.c1.homelab.net. 212 PTR master2.c1.homelab.net. 220 PTR worker0.c1.homelab.net. 221 PTR worker1.c1.homelab.net. 222 PTR worker2.c1.homelab.net. 5 PTR api.c1.homelab.net. 5 PTR api-int.c1.homelab.net.","title":"Basic configuration"},{"location":"guides/vsphere-prereqs/#dns-records-for-okd-4","text":"Zone file for c1.homelab.net (our OKD 4 cluster will be in this domain): /etc/bind/forward.c1.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA c1.homelab.net. root.c1.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS c1.homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 load-balancer IN A 192.168.178.5 bootstrap IN A 192.168.178.200 master0 IN A 192.168.178.210 master1 IN A 192.168.178.211 master2 IN A 192.168.178.212 worker0 IN A 192.168.178.220 worker1 IN A 192.168.178.221 worker2 IN A 192.168.178.222 worker3 IN A 192.168.178.223 *.apps.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api-int.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net.","title":"DNS records for OKD 4"},{"location":"guides/vsphere-prereqs/#set-file-permissions","text":"For dynamic DNS (ddns) to work you should do this: sudo chown -R bind:bind /etc/bind","title":"Set file permissions"},{"location":"guides/vsphere-prereqs/#load-balancer","text":"","title":"Load Balancer"},{"location":"guides/vsphere-prereqs/#install_2","text":"sudo apt-get install haproxy","title":"Install"},{"location":"guides/vsphere-prereqs/#configure_1","text":"/etc/haproxy/haproxy.cfg global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from: # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 20000 timeout client 10000 timeout server 10000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # You can see the stats and observe OKD's bootstrap process by opening # http://<IP>:4321/haproxy?stats listen stats bind :4321 mode http log global maxconn 10 timeout client 100s timeout server 100s timeout connect 100s timeout queue 100s stats enable stats hide-version stats refresh 30s stats show-node stats auth admin:password stats uri /haproxy?stats frontend openshift-api-server bind *:6443 default_backend openshift-api-server mode tcp option tcplog backend openshift-api-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:6443 check server master0 master0.c1.homelab.net:6443 check server master1 master1.c1.homelab.net:6443 check server master2 master2.c1.homelab.net:6443 check frontend machine-config-server bind *:22623 default_backend machine-config-server mode tcp option tcplog backend machine-config-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:22623 check server master0 master0.c1.homelab.net:22623 check server master1 master1.c1.homelab.net:22623 check server master2 master2.c1.homelab.net:22623 check frontend ingress-http bind *:80 default_backend ingress-http mode tcp option tcplog backend ingress-http balance source mode tcp server master0 master0.c1.homelab.net:80 check server master1 master1.c1.homelab.net:80 check server master2 master2.c1.homelab.net:80 check server worker0 worker0.c1.homelab.net:80 check server worker1 worker1.c1.homelab.net:80 check server worker2 worker2.c1.homelab.net:80 check server worker3 worker3.c1.homelab.net:80 check frontend ingress-https bind *:443 default_backend ingress-https mode tcp option tcplog backend ingress-https balance source mode tcp server master0 master0.c1.homelab.net:443 check server master1 master1.c1.homelab.net:443 check server master2 master2.c1.homelab.net:443 check server worker0 worker0.c1.homelab.net:443 check server worker1 worker1.c1.homelab.net:443 check server worker2 worker2.c1.homelab.net:443 check server worker3 worker3.c1.homelab.net:443 check","title":"Configure"},{"location":"guides/vsphere-prereqs/#reboot-and-check-status","text":"Reboot Raspberry Pi: sudo reboot Check status of DNS/DHCP server and Load Balancer: sudo systemctl status haproxy.service sudo systemctl status isc-dhcp-server.service sudo systemctl status bind9","title":"Reboot and check status"},{"location":"guides/vsphere-prereqs/#proxy-if-on-a-private-network","text":"If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid","title":"Proxy (if on a private network)"},{"location":"guides/virt-baremetal-upi/","text":"OKD Virtualization on user provided infrastructure \u00b6 Preparing the hardware \u00b6 As a first step for providing an infrastructure for OKD Virtualization, you need to prepare the hardware: * check that minimum hardware requirements for running OKD are satisfied; * check that additional hardware requirements for running OKD Virtualization are also satisfied. Preparing the infrastructure \u00b6 Once your hardware is ready and connected to the network you need to configure your services, your network and your DNS for allowing the OKD installer to deploy the software. You may need to prepare in advance also a few services you'll need during the deployment. Read carefully the Preparing the user-provisioned infrastructure section and ensure all the requirements are met. Provision your hosts \u00b6 For the bastion / service host you can use CentOS Stream 8. You can follow CentOS 8 installation documentation but we recommend using the latest CentOS Stream 8 ISO . For the OKD nodes you\u2019ll need Fedora CoreOS. You can get it from Get Fedora! website, choose the Bare Metal ISO. Configure the bastion to host needed services \u00b6 Configure Apache to serve on port 8080/8443 as the http/https port will be used by the haproxy service. Apache will be needed to provide ignition configuration for OKD nodes. dnf install -y httpd sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf sed -i 's/Listen 443/Listen 8443/' /etc/httpd/conf.d/ssl.conf setsebool -P httpd_read_user_content 1 systemctl enable --now httpd.service firewall-cmd --permanent --add-port = 8080 /tcp firewall-cmd --permanent --add-port = 8443 /tcp firewall-cmd --reload # Verify it\u2019s up: curl localhost:8080 Configure haproxy dnf install haproxy -y firewall-cmd --permanent --add-port = 6443 /tcp firewall-cmd --permanent --add-port = 22623 /tcp firewall-cmd --permanent --add-service = http firewall-cmd --permanent --add-service = https firewall-cmd --reload setsebool -P haproxy_connect_any 1 systemctl enable --now haproxy.service Installing OKD \u00b6 OKD current stable-4 branch is delivering OKD 4.8. If you're using an older version we recommend to update to ODK 4.8. At this point you should have all OKD nodes ready to be installed with Fedora CoreOS and the bastion with all the needed services. Check that all nodes and the bastion have the correct ip addresses and fqdn and that they are resolvable via DNS. As we are going to use baremetal UPI installation you\u2019ll need to create a install-config.yaml following the example for installing bare metal Remember to configure your proxy settings if you have a proxy Apply workarounds \u00b6 qemu-ga is hitting selinux denials https://bugzilla.redhat.com/show_bug.cgi?id=1927639 You can workaround this by adding a custom policy: echo '(allow virt_qemu_ga_t container_var_lib_t (dir (search)))' >local_virtqemu_ga.cil semodule -i local_virtqemu_ga.cil iptables is hitting selinux denials https://bugzilla.redhat.com/show_bug.cgi?id=2008097 You can workaround this by adding a custom policy: echo '(allow iptables_t cgroup_t (dir (ioctl)))' >local_iptables.cil semodule -i local_iptables.cil rpcbind echo '(allow rpcbind_t unreserved_port_t (udp_socket (name_bind)))' >local_rpcbind.cil semodule -i local_rpcbind.cil master nodes are failing the first boot with access denied to [::1]:53 https://github.com/openshift/okd/issues/897 While the master node is booting edit the grub config adding to kernel command line console=null . worker nodes may fail on openvswitch echo '(allow openvswitch_t init_var_run_t (capability (fsetid)))' >local_openvswitch.cil semodule -i local_openvswitch.cil Installing HCO and KubeVirt \u00b6 Once OKD console is up, connect to it. Go to Operators -> OperatorHub Look for KubeVirt HyperConverged Cluster Operator and install it. Click on Create Hyperconverged button, all the defaults should be fine. Providing storage \u00b6 Shared storage is not mandatory for OKD Virtualization, but without a doubt it provides many advantages over a configuration based on local storage which is considered a suboptimal configuration. Between the advantages enabled by shared storage is worth to mention: - Live migration of Virtual Machines - Founding pillar for HA - Enables seamless cluster upgrades without the need to shut down and restart all the VMs on each upgrade - Centralized storage management enabling elastic scalability - Centralized backup Shared storage \u00b6 TBD: rook.io deployment Local storage \u00b6 You can configure local storage for your virtual machines by using the OKD Virtualization hostpath provisioner feature. When you install OKD Virtualization, the hostpath provisioner Operator is automatically installed. To use it, you must: - Configure SELinux on your worker nodes via a Machine Config object. - Create a HostPathProvisioner custom resource. - Create a StorageClass object for the hostpath provisioner. Configuring SELinux for the hostpath provisioner on OKD worker nodes \u00b6 You can configure SELinux for your OKD Worker nodes using a MachineConfig . Creating a CR for the HostPathProvisioner operator \u00b6 Create the HostPathProvisioner custom resource file. For example: $ touch hostpathprovisioner_cr.yaml Edit that file. For example: apiVersion : hostpathprovisioner.kubevirt.io/v1beta1 kind : HostPathProvisioner metadata : name : hostpath-provisioner spec : imagePullPolicy : IfNotPresent pathConfig : path : \"/var/hpvolumes\" #The path of the directory on the node useNamingPrefix : false #Use the name of the PVC bound to the created PV as part of the directory name. Creating the CR in the kubevirt-hyperconverged namespace: $ oc create -n kubevirt-hyperconverged -f hostpathprovisioner_cr.yaml Creating a StorageClass for the HostPathProvisioner operator \u00b6 Create the YAML file for the storage class. For example: $ touch hppstorageclass.yaml Edit that file. For example: apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : hostpath-provisioner provisioner : kubevirt.io/hostpath-provisioner reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer Creating the Storage Class object: $ oc create -f hppstorageclass.yaml","title":"OKD Virtualization"},{"location":"guides/virt-baremetal-upi/#okd-virtualization-on-user-provided-infrastructure","text":"","title":"OKD Virtualization on user provided infrastructure"},{"location":"guides/virt-baremetal-upi/#preparing-the-hardware","text":"As a first step for providing an infrastructure for OKD Virtualization, you need to prepare the hardware: * check that minimum hardware requirements for running OKD are satisfied; * check that additional hardware requirements for running OKD Virtualization are also satisfied.","title":"Preparing the hardware"},{"location":"guides/virt-baremetal-upi/#preparing-the-infrastructure","text":"Once your hardware is ready and connected to the network you need to configure your services, your network and your DNS for allowing the OKD installer to deploy the software. You may need to prepare in advance also a few services you'll need during the deployment. Read carefully the Preparing the user-provisioned infrastructure section and ensure all the requirements are met.","title":"Preparing the infrastructure"},{"location":"guides/virt-baremetal-upi/#provision-your-hosts","text":"For the bastion / service host you can use CentOS Stream 8. You can follow CentOS 8 installation documentation but we recommend using the latest CentOS Stream 8 ISO . For the OKD nodes you\u2019ll need Fedora CoreOS. You can get it from Get Fedora! website, choose the Bare Metal ISO.","title":"Provision your hosts"},{"location":"guides/virt-baremetal-upi/#configure-the-bastion-to-host-needed-services","text":"Configure Apache to serve on port 8080/8443 as the http/https port will be used by the haproxy service. Apache will be needed to provide ignition configuration for OKD nodes. dnf install -y httpd sed -i 's/Listen 80/Listen 8080/' /etc/httpd/conf/httpd.conf sed -i 's/Listen 443/Listen 8443/' /etc/httpd/conf.d/ssl.conf setsebool -P httpd_read_user_content 1 systemctl enable --now httpd.service firewall-cmd --permanent --add-port = 8080 /tcp firewall-cmd --permanent --add-port = 8443 /tcp firewall-cmd --reload # Verify it\u2019s up: curl localhost:8080 Configure haproxy dnf install haproxy -y firewall-cmd --permanent --add-port = 6443 /tcp firewall-cmd --permanent --add-port = 22623 /tcp firewall-cmd --permanent --add-service = http firewall-cmd --permanent --add-service = https firewall-cmd --reload setsebool -P haproxy_connect_any 1 systemctl enable --now haproxy.service","title":"Configure the bastion to host needed services"},{"location":"guides/virt-baremetal-upi/#installing-okd","text":"OKD current stable-4 branch is delivering OKD 4.8. If you're using an older version we recommend to update to ODK 4.8. At this point you should have all OKD nodes ready to be installed with Fedora CoreOS and the bastion with all the needed services. Check that all nodes and the bastion have the correct ip addresses and fqdn and that they are resolvable via DNS. As we are going to use baremetal UPI installation you\u2019ll need to create a install-config.yaml following the example for installing bare metal Remember to configure your proxy settings if you have a proxy","title":"Installing OKD"},{"location":"guides/virt-baremetal-upi/#apply-workarounds","text":"qemu-ga is hitting selinux denials https://bugzilla.redhat.com/show_bug.cgi?id=1927639 You can workaround this by adding a custom policy: echo '(allow virt_qemu_ga_t container_var_lib_t (dir (search)))' >local_virtqemu_ga.cil semodule -i local_virtqemu_ga.cil iptables is hitting selinux denials https://bugzilla.redhat.com/show_bug.cgi?id=2008097 You can workaround this by adding a custom policy: echo '(allow iptables_t cgroup_t (dir (ioctl)))' >local_iptables.cil semodule -i local_iptables.cil rpcbind echo '(allow rpcbind_t unreserved_port_t (udp_socket (name_bind)))' >local_rpcbind.cil semodule -i local_rpcbind.cil master nodes are failing the first boot with access denied to [::1]:53 https://github.com/openshift/okd/issues/897 While the master node is booting edit the grub config adding to kernel command line console=null . worker nodes may fail on openvswitch echo '(allow openvswitch_t init_var_run_t (capability (fsetid)))' >local_openvswitch.cil semodule -i local_openvswitch.cil","title":"Apply workarounds"},{"location":"guides/virt-baremetal-upi/#installing-hco-and-kubevirt","text":"Once OKD console is up, connect to it. Go to Operators -> OperatorHub Look for KubeVirt HyperConverged Cluster Operator and install it. Click on Create Hyperconverged button, all the defaults should be fine.","title":"Installing HCO and KubeVirt"},{"location":"guides/virt-baremetal-upi/#providing-storage","text":"Shared storage is not mandatory for OKD Virtualization, but without a doubt it provides many advantages over a configuration based on local storage which is considered a suboptimal configuration. Between the advantages enabled by shared storage is worth to mention: - Live migration of Virtual Machines - Founding pillar for HA - Enables seamless cluster upgrades without the need to shut down and restart all the VMs on each upgrade - Centralized storage management enabling elastic scalability - Centralized backup","title":"Providing storage"},{"location":"guides/virt-baremetal-upi/#shared-storage","text":"TBD: rook.io deployment","title":"Shared storage"},{"location":"guides/virt-baremetal-upi/#local-storage","text":"You can configure local storage for your virtual machines by using the OKD Virtualization hostpath provisioner feature. When you install OKD Virtualization, the hostpath provisioner Operator is automatically installed. To use it, you must: - Configure SELinux on your worker nodes via a Machine Config object. - Create a HostPathProvisioner custom resource. - Create a StorageClass object for the hostpath provisioner.","title":"Local storage"},{"location":"guides/virt-baremetal-upi/#configuring-selinux-for-the-hostpath-provisioner-on-okd-worker-nodes","text":"You can configure SELinux for your OKD Worker nodes using a MachineConfig .","title":"Configuring SELinux for the hostpath provisioner on OKD worker nodes"},{"location":"guides/virt-baremetal-upi/#creating-a-cr-for-the-hostpathprovisioner-operator","text":"Create the HostPathProvisioner custom resource file. For example: $ touch hostpathprovisioner_cr.yaml Edit that file. For example: apiVersion : hostpathprovisioner.kubevirt.io/v1beta1 kind : HostPathProvisioner metadata : name : hostpath-provisioner spec : imagePullPolicy : IfNotPresent pathConfig : path : \"/var/hpvolumes\" #The path of the directory on the node useNamingPrefix : false #Use the name of the PVC bound to the created PV as part of the directory name. Creating the CR in the kubevirt-hyperconverged namespace: $ oc create -n kubevirt-hyperconverged -f hostpathprovisioner_cr.yaml","title":"Creating a CR for the HostPathProvisioner operator"},{"location":"guides/virt-baremetal-upi/#creating-a-storageclass-for-the-hostpathprovisioner-operator","text":"Create the YAML file for the storage class. For example: $ touch hppstorageclass.yaml Edit that file. For example: apiVersion : storage.k8s.io/v1 kind : StorageClass metadata : name : hostpath-provisioner provisioner : kubevirt.io/hostpath-provisioner reclaimPolicy : Delete volumeBindingMode : WaitForFirstConsumer Creating the Storage Class object: $ oc create -f hppstorageclass.yaml","title":"Creating a StorageClass for the HostPathProvisioner operator"},{"location":"okd_tech_docs/","text":"OKD Technical Documentation \u00b6 Warning This section is under construction This section of the documentation is for developers that want to customize OKD. The section will cover: How is OKD delivered How to build an OKD operator How to deploy a self-build version of an OKD operator to an existing cluster How to create a customized OKD installer containing your self-built operator The above section will allow you to work on fixes and enhancements to core OKD operators and be able to run them locally. In addition to the above this section will also look at the Red Hat build and test setup, looking at how OpenShift and OKD operators are built and tested and how releases are created. OKD Releases \u00b6 OKD is a Kubernetes based platform that delivers a fully managed platform from the core operating system to the Kubernetes platform and the services running on it. All aspects of OKD are managed by a collection of operators. OKD shares most of the same source code as Red Hat OpenShift. One of the primary differences is that OKD uses Fedora CoreOS where OpenShift uses Red Hat Enterprise Linux CoreOS as the base platform for cluster nodes. An OKD release is a strictly defined set of software. A release is defined by a release payload , which contains an operator (Cluster Version Operator), a list of manifests to apply and a reference file. You can get information about a release using the oc command line utility, oc adm release info <release name> . You can find the latest available release here . You can get the current version of your cluster using the oc get clusterversion command, or from the Cluster Settings page in the Administration section of the OKD web console. For the OKD 4.10 release named 4.10.0-0.okd-2022-03-07-131213 the command would be oc adm release info 4.10.0-0.okd-2022-03-07-131213 you can add additional command line options to get more specific information about a release: --commit-urls shows the source code that makes up the release --commits allows you to specify 2 releases and see the differences between the releases --pullspecs show the exact container images that will be used by a release","title":"Overview"},{"location":"okd_tech_docs/#okd-technical-documentation","text":"Warning This section is under construction This section of the documentation is for developers that want to customize OKD. The section will cover: How is OKD delivered How to build an OKD operator How to deploy a self-build version of an OKD operator to an existing cluster How to create a customized OKD installer containing your self-built operator The above section will allow you to work on fixes and enhancements to core OKD operators and be able to run them locally. In addition to the above this section will also look at the Red Hat build and test setup, looking at how OpenShift and OKD operators are built and tested and how releases are created.","title":"OKD Technical Documentation"},{"location":"okd_tech_docs/#okd-releases","text":"OKD is a Kubernetes based platform that delivers a fully managed platform from the core operating system to the Kubernetes platform and the services running on it. All aspects of OKD are managed by a collection of operators. OKD shares most of the same source code as Red Hat OpenShift. One of the primary differences is that OKD uses Fedora CoreOS where OpenShift uses Red Hat Enterprise Linux CoreOS as the base platform for cluster nodes. An OKD release is a strictly defined set of software. A release is defined by a release payload , which contains an operator (Cluster Version Operator), a list of manifests to apply and a reference file. You can get information about a release using the oc command line utility, oc adm release info <release name> . You can find the latest available release here . You can get the current version of your cluster using the oc get clusterversion command, or from the Cluster Settings page in the Administration section of the OKD web console. For the OKD 4.10 release named 4.10.0-0.okd-2022-03-07-131213 the command would be oc adm release info 4.10.0-0.okd-2022-03-07-131213 you can add additional command line options to get more specific information about a release: --commit-urls shows the source code that makes up the release --commits allows you to specify 2 releases and see the differences between the releases --pullspecs show the exact container images that will be used by a release","title":"OKD Releases"},{"location":"okd_tech_docs/modifying_okd/","text":"Making changes to OKD \u00b6 Warning This section is under construction The source code for OKD is available on github . OKD is made up of many components bundled into a release. You can find the exact commit for each component included in a release using the oc adm release info command with the --commit-urls option, as outlined in the overview section. To make a change to OKD you need to: Identify the component(s) that needs to be changed Clone/fork the git repository (you can choose to fork the exact commit used to create the image referenced by the OKD release or a newer version of the source) Make the change Build the image and push to a container registry that the OKD cluster will be able to access Run the modified container on a cluster Building images \u00b6 Most component repositories contain a Dockerfile, so building the image is as simple as podman build or docker build depending on your container tool of choice. Some component repositories contain a Makefile, so building the image can be done using the Makefile, typically with make build First thing to do is to replace the FROM images in Dockerfile.rhel7 . You may want to just copy it to Dockerfile and then make the changes. FROM registry.ci.openshift.org/openshift/release:golang-1.17 AS builder and FROM registry.ci.openshift.org/origin/4.10:base Note The original and replacement image may change as golang version and release requirements change. Question Is there a way to find the correct base image for an OKD release? The original images are unavailable to the public. There is an effort to update the Dockerfiles with publically available images. Example Scenario \u00b6 Modify console-operator to have a link to the community site okd.io instead of docs.okd.io add to pre-existing cluster build a custom release to include the modified console-operator, then install a new cluster will custom release To complete the scenario the following steps need to be performed: Fork the console-operator repository Clone the new fork locally: git clone https://github.com/<username>/console-operator.git create new branch from master (or main): git switch -c <branch name> Make needed modifications. Commit/squash as needed. Maintainers like to see 1 commit rather than several. Create the image: podman build -f <Dockerfile file> -t <target repo>/<username>/console-operator:4.11-<some additional identifier> Push image to external repository: podman push <target repo>/<username>/console-operator:4.11-<some additional identifier> Create new release to test with. This requires the oc command to be available. I use the following script (make_payload.sh). It can be modified as needed, such as adding the correct container registry and username: server = https://api.ci.openshift.org from_release = registry.ci.openshift.org/origin/release:4.11.0-0.okd-2022-04-12-000907 release_name = 4 .11.0-0.jef-2022-04-12-0 to_image = quay.io/fortinj66/origin-release:v4.11-console-operator oc adm release new --from-release ${ from_release } \\ --name ${ release_name } \\ --to-image ${ to_image } \\ console-operator = <target repo>/<username>/console-operator:4.11-<some additional identifier> from_release , release_name , to_image will need to be updated as needed Pull installer for cluster release: oc adm release extract --tools <to_image from above> (Make sure image is publically available) Warning When working with some Go lang projects you may need to be on Go lang v1.17 or better, as some projects use language features not supported before v1.17, even though some of the project README.md files may specify V1.15, these README files are out of date If it is not clear how to build a component you can look in the release repository at https://github.com/openshift/release/tree/master/ci-operator/config/openshift/<operator repo name> , this is used by the Red Hat build system to build components so can be used to determine how to build a component. You should also check the repo README.md file or any documentation, typically in a doc folder, as there may be some repo specific details Question Are there any special repos unique to OKD that need specific mention here, such as machine config? Running the modified image on a cluster \u00b6 An OKD release contain a specific set of images and there are operators that ensure that only the correct set of images are running a cluster, so you need to do some specific actions to be able to run your modified image on a cluster. You can do this by: configuring an existing cluster to run a modified image create a new installer containing your image then creating a new cluster with the modified installer Running on an existing cluster \u00b6 The Cluster Version Operator watches the deployments and images related to the core OKD services to ensure that only valid images are running in the core. This prevents you from changing any of the core images. If you want to replace an image you need to scale the Cluster Version Operator down to 0 replicas: oc scale --replicas = 0 deployment/cluster-version-operator -n openshift-cluster-version Some images, such as the Cluster Cloud Controller Manager Operator and the Machine API Operator need additional steps to be able to make changes, but these typically have a docs folder containing additional information about how to make changes to these images. Create custom release \u00b6","title":"Modifying OKD"},{"location":"okd_tech_docs/modifying_okd/#making-changes-to-okd","text":"Warning This section is under construction The source code for OKD is available on github . OKD is made up of many components bundled into a release. You can find the exact commit for each component included in a release using the oc adm release info command with the --commit-urls option, as outlined in the overview section. To make a change to OKD you need to: Identify the component(s) that needs to be changed Clone/fork the git repository (you can choose to fork the exact commit used to create the image referenced by the OKD release or a newer version of the source) Make the change Build the image and push to a container registry that the OKD cluster will be able to access Run the modified container on a cluster","title":"Making changes to OKD"},{"location":"okd_tech_docs/modifying_okd/#building-images","text":"Most component repositories contain a Dockerfile, so building the image is as simple as podman build or docker build depending on your container tool of choice. Some component repositories contain a Makefile, so building the image can be done using the Makefile, typically with make build First thing to do is to replace the FROM images in Dockerfile.rhel7 . You may want to just copy it to Dockerfile and then make the changes. FROM registry.ci.openshift.org/openshift/release:golang-1.17 AS builder and FROM registry.ci.openshift.org/origin/4.10:base Note The original and replacement image may change as golang version and release requirements change. Question Is there a way to find the correct base image for an OKD release? The original images are unavailable to the public. There is an effort to update the Dockerfiles with publically available images.","title":"Building images"},{"location":"okd_tech_docs/modifying_okd/#example-scenario","text":"Modify console-operator to have a link to the community site okd.io instead of docs.okd.io add to pre-existing cluster build a custom release to include the modified console-operator, then install a new cluster will custom release To complete the scenario the following steps need to be performed: Fork the console-operator repository Clone the new fork locally: git clone https://github.com/<username>/console-operator.git create new branch from master (or main): git switch -c <branch name> Make needed modifications. Commit/squash as needed. Maintainers like to see 1 commit rather than several. Create the image: podman build -f <Dockerfile file> -t <target repo>/<username>/console-operator:4.11-<some additional identifier> Push image to external repository: podman push <target repo>/<username>/console-operator:4.11-<some additional identifier> Create new release to test with. This requires the oc command to be available. I use the following script (make_payload.sh). It can be modified as needed, such as adding the correct container registry and username: server = https://api.ci.openshift.org from_release = registry.ci.openshift.org/origin/release:4.11.0-0.okd-2022-04-12-000907 release_name = 4 .11.0-0.jef-2022-04-12-0 to_image = quay.io/fortinj66/origin-release:v4.11-console-operator oc adm release new --from-release ${ from_release } \\ --name ${ release_name } \\ --to-image ${ to_image } \\ console-operator = <target repo>/<username>/console-operator:4.11-<some additional identifier> from_release , release_name , to_image will need to be updated as needed Pull installer for cluster release: oc adm release extract --tools <to_image from above> (Make sure image is publically available) Warning When working with some Go lang projects you may need to be on Go lang v1.17 or better, as some projects use language features not supported before v1.17, even though some of the project README.md files may specify V1.15, these README files are out of date If it is not clear how to build a component you can look in the release repository at https://github.com/openshift/release/tree/master/ci-operator/config/openshift/<operator repo name> , this is used by the Red Hat build system to build components so can be used to determine how to build a component. You should also check the repo README.md file or any documentation, typically in a doc folder, as there may be some repo specific details Question Are there any special repos unique to OKD that need specific mention here, such as machine config?","title":"Example Scenario"},{"location":"okd_tech_docs/modifying_okd/#running-the-modified-image-on-a-cluster","text":"An OKD release contain a specific set of images and there are operators that ensure that only the correct set of images are running a cluster, so you need to do some specific actions to be able to run your modified image on a cluster. You can do this by: configuring an existing cluster to run a modified image create a new installer containing your image then creating a new cluster with the modified installer","title":"Running the modified image on a cluster"},{"location":"okd_tech_docs/modifying_okd/#running-on-an-existing-cluster","text":"The Cluster Version Operator watches the deployments and images related to the core OKD services to ensure that only valid images are running in the core. This prevents you from changing any of the core images. If you want to replace an image you need to scale the Cluster Version Operator down to 0 replicas: oc scale --replicas = 0 deployment/cluster-version-operator -n openshift-cluster-version Some images, such as the Cluster Cloud Controller Manager Operator and the Machine API Operator need additional steps to be able to make changes, but these typically have a docs folder containing additional information about how to make changes to these images.","title":"Running on an existing cluster"},{"location":"okd_tech_docs/modifying_okd/#create-custom-release","text":"","title":"Create custom release"},{"location":"okd_tech_docs/operators/","text":"Operator Hub Catalogs \u00b6 Warning This section is under construction OKD contains many operators which deliver the base platform, however there is also additional capabilities delivered as operators available via the Operator Hub. The operator hub story for OKD isn't ideal currently (as at OKD 4.10) as OKD shares source with OpenShift, the commercial sibling to OKD. OpenShift has additional operator hub catalogs provided by Red Hat, which deliver additional capabilities as part of the supported OpenShift product. These additional capabilities are not currently provided to OKD. OpenShift and OKD share a community catalog of operators, which are a subset of the operators available in the OperatorHub . The operators in the community catalog should run on OKD/OpenShift and will include any additional configuration, such as security context configuration. However, where an operator in the community catalog has a dependency that Red Hat supports and delivers as part of the additional OpenShift operator catalog, then the community catalog operator will specify the dependency from the supported OpenShift catalog. This results in missing dependency errors when attempting to install on OKD. Question will the proposed OKD catalog solve all the dependency issues in the community catalog? what is the timeline for the OKD catalog? Todo Some useful repo links - do we need to create instructions for specific operators? Community operator source docs repo OKD operators : repo Marketplace operator : repo Devworkspace operator : repo GitOps operator : repo devspaces (crw) : public repo","title":"Operator Hub catalogs"},{"location":"okd_tech_docs/operators/#operator-hub-catalogs","text":"Warning This section is under construction OKD contains many operators which deliver the base platform, however there is also additional capabilities delivered as operators available via the Operator Hub. The operator hub story for OKD isn't ideal currently (as at OKD 4.10) as OKD shares source with OpenShift, the commercial sibling to OKD. OpenShift has additional operator hub catalogs provided by Red Hat, which deliver additional capabilities as part of the supported OpenShift product. These additional capabilities are not currently provided to OKD. OpenShift and OKD share a community catalog of operators, which are a subset of the operators available in the OperatorHub . The operators in the community catalog should run on OKD/OpenShift and will include any additional configuration, such as security context configuration. However, where an operator in the community catalog has a dependency that Red Hat supports and delivers as part of the additional OpenShift operator catalog, then the community catalog operator will specify the dependency from the supported OpenShift catalog. This results in missing dependency errors when attempting to install on OKD. Question will the proposed OKD catalog solve all the dependency issues in the community catalog? what is the timeline for the OKD catalog? Todo Some useful repo links - do we need to create instructions for specific operators? Community operator source docs repo OKD operators : repo Marketplace operator : repo Devworkspace operator : repo GitOps operator : repo devspaces (crw) : public repo","title":"Operator Hub Catalogs"},{"location":"okd_tech_docs/release/","text":"OKD Development Resources \u00b6 Warning This section is under construction Question What is the end-to-end process to build an OKD release? Is it possible outside Red Hat CI infrastructure? openshift-installer source","title":"Release"},{"location":"okd_tech_docs/release/#okd-development-resources","text":"Warning This section is under construction Question What is the end-to-end process to build an OKD release? Is it possible outside Red Hat CI infrastructure? openshift-installer source","title":"OKD Development Resources"},{"location":"okd_tech_docs/troubleshoot/","text":"Troubleshooting OKD \u00b6 Warning This section is under construction Todo Complete this section from comments in discussion thread","title":"Troubleshooting"},{"location":"okd_tech_docs/troubleshoot/#troubleshooting-okd","text":"Warning This section is under construction Todo Complete this section from comments in discussion thread","title":"Troubleshooting OKD"},{"location":"wg_crc/overview/","text":"CRC Build Subgroup \u00b6 Code-Ready Containers is a cut down version of OKD, designed to run on a developer's machine, which would not have sufficient resources for a full installation of OKD. The working group was established after a live session where Red Hat's Charro Gruver walked through the build process for OKD CRC The build process is currently manual, so the working group was established to automate the process and investigate options for creating a continuous integration setup to build and test OKD CRC.","title":"Overview"},{"location":"wg_crc/overview/#crc-build-subgroup","text":"Code-Ready Containers is a cut down version of OKD, designed to run on a developer's machine, which would not have sufficient resources for a full installation of OKD. The working group was established after a live session where Red Hat's Charro Gruver walked through the build process for OKD CRC The build process is currently manual, so the working group was established to automate the process and investigate options for creating a continuous integration setup to build and test OKD CRC.","title":"CRC Build Subgroup"},{"location":"wg_docs/content/","text":"Site content maintainability \u00b6 The site has adopted Markdown as the standard way to create content for the site. Previously the site used an HTML based framework, which resulted in content not being frequently updated as there was a steep learning curve. All content on the site should be created using Markdown. To ensure content is maintainable going forward only markdown features outlined below should be used to create site content. If you wish to use additional components on a page then please contact the documentation working group to discuss your requirements before creating a pull request containing additional components. MkDocs includes the ability to create custom page templates. This facility has been used to create a customized home page for the site. If any other pages require a custom layout or custom features, then a page template should be used so the content can remain in Markdown. Creation of custom page templates should be discussed with the documentation working group. Changing content \u00b6 MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the in-page navigation on the right of the page. Standard Markdown features \u00b6 The following markdown syntax is used within the documentation Syntax Result # Title heading - you can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic `code` inline code block ```shell ... ``` multi-line (Fenced) code block 1. list item ordered list - unordered list item unordered list --- horizontal break HTML can be embedded in Markdown, but embedded HTML should not be used in the documentation. All content should use Markdown with the permitted extensions. Indentation \u00b6 MkDocs uses 4 spaces for tabs, so when indenting code ensure you are working with tabs set to 4 spaces rather than 2, which is commonly used. When using some features of Markdown indentation is used to identify blocks. 1. Ubiquity EdgeRouter ER-X - runs DHCP (embedded), custom DNS server via AdGuard ![ pic ]( ./img/erx.jpg ){width=80%} In the code block above you will see the unordered list item is indented, so it aligns with the content of the ordered list (rather than aligning with the number of the ordered list). The image is also indented so it too aligns with the ordered list text. Many of the Markdown elements can be nested and indentation is used to define the nesting relationship. If you look down on this page at the Information boxes section, the example shows an example of nesting elements and the Markdown tab shows how indentation is being used to identify the nesting relationships. Links within MkDocs generated content \u00b6 MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information Markdown Extensions used in OKD.io \u00b6 There are a number of Markdown extensions being used to create the site. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here Link configuration \u00b6 Links on the page or embedded images can be annotated to control the links and also the appearance of the links: Image \u00b6 Images are embedded in a page using the standard Markdown syntax ![description](URL) , but the image can be formatted with Attribute Lists . This is most commonly used to scale an image or center an image, e.g. ![ GitHub repo url ]( images/github-repo-url.png ){style=\"width: 80%\" .center } External Links \u00b6 External links can also use attribute lists to control behaviors, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below: [ MkDocs ]( http://mkdocs.org ){: target=\"_blank\" .external } Info You can embed an image as the description of a link to create clickable images that launch to another site: [![Image description](Image URL)](target URL \"hover text\"){: target=_blank} YouTube videos \u00b6 It is not possible to embed a YouTube video and have it play in place using pure markdown. You can use HTML within the markdown file to embed a video: < iframe width = \"100%\" height = \"500\" src = \"https://www.youtube.com/watch?v=qh1zYW7BLxE&t=431s\" title = \"Building an OKD 4 Home Lab with special guest Craig Robinson\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></ iframe > Tabs \u00b6 Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World Information boxes \u00b6 The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a collapsible note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note Supported Admonition Classes \u00b6 The Admonitions supported by the Material theme are : Note This is a note Abstract This is an abstract Info This is an info Tip This is a tip Success This is a success Question This is a question Warning This is a warning Failure This is a failure Danger This is a danger Bug This is a bug Example This is an example Quote This is a quote Code blocks \u00b6 Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ```shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content Advanced highlighting of code blocks \u00b6 There are some additional features available due to the highlight plugin installed in MkDocs. Full details can be found in the MkDocs Materials documentation . Line numbers \u00b6 You can add line numbers to a code block with the linenums directive. You must specify the starting line number, 1 in the example below: ``` javascript linenums=\"1\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 1 2 3 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Info The line numbers do not get included when the copy to clipboard link is selected Spell checking \u00b6 This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project. Adding local words \u00b6 You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment. Adding global words \u00b6 The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Content guidelines"},{"location":"wg_docs/content/#site-content-maintainability","text":"The site has adopted Markdown as the standard way to create content for the site. Previously the site used an HTML based framework, which resulted in content not being frequently updated as there was a steep learning curve. All content on the site should be created using Markdown. To ensure content is maintainable going forward only markdown features outlined below should be used to create site content. If you wish to use additional components on a page then please contact the documentation working group to discuss your requirements before creating a pull request containing additional components. MkDocs includes the ability to create custom page templates. This facility has been used to create a customized home page for the site. If any other pages require a custom layout or custom features, then a page template should be used so the content can remain in Markdown. Creation of custom page templates should be discussed with the documentation working group.","title":"Site content maintainability"},{"location":"wg_docs/content/#changing-content","text":"MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the in-page navigation on the right of the page.","title":"Changing content"},{"location":"wg_docs/content/#standard-markdown-features","text":"The following markdown syntax is used within the documentation Syntax Result # Title heading - you can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic `code` inline code block ```shell ... ``` multi-line (Fenced) code block 1. list item ordered list - unordered list item unordered list --- horizontal break HTML can be embedded in Markdown, but embedded HTML should not be used in the documentation. All content should use Markdown with the permitted extensions.","title":"Standard Markdown features"},{"location":"wg_docs/content/#indentation","text":"MkDocs uses 4 spaces for tabs, so when indenting code ensure you are working with tabs set to 4 spaces rather than 2, which is commonly used. When using some features of Markdown indentation is used to identify blocks. 1. Ubiquity EdgeRouter ER-X - runs DHCP (embedded), custom DNS server via AdGuard ![ pic ]( ./img/erx.jpg ){width=80%} In the code block above you will see the unordered list item is indented, so it aligns with the content of the ordered list (rather than aligning with the number of the ordered list). The image is also indented so it too aligns with the ordered list text. Many of the Markdown elements can be nested and indentation is used to define the nesting relationship. If you look down on this page at the Information boxes section, the example shows an example of nesting elements and the Markdown tab shows how indentation is being used to identify the nesting relationships.","title":"Indentation"},{"location":"wg_docs/content/#links-within-mkdocs-generated-content","text":"MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information","title":"Links within MkDocs generated content"},{"location":"wg_docs/content/#markdown-extensions-used-in-okdio","text":"There are a number of Markdown extensions being used to create the site. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here","title":"Markdown Extensions used in OKD.io"},{"location":"wg_docs/content/#link-configuration","text":"Links on the page or embedded images can be annotated to control the links and also the appearance of the links:","title":"Link configuration"},{"location":"wg_docs/content/#image","text":"Images are embedded in a page using the standard Markdown syntax ![description](URL) , but the image can be formatted with Attribute Lists . This is most commonly used to scale an image or center an image, e.g. ![ GitHub repo url ]( images/github-repo-url.png ){style=\"width: 80%\" .center }","title":"Image"},{"location":"wg_docs/content/#external-links","text":"External links can also use attribute lists to control behaviors, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below: [ MkDocs ]( http://mkdocs.org ){: target=\"_blank\" .external } Info You can embed an image as the description of a link to create clickable images that launch to another site: [![Image description](Image URL)](target URL \"hover text\"){: target=_blank}","title":"External Links"},{"location":"wg_docs/content/#youtube-videos","text":"It is not possible to embed a YouTube video and have it play in place using pure markdown. You can use HTML within the markdown file to embed a video: < iframe width = \"100%\" height = \"500\" src = \"https://www.youtube.com/watch?v=qh1zYW7BLxE&t=431s\" title = \"Building an OKD 4 Home Lab with special guest Craig Robinson\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></ iframe >","title":"YouTube videos"},{"location":"wg_docs/content/#tabs","text":"Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World","title":"Tabs"},{"location":"wg_docs/content/#information-boxes","text":"The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a collapsible note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note","title":"Information boxes"},{"location":"wg_docs/content/#supported-admonition-classes","text":"The Admonitions supported by the Material theme are : Note This is a note Abstract This is an abstract Info This is an info Tip This is a tip Success This is a success Question This is a question Warning This is a warning Failure This is a failure Danger This is a danger Bug This is a bug Example This is an example Quote This is a quote","title":"Supported Admonition Classes"},{"location":"wg_docs/content/#code-blocks","text":"Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ```shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content","title":"Code blocks"},{"location":"wg_docs/content/#advanced-highlighting-of-code-blocks","text":"There are some additional features available due to the highlight plugin installed in MkDocs. Full details can be found in the MkDocs Materials documentation .","title":"Advanced highlighting of code blocks"},{"location":"wg_docs/content/#line-numbers","text":"You can add line numbers to a code block with the linenums directive. You must specify the starting line number, 1 in the example below: ``` javascript linenums=\"1\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 1 2 3 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Info The line numbers do not get included when the copy to clipboard link is selected","title":"Line numbers"},{"location":"wg_docs/content/#spell-checking","text":"This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project.","title":"Spell checking"},{"location":"wg_docs/content/#adding-local-words","text":"You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment.","title":"Adding local words"},{"location":"wg_docs/content/#adding-global-words","text":"The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Adding global words"},{"location":"wg_docs/doc-env/","text":"Setting up a documentation environment \u00b6 To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: MkDocs with the Materials theme CSpell linkchecker Node.js Python 3 You can create the environment by : running the tooling within a container runtime, so you don't need to do any local installs if you have an Eclipse Che installation on an OKD cluster then you can make the changes using only a browser, with all the tooling running inside Che on the OKD cluster. installing the components on your local system Tooling within a container You can use a container to run MkDocs so no local installation is required, however you do need to have Docker Desktop installed if using Mac OS or Windows. If running on Linux you can use Docker or Podman . If you have a node.js environment installed that includes the npm command then you can make use of the run scripts provided in the project to run the docker or podman commands The following commands all assume you are working in the root directory of your local git clone of your forked copy of the okd.io git repo. (your working directory should contain mkdocs.yml and package.json files) Warning If you are using Linux with SELinux enabled, then you need to configure your system to allow the local directory containing the cloned git repo to be mounted inside a container. The following commands will configure SELinux to allow this: (change the path to the location of your okd.io directory) sudo semanage fcontext -a -t container_file_t '/home/brian/Documents/projects/okd.io(/.*)?' sudo restorecon -Rv /home/brian/Documents/projects/okd.io Creating the container \u00b6 To create the container image on your local system choose the appropriate command from the list: if you are using the docker command on Linux, Mac OS or Windows: docker build -t mkdocs-build -f ./dev/Dockerfile . if you are using the podman command on Linux: podman build -t mkdocs-build -f ./dev/Dockerfile . if you have npm and use Docker on Linux, Mac OS or Windows: npm run docker-build-image if you have npm and use Podman on Linux: npm run podman-build-image This will build a local container image named mkdocs-build Live editing of the content \u00b6 To change the content of the web site you can use your preferred editing application. To see the changes you can run a live local copy of okd.io that will automatically update as you save local changes. Ensure you have the local container image, built in the previous step, available on your system then choose the appropriate command from the list: if you are using the docker command on Linux or Mac OS: docker run -it --rm --name mkdocs-serve -p 8000 :8000 -v ` pwd ` :/site mkdocs-build if you are using the podman command on Linux: podman run -it --rm --name mkdocs-serve -p 8000 :8000 -v ` pwd ` :/site mkdocs-build if you are on Windows using the docker command in Powershell: docker run -it - -rm - -name mkdocs-build -p 8000 : 8000 -v \" $( pwd ) :/site\" mkdocs-build if you are on Windows using the docker command in CMD prompt: docker run -it --rm --name mkdocs-build -p 8000:8000 -v %cd%:/site mkdocs-build if you have npm and use Docker on Linux or Mac OS: npm run docker-serve if you have npm on Windows: npm run win-docker-serve if you have npm and use Podman on Linux: npm run podman-serve You can now open a browser to localhost:8000 . You should see the okd.io web site in the browser. As you change files on your local system the web pages will automatically update. When you have completed editing the site use Ctrl-c (hold down the control key then press c) to quit the site. Build and validate the site \u00b6 Before you submit any changes to the site in a pull request please check there are no spelling mistakes or broken links , by running the build script and checking the output. The build script will create or update the static web site in the public directory - this is what will be created and published as the live site if you submit a pull request with your modifications. if you are using the docker command on Linux or Mac OS: docker run -it --rm --name mkdocs-build -p -v ` pwd ` :/site --entrypoint /site/build.sh mkdocs-build if you are using the podman command on Linux: podman run -it --rm --name mkdocs-build -p -v ` pwd ` :/site --entrypoint /site/build.sh mkdocs-build if you are on Windows using the docker command in Powershell: docker run -it - -rm - -name mkdocs-build -v \" $( pwd ) :/site\" - -entrypoint / site / build . sh mkdocs-build if you are on Windows using the docker command in CMD prompt: docker run -it --rm --name mkdocs-build -v %cd%:/site --entrypoint /site/build.sh mkdocs-build if you have npm and use Docker on Linux or Mac OS: npm run docker-build if you have npm on Windows: npm run win-docker-build if you have npm and use Podman on Linux: npm run podman-build You should verify there are no spelling mistakes, by finding the last line of the CSpell output: CSpell: Files checked: 31, Issues found: 0 in 0 files Further down in the console output wil be the summary of the link checker: That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site. Editing on cluster There is a community operator available in the OperatorHub on OKD to install Eclipse Che, the upstream project for Red Hat CodeReady Workspaces. You can use Che to modify site content through your browser, with your OKD cluster hosting the workspace and developer environment. You need to have access to an OKD cluster and have the Che operator installed and an Che instance deployed and running. In your OKD console, you should have an applications link in the top toolbar. Open the Applications menu (3x3 grid icon) and select Che. This will open the Che application - Google Chrome is the supported browser and will give the best user experience. In the Che console side menu, select to Create Workspace , then in the Import from Git section add the URL of your fork of the okd.io git repository (should be similar to https://github.com/<user or org name>/okd.io.git ) then press Create & Open to start the workspace. After a short while the workspace will open ( the cluster has to download and start a number of containers, so the first run may take a few minutes depending on your cluster network access ). When the workspace is displayed you may have to wait a few seconds for the workspace to initialize and clone your git repo into the workspace. You may also get asked if you trust the author of the git repository, answer yes to this question. Your environment should then be ready to start work. The web based developer environment uses the same code base as Microsoft Visual Studio Code, so provides a similar user experience, but within your browser. Live editing of the content \u00b6 To change the content of the web site you can use the in browser editor provided by Che. To see the changes you can run a live local copy of okd.io that will automatically update as you save local changes. On the right side of the workspace window you should see 3 icons, hovering over them should reveal they are the Outline , Endpoints and Workspace . Clicking into the workspace, you should see a User Runtimes section with the option to open a new terminal, then 2 commands (Live edit and Build) and finally a link to launch MkDocs web site (initially this link will not work) To allow you to see your changes in a live site (where any change you save will automatically be updated on the site) click on the 1. Live edit link. This will launch a new terminal window where the mkdocs serve command will run, which provides a local live site. However, as you are running the development site on a cluster, the Che runtime automatically makes this site available to you. The MkDocs link now points to the site, but you will be asked if you want to open the site in a new tab or in Preview. Preview will add a 4th icon to the side toolbar and open the web site in the side panel. You can drag the side of the window to resize the browser view to allow you to edit on the left and view the results on the right of your browser window. If you have multiple monitors you may want to select to open the website in a new Tab or use the MkDocs link, then drag the browser tab on to a different monitor. By default, the Che environment auto-saves any file modification after half a second of no activity. You can alter this in the preferences section. When ever a file is saved the live site will update in the browser. When you finished editing simply close the terminal window running the Live edit script. This will stop the web server running the preview site. Build and validate the site \u00b6 The build script will create or update the static web site in the public directory - this is what will be created and published as the live site if you submit a pull request with your modifications. To run the build script simply click the 2. Build link in the Workspace panel. You should verify there are no spelling mistakes, by finding the last line of the CSpell output: CSpell: Files checked: 31, Issues found: 0 in 0 files Further down in the console output wil be the summary of the link checker: That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site. Local mkdocs and python tooling installation You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Install Node.js on your system Clone your fork of the okd.io repository cd into the local repo directory (./okd.io) Install the required python packages `pip install -r requirements.txt' Install the spell checker using command npm ci . If you want to use the cspell command on the command line, then you need to install it globally npm -g i cspell Note sudo command may be needed to install globally, depending on your system configuration You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located. The static site will be created/updated in the public directory mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command linkchecker -f linkcheckerrc --check-extern public . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling cspell docs/**/*.md should be run in the root folder of the project, containing the cspell.json file. There is also a convenience script ./build.sh in the root of the repository that will check spelling, build the site then run the link checker. You should verify there are no spelling mistakes, by finding the last line of the CSpell output: CSpell: Files checked: 31, Issues found: 0 in 0 files Similarly, the link checker creates a summary after checking the site: That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site.","title":"Setup environment"},{"location":"wg_docs/doc-env/#setting-up-a-documentation-environment","text":"To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: MkDocs with the Materials theme CSpell linkchecker Node.js Python 3 You can create the environment by : running the tooling within a container runtime, so you don't need to do any local installs if you have an Eclipse Che installation on an OKD cluster then you can make the changes using only a browser, with all the tooling running inside Che on the OKD cluster. installing the components on your local system Tooling within a container You can use a container to run MkDocs so no local installation is required, however you do need to have Docker Desktop installed if using Mac OS or Windows. If running on Linux you can use Docker or Podman . If you have a node.js environment installed that includes the npm command then you can make use of the run scripts provided in the project to run the docker or podman commands The following commands all assume you are working in the root directory of your local git clone of your forked copy of the okd.io git repo. (your working directory should contain mkdocs.yml and package.json files) Warning If you are using Linux with SELinux enabled, then you need to configure your system to allow the local directory containing the cloned git repo to be mounted inside a container. The following commands will configure SELinux to allow this: (change the path to the location of your okd.io directory) sudo semanage fcontext -a -t container_file_t '/home/brian/Documents/projects/okd.io(/.*)?' sudo restorecon -Rv /home/brian/Documents/projects/okd.io","title":"Setting up a documentation environment"},{"location":"wg_docs/doc-env/#creating-the-container","text":"To create the container image on your local system choose the appropriate command from the list: if you are using the docker command on Linux, Mac OS or Windows: docker build -t mkdocs-build -f ./dev/Dockerfile . if you are using the podman command on Linux: podman build -t mkdocs-build -f ./dev/Dockerfile . if you have npm and use Docker on Linux, Mac OS or Windows: npm run docker-build-image if you have npm and use Podman on Linux: npm run podman-build-image This will build a local container image named mkdocs-build","title":"Creating the container"},{"location":"wg_docs/doc-env/#live-editing-of-the-content","text":"To change the content of the web site you can use your preferred editing application. To see the changes you can run a live local copy of okd.io that will automatically update as you save local changes. Ensure you have the local container image, built in the previous step, available on your system then choose the appropriate command from the list: if you are using the docker command on Linux or Mac OS: docker run -it --rm --name mkdocs-serve -p 8000 :8000 -v ` pwd ` :/site mkdocs-build if you are using the podman command on Linux: podman run -it --rm --name mkdocs-serve -p 8000 :8000 -v ` pwd ` :/site mkdocs-build if you are on Windows using the docker command in Powershell: docker run -it - -rm - -name mkdocs-build -p 8000 : 8000 -v \" $( pwd ) :/site\" mkdocs-build if you are on Windows using the docker command in CMD prompt: docker run -it --rm --name mkdocs-build -p 8000:8000 -v %cd%:/site mkdocs-build if you have npm and use Docker on Linux or Mac OS: npm run docker-serve if you have npm on Windows: npm run win-docker-serve if you have npm and use Podman on Linux: npm run podman-serve You can now open a browser to localhost:8000 . You should see the okd.io web site in the browser. As you change files on your local system the web pages will automatically update. When you have completed editing the site use Ctrl-c (hold down the control key then press c) to quit the site.","title":"Live editing of the content"},{"location":"wg_docs/doc-env/#build-and-validate-the-site","text":"Before you submit any changes to the site in a pull request please check there are no spelling mistakes or broken links , by running the build script and checking the output. The build script will create or update the static web site in the public directory - this is what will be created and published as the live site if you submit a pull request with your modifications. if you are using the docker command on Linux or Mac OS: docker run -it --rm --name mkdocs-build -p -v ` pwd ` :/site --entrypoint /site/build.sh mkdocs-build if you are using the podman command on Linux: podman run -it --rm --name mkdocs-build -p -v ` pwd ` :/site --entrypoint /site/build.sh mkdocs-build if you are on Windows using the docker command in Powershell: docker run -it - -rm - -name mkdocs-build -v \" $( pwd ) :/site\" - -entrypoint / site / build . sh mkdocs-build if you are on Windows using the docker command in CMD prompt: docker run -it --rm --name mkdocs-build -v %cd%:/site --entrypoint /site/build.sh mkdocs-build if you have npm and use Docker on Linux or Mac OS: npm run docker-build if you have npm on Windows: npm run win-docker-build if you have npm and use Podman on Linux: npm run podman-build You should verify there are no spelling mistakes, by finding the last line of the CSpell output: CSpell: Files checked: 31, Issues found: 0 in 0 files Further down in the console output wil be the summary of the link checker: That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site. Editing on cluster There is a community operator available in the OperatorHub on OKD to install Eclipse Che, the upstream project for Red Hat CodeReady Workspaces. You can use Che to modify site content through your browser, with your OKD cluster hosting the workspace and developer environment. You need to have access to an OKD cluster and have the Che operator installed and an Che instance deployed and running. In your OKD console, you should have an applications link in the top toolbar. Open the Applications menu (3x3 grid icon) and select Che. This will open the Che application - Google Chrome is the supported browser and will give the best user experience. In the Che console side menu, select to Create Workspace , then in the Import from Git section add the URL of your fork of the okd.io git repository (should be similar to https://github.com/<user or org name>/okd.io.git ) then press Create & Open to start the workspace. After a short while the workspace will open ( the cluster has to download and start a number of containers, so the first run may take a few minutes depending on your cluster network access ). When the workspace is displayed you may have to wait a few seconds for the workspace to initialize and clone your git repo into the workspace. You may also get asked if you trust the author of the git repository, answer yes to this question. Your environment should then be ready to start work. The web based developer environment uses the same code base as Microsoft Visual Studio Code, so provides a similar user experience, but within your browser.","title":"Build and validate the site"},{"location":"wg_docs/doc-env/#live-editing-of-the-content_1","text":"To change the content of the web site you can use the in browser editor provided by Che. To see the changes you can run a live local copy of okd.io that will automatically update as you save local changes. On the right side of the workspace window you should see 3 icons, hovering over them should reveal they are the Outline , Endpoints and Workspace . Clicking into the workspace, you should see a User Runtimes section with the option to open a new terminal, then 2 commands (Live edit and Build) and finally a link to launch MkDocs web site (initially this link will not work) To allow you to see your changes in a live site (where any change you save will automatically be updated on the site) click on the 1. Live edit link. This will launch a new terminal window where the mkdocs serve command will run, which provides a local live site. However, as you are running the development site on a cluster, the Che runtime automatically makes this site available to you. The MkDocs link now points to the site, but you will be asked if you want to open the site in a new tab or in Preview. Preview will add a 4th icon to the side toolbar and open the web site in the side panel. You can drag the side of the window to resize the browser view to allow you to edit on the left and view the results on the right of your browser window. If you have multiple monitors you may want to select to open the website in a new Tab or use the MkDocs link, then drag the browser tab on to a different monitor. By default, the Che environment auto-saves any file modification after half a second of no activity. You can alter this in the preferences section. When ever a file is saved the live site will update in the browser. When you finished editing simply close the terminal window running the Live edit script. This will stop the web server running the preview site.","title":"Live editing of the content"},{"location":"wg_docs/doc-env/#build-and-validate-the-site_1","text":"The build script will create or update the static web site in the public directory - this is what will be created and published as the live site if you submit a pull request with your modifications. To run the build script simply click the 2. Build link in the Workspace panel. You should verify there are no spelling mistakes, by finding the last line of the CSpell output: CSpell: Files checked: 31, Issues found: 0 in 0 files Further down in the console output wil be the summary of the link checker: That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site. Local mkdocs and python tooling installation You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Install Node.js on your system Clone your fork of the okd.io repository cd into the local repo directory (./okd.io) Install the required python packages `pip install -r requirements.txt' Install the spell checker using command npm ci . If you want to use the cspell command on the command line, then you need to install it globally npm -g i cspell Note sudo command may be needed to install globally, depending on your system configuration You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located. The static site will be created/updated in the public directory mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command linkchecker -f linkcheckerrc --check-extern public . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling cspell docs/**/*.md should be run in the root folder of the project, containing the cspell.json file. There is also a convenience script ./build.sh in the root of the repository that will check spelling, build the site then run the link checker. You should verify there are no spelling mistakes, by finding the last line of the CSpell output: CSpell: Files checked: 31, Issues found: 0 in 0 files Similarly, the link checker creates a summary after checking the site: That's it. 662 links in 695 URLs checked. 0 warnings found. 0 errors found Any issues reported should be fixed before submitting a pull request to add your changes to the okd.io site.","title":"Build and validate the site"},{"location":"wg_docs/okd-io/","text":"Contributing to okd.io \u00b6 The source for okd.io is in a github repository . The site is created using MkDocs . which takes Markdown documents and turns them into a static website that can be accessed from a filesystem or served from a web server. To update or add new content to the site you need to fork the repository in your own GitHub account it important to leave the repo name as okd.io in your GitHub account if you want to use Che/CodeReady Containers to modify the content make the changes in your local repository create a pull request to deliver the updates to the primary repository. The site is created using MkDocs with the Materials theme theme. Updating the site \u00b6 To make changes to the site. Create a pull request to deliver the changes in your fork of the repo to the main branch of the okd.io repo. Before creating a pull request you should run the build script and verify there are no spelling mistakes or broken links. Details on how to do this can be found at the end of the instructions for setting up a documentation environment Github automation is used to generate the site then publish to GitHub pages, which serves the site. If your changes contain spelling issues or broken links, then the automation will fail and the GitHub pages site will not be updated, so please do a local test using the build.sh script before creating the pull request.","title":"Modifying OKD.io"},{"location":"wg_docs/okd-io/#contributing-to-okdio","text":"The source for okd.io is in a github repository . The site is created using MkDocs . which takes Markdown documents and turns them into a static website that can be accessed from a filesystem or served from a web server. To update or add new content to the site you need to fork the repository in your own GitHub account it important to leave the repo name as okd.io in your GitHub account if you want to use Che/CodeReady Containers to modify the content make the changes in your local repository create a pull request to deliver the updates to the primary repository. The site is created using MkDocs with the Materials theme theme.","title":"Contributing to okd.io"},{"location":"wg_docs/okd-io/#updating-the-site","text":"To make changes to the site. Create a pull request to deliver the changes in your fork of the repo to the main branch of the okd.io repo. Before creating a pull request you should run the build script and verify there are no spelling mistakes or broken links. Details on how to do this can be found at the end of the instructions for setting up a documentation environment Github automation is used to generate the site then publish to GitHub pages, which serves the site. If your changes contain spelling issues or broken links, then the automation will fail and the GitHub pages site will not be updated, so please do a local test using the build.sh script before creating the pull request.","title":"Updating the site"},{"location":"wg_docs/overview/","text":"Documentation Subgroup \u00b6 The Documentation working group is responsible for improving the OKD documentation. Both the community documentation (this site) and the product documentation. Joining the group \u00b6 The Documentation Subgroup is open to all. You don't need to be invited to join, just attend on of the bi-weekly video calls: Calendar link : OKD working group calendar Agenda link : Documentation working group agenda and meeting nodes Product Documentation \u00b6 The OKD product documentation is maintained in the same git repository as Red Hat OpenShift product documentation, as they are sibling projects and largely share the same source code. The process for making changes to the documentation is outlined in the documentation section Community Documentation \u00b6 This site is the community documentation. It is hosted on github and uses a static site generator to convert the Markdown documents in the git repo into this website. Details of how to modify the site content is contained on the page Modifying OKD.io .","title":"Overview"},{"location":"wg_docs/overview/#documentation-subgroup","text":"The Documentation working group is responsible for improving the OKD documentation. Both the community documentation (this site) and the product documentation.","title":"Documentation Subgroup"},{"location":"wg_docs/overview/#joining-the-group","text":"The Documentation Subgroup is open to all. You don't need to be invited to join, just attend on of the bi-weekly video calls: Calendar link : OKD working group calendar Agenda link : Documentation working group agenda and meeting nodes","title":"Joining the group"},{"location":"wg_docs/overview/#product-documentation","text":"The OKD product documentation is maintained in the same git repository as Red Hat OpenShift product documentation, as they are sibling projects and largely share the same source code. The process for making changes to the documentation is outlined in the documentation section","title":"Product Documentation"},{"location":"wg_docs/overview/#community-documentation","text":"This site is the community documentation. It is hosted on github and uses a static site generator to convert the Markdown documents in the git repo into this website. Details of how to modify the site content is contained on the page Modifying OKD.io .","title":"Community Documentation"},{"location":"wg_virt/community/","text":"Get involved! \u00b6 The OKD Virtualization SIG is a group of people just like you who are aiming to promote the adoption of the virtualization components on OKD. Social Media \u00b6 Reddit : r/OKD Virtualization YouTube : OKD Workgroup meeting Twitter : Follow @OKD_Virt_SIG Getting started as a user (future contributor!) \u00b6 Before getting started, please read OKD community etiquette guidelines. Feel free to dive into OKD documentation following the installation guide for setting up your initial OKD deployment on your bare metal datacenter. Once it's up, please follow the OKD documentation regarding Virtualization installation. If you find difficulties during the process let us know! Please report issues in our GitHub tracker . TODO: we may switch to okd organization once it will be ready Getting started as contributor \u00b6 The OKD Virtualization SIG is a group of multidisciplinary individuals who are contributing code, writing documentation, reporting bugs, contributing UX and design expertise, and engaging with the community. Before getting started, we recommend that you: Join the OKD Workgroup Google Group and send a message presenting yourself and saying how you would like to contribute. Join the biweekly OKD Workgroup meetings ( agenda , previous meetings recordings ) Please read OKD community etiquette guidelines. (Quick summary: Be nice!) The OKD Virtualization SIG is a community project, and we welcome contributions from everyone! If you'd like to write code, report bugs, contribute designs, or enhance the documentation, we would love your help! Testing \u00b6 We're always eager to have new contributors to join improving the OKD Virtualization quality, no matter your experience level. Please try to deploy and use OKD Virtualization and report issues in our GitHub tracker . TODO: we may switch to okd organization once it will be ready Documentation \u00b6 OKD Virtualization documentation is mostly included in GitHub openshift-docs repository and we are working for getting it published on OKD documentation website Some additional documentation may be available within this SubGroup space. Supporters, Sponsors, and Providers \u00b6 OKD Virtualization SIG is still in its early days. If you are using, supporting or providing services with OKD Virtualization we would like to share your story here!","title":"Community"},{"location":"wg_virt/community/#get-involved","text":"The OKD Virtualization SIG is a group of people just like you who are aiming to promote the adoption of the virtualization components on OKD.","title":"Get involved!"},{"location":"wg_virt/community/#social-media","text":"Reddit : r/OKD Virtualization YouTube : OKD Workgroup meeting Twitter : Follow @OKD_Virt_SIG","title":"Social Media"},{"location":"wg_virt/community/#getting-started-as-a-user-future-contributor","text":"Before getting started, please read OKD community etiquette guidelines. Feel free to dive into OKD documentation following the installation guide for setting up your initial OKD deployment on your bare metal datacenter. Once it's up, please follow the OKD documentation regarding Virtualization installation. If you find difficulties during the process let us know! Please report issues in our GitHub tracker . TODO: we may switch to okd organization once it will be ready","title":"Getting started as a user (future contributor!)"},{"location":"wg_virt/community/#getting-started-as-contributor","text":"The OKD Virtualization SIG is a group of multidisciplinary individuals who are contributing code, writing documentation, reporting bugs, contributing UX and design expertise, and engaging with the community. Before getting started, we recommend that you: Join the OKD Workgroup Google Group and send a message presenting yourself and saying how you would like to contribute. Join the biweekly OKD Workgroup meetings ( agenda , previous meetings recordings ) Please read OKD community etiquette guidelines. (Quick summary: Be nice!) The OKD Virtualization SIG is a community project, and we welcome contributions from everyone! If you'd like to write code, report bugs, contribute designs, or enhance the documentation, we would love your help!","title":"Getting started as contributor"},{"location":"wg_virt/community/#testing","text":"We're always eager to have new contributors to join improving the OKD Virtualization quality, no matter your experience level. Please try to deploy and use OKD Virtualization and report issues in our GitHub tracker . TODO: we may switch to okd organization once it will be ready","title":"Testing"},{"location":"wg_virt/community/#documentation","text":"OKD Virtualization documentation is mostly included in GitHub openshift-docs repository and we are working for getting it published on OKD documentation website Some additional documentation may be available within this SubGroup space.","title":"Documentation"},{"location":"wg_virt/community/#supporters-sponsors-and-providers","text":"OKD Virtualization SIG is still in its early days. If you are using, supporting or providing services with OKD Virtualization we would like to share your story here!","title":"Supporters, Sponsors, and Providers"},{"location":"wg_virt/overview/","text":"OKD Virtualization Subgroup \u00b6 The Goal of the OKD Virtualization Subgroup is to provide an integrated solution for classical virtualization users based on OKD , HCO and KubeVirt , including a graphical user interface and deployed using bare metal suited method. Meet our community ! Documentation \u00b6 Guide to installing OKD Virtualization on Bare Metal UPI Projects \u00b6 The OKD Virtualization Subgroup is monitoring and integrating the following projects in a user consumable virtualization solution: OKD - as the platform KubeVirt - as the virtualization plugin HyperConverged Cluster Operator (HCO) - for supporting tools Rook ? (Or whatever the upstream operator is called) for feature rich data storage Medik8s and NHC for high-availability Konveyor for migration from other platforms Faros deploy on small footprint, bare-metal clusters Deployment \u00b6 UPI first Assisted Installer ? for easy provisioning of OKD on bare-metal nodes Mailing List & Slack \u00b6 OKD Workgroup Google Group: https://groups.google.com/forum/#!forum/okd-wg Slack Channel: https://kubernetes.slack.com/messages/openshift-dev TODO \u00b6 some social activity like a blog post and more upstream documentation improve reliability of testing and CI on OKD SIG Membership \u00b6 Fabian Deutsch (Red Hat) Sandro Bonazzola (Red Hat) Simone Tiraboschi (Red Hat) Michal Skrivanek (Red Hat) Resources for the SIG \u00b6 Automation in place: \u00b6 HCO main branch gets tested against OKD 4.9: https://github.com/openshift/release/blob/master/ci-operator/config/kubevirt/hyperconverged-cluster-operator/kubevirt-hyperconverged-cluster-operator-main__okd.yaml HCO precondition job: https://prow.ci.openshift.org/job-history/gs/origin-ci-test/pr-logs/directory/pull-ci-kubevirt-hyperconverged-cluster-operator-main-okd-hco-e2e-image-index-gcp KubeVirt is uploaded to operatorhub and on community-operators: https://github.com/redhat-openshift-ecosystem/community-operators-prod/tree/main/operators/community-kubevirt-hyperconverged","title":"Overview"},{"location":"wg_virt/overview/#okd-virtualization-subgroup","text":"The Goal of the OKD Virtualization Subgroup is to provide an integrated solution for classical virtualization users based on OKD , HCO and KubeVirt , including a graphical user interface and deployed using bare metal suited method. Meet our community !","title":"OKD Virtualization Subgroup"},{"location":"wg_virt/overview/#documentation","text":"Guide to installing OKD Virtualization on Bare Metal UPI","title":"Documentation"},{"location":"wg_virt/overview/#projects","text":"The OKD Virtualization Subgroup is monitoring and integrating the following projects in a user consumable virtualization solution: OKD - as the platform KubeVirt - as the virtualization plugin HyperConverged Cluster Operator (HCO) - for supporting tools Rook ? (Or whatever the upstream operator is called) for feature rich data storage Medik8s and NHC for high-availability Konveyor for migration from other platforms Faros deploy on small footprint, bare-metal clusters","title":"Projects"},{"location":"wg_virt/overview/#deployment","text":"UPI first Assisted Installer ? for easy provisioning of OKD on bare-metal nodes","title":"Deployment"},{"location":"wg_virt/overview/#mailing-list-slack","text":"OKD Workgroup Google Group: https://groups.google.com/forum/#!forum/okd-wg Slack Channel: https://kubernetes.slack.com/messages/openshift-dev","title":"Mailing List &amp; Slack"},{"location":"wg_virt/overview/#todo","text":"some social activity like a blog post and more upstream documentation improve reliability of testing and CI on OKD","title":"TODO"},{"location":"wg_virt/overview/#sig-membership","text":"Fabian Deutsch (Red Hat) Sandro Bonazzola (Red Hat) Simone Tiraboschi (Red Hat) Michal Skrivanek (Red Hat)","title":"SIG Membership"},{"location":"wg_virt/overview/#resources-for-the-sig","text":"","title":"Resources for the SIG"},{"location":"wg_virt/overview/#automation-in-place","text":"HCO main branch gets tested against OKD 4.9: https://github.com/openshift/release/blob/master/ci-operator/config/kubevirt/hyperconverged-cluster-operator/kubevirt-hyperconverged-cluster-operator-main__okd.yaml HCO precondition job: https://prow.ci.openshift.org/job-history/gs/origin-ci-test/pr-logs/directory/pull-ci-kubevirt-hyperconverged-cluster-operator-main-okd-hco-e2e-image-index-gcp KubeVirt is uploaded to operatorhub and on community-operators: https://github.com/redhat-openshift-ecosystem/community-operators-prod/tree/main/operators/community-kubevirt-hyperconverged","title":"Automation in place:"},{"location":"working-group/minutes/WG-Meeting-Minutes-04-12-2022/","text":"OKD Working Group Meeting Notes \u00b6 April 12, 2022 \u00b6 Attendees: \u00b6 Jaime Magiera - University of Michigan Brian Innes - IBM Larry Brigman - CommScope Michael elmiko McCune - Red Hat Timoth\u00e9e Ravier - Red Hat Mohammad Reza Ostadi Daniel Axelrod - Datto Sri Ramanujam Bruce Link - BCIT Neal Gompa - Datto Christian Glombek - Red Hat Agenda \u00b6 Agenda Review OKD Release updates (Christian) https://github.com/systemd/systemd/pull/22868 (rejected, new try WIP) https://github.com/openshift/installer/pull/5788 (Installer fix for bootstrap api-int issue) FCOS updates (Timoth\u00e9e) Fedora 36 test day/week: https://github.com/coreos/fedora-coreos-tracker/issues/1147 https://github.com/coreos/fedora-coreos-tracker/issues/1123 https://github.com/coreos/fedora-coreos-tracker/issues/1101 Removing libvarlink-utils from FCOS: https://github.com/coreos/fedora-coreos-tracker/issues/1130 Update hardware version in VMware OVA after 6.5/6.7 EOL date: https://github.com/coreos/fedora-coreos-tracker/issues/1141 https://docs.fedoraproject.org/en-US/fedora-coreos/provisioning-vmware/#_modifying_ovf_metadata VirtualBox images coming soon! https://github.com/coreos/fedora-coreos-tracker/issues/1008 Docs updates (Brian, Jaime) Systems and Contributors guides https://binnes.github.io/okd-io/okd_tech_docs/ Meeting minutes going into the website Website styling http://luminouscoder.com/okd.io/public Automated testing Example: https://github.com/JaimeMagiera/oct Troubleshooting https://github.com/kxr/o-must-gather https://github.com/elmiko/okd-camgi https://github.com/elmiko/camgi.rs (rust rewrite) https://github.com/rvanderp3/release-devenv Issues Ceph/Rook issues with FCOS 35 release in OKD 4.10 (https://github.com/openshift/okd/issues/1160) NetworkPolicy - deny-all policy does not correctly restrict traffic to pod when using nodeports Discussions New Business CRC Survey Vote for subcommittee co-chairs Tasks Jaime - get 4.10 and 4.11 update info (ongoing) Charro Gruver - gather OKD download stats from dl.fedoraproject.org or delegate Daniel - Write up guides format into README in guides directory and then file tickets against people who have agreed to update them Ceate troubleshooting dicsussion thread (Brian)","title":"OKD Working Group Meeting Notes"},{"location":"working-group/minutes/WG-Meeting-Minutes-04-12-2022/#okd-working-group-meeting-notes","text":"","title":"OKD Working Group Meeting Notes"},{"location":"working-group/minutes/WG-Meeting-Minutes-04-12-2022/#april-12-2022","text":"","title":"April 12, 2022"},{"location":"working-group/minutes/WG-Meeting-Minutes-04-12-2022/#attendees","text":"Jaime Magiera - University of Michigan Brian Innes - IBM Larry Brigman - CommScope Michael elmiko McCune - Red Hat Timoth\u00e9e Ravier - Red Hat Mohammad Reza Ostadi Daniel Axelrod - Datto Sri Ramanujam Bruce Link - BCIT Neal Gompa - Datto Christian Glombek - Red Hat","title":"Attendees:"},{"location":"working-group/minutes/WG-Meeting-Minutes-04-12-2022/#agenda","text":"Agenda Review OKD Release updates (Christian) https://github.com/systemd/systemd/pull/22868 (rejected, new try WIP) https://github.com/openshift/installer/pull/5788 (Installer fix for bootstrap api-int issue) FCOS updates (Timoth\u00e9e) Fedora 36 test day/week: https://github.com/coreos/fedora-coreos-tracker/issues/1147 https://github.com/coreos/fedora-coreos-tracker/issues/1123 https://github.com/coreos/fedora-coreos-tracker/issues/1101 Removing libvarlink-utils from FCOS: https://github.com/coreos/fedora-coreos-tracker/issues/1130 Update hardware version in VMware OVA after 6.5/6.7 EOL date: https://github.com/coreos/fedora-coreos-tracker/issues/1141 https://docs.fedoraproject.org/en-US/fedora-coreos/provisioning-vmware/#_modifying_ovf_metadata VirtualBox images coming soon! https://github.com/coreos/fedora-coreos-tracker/issues/1008 Docs updates (Brian, Jaime) Systems and Contributors guides https://binnes.github.io/okd-io/okd_tech_docs/ Meeting minutes going into the website Website styling http://luminouscoder.com/okd.io/public Automated testing Example: https://github.com/JaimeMagiera/oct Troubleshooting https://github.com/kxr/o-must-gather https://github.com/elmiko/okd-camgi https://github.com/elmiko/camgi.rs (rust rewrite) https://github.com/rvanderp3/release-devenv Issues Ceph/Rook issues with FCOS 35 release in OKD 4.10 (https://github.com/openshift/okd/issues/1160) NetworkPolicy - deny-all policy does not correctly restrict traffic to pod when using nodeports Discussions New Business CRC Survey Vote for subcommittee co-chairs Tasks Jaime - get 4.10 and 4.11 update info (ongoing) Charro Gruver - gather OKD download stats from dl.fedoraproject.org or delegate Daniel - Write up guides format into README in guides directory and then file tickets against people who have agreed to update them Ceate troubleshooting dicsussion thread (Brian)","title":"Agenda"},{"location":"working-group/minutes/minutes/","text":"04-12-2022","title":"Minutes"}]}