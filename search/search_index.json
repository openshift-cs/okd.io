{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Built around a core of OCI container packaging and Kubernetes container cluster management, OKD is also augmented by application lifecycle management functionality and DevOps tooling. OKD provides a complete open source container application platform. OKD 4 \u00b6 $ openshift-install create cluster Tons of amazing new features Automatic updates not only for OKD but also for the host OS, k8s Operators are first class citizens, a fancy UI, and much much more CodeReady Containers for OKD: local OKD 4 cluster for development CodeReady Containers brings a minimal OpenShift 4 cluster to your local laptop or desktop computer! Download it here: CodeReady Containers for OKD Images What is OKD? \u00b6 OKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment OKD embeds Kubernetes and extends it with security and other integrated concepts OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams OKD is also referred to as Origin in github and in the documentation OKD is a sibling Kubernetes distribution to Red Hat OpenShift | If you are looking for enterprise-level support, or information on partner certification, Red Hat also offers Red Hat OpenShift Container Platform OKD Community \u00b6 We know you've got great ideas for improving OKD and its network of open source projects. So roll up your sleeves and come join us in the community! Get Started \u00b6 All contributions are welcome! OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the #openshift-dev on Kubernetes Slack Channel, or get involved in the OKD-WG by joining the OKD-WG google group. Get started with the Contributors Guide Fork the repository Read the documentation Read our charter Help Resolve an Open Issue Review our Apache 2 license Connect to the community \u00b6 Join the OKD Working Group Talk to Us \u00b6 Follow the public user or development mailing lists Chat with us on the #openshift-dev channel on Slack Standardization through Containerization \u00b6 Standards are powerful forces in the software industry. They can drive technology forward by bringing together the combined efforts of multiple developers, different communities, and even competing vendors. Open source container orchestration and cluster management at scale Standardized Linux container packaging for applications and their dependencies A container-focused OS that's designed for painless management in large clusters An open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator A lightweight container runtime for Kubernetes Prometheus is a systems and service monitoring toolkit that collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true OKD End User Community \u00b6 There is a large, vibrant end user community Become a part of something bigger \u00b6 OpenShift Commons is open to all community participants: users, operators, enterprises, non-profits, educational institutions, partners, and service providers as well as other open source technology initiatives utilized under the hood or to extend the OpenShift platform If you are an OpenShift Online or an OpenShift Container Platform customer or have deployed OKD on premise or on a public cloud If you have contributed to the OKD project and want to connect with your peers and end users If you simply want to stay up-to-date on the roadmap and best practices for using, deploying and operating OpenShift ... then OpenShift Commons is the right place for you","title":"Home"},{"location":"#okd-4","text":"$ openshift-install create cluster Tons of amazing new features Automatic updates not only for OKD but also for the host OS, k8s Operators are first class citizens, a fancy UI, and much much more CodeReady Containers for OKD: local OKD 4 cluster for development CodeReady Containers brings a minimal OpenShift 4 cluster to your local laptop or desktop computer! Download it here: CodeReady Containers for OKD Images","title":"OKD 4"},{"location":"#what-is-okd","text":"OKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment OKD embeds Kubernetes and extends it with security and other integrated concepts OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams OKD is also referred to as Origin in github and in the documentation OKD is a sibling Kubernetes distribution to Red Hat OpenShift | If you are looking for enterprise-level support, or information on partner certification, Red Hat also offers Red Hat OpenShift Container Platform","title":"What is OKD?"},{"location":"#okd-community","text":"We know you've got great ideas for improving OKD and its network of open source projects. So roll up your sleeves and come join us in the community!","title":"OKD Community"},{"location":"#get-started","text":"All contributions are welcome! OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the #openshift-dev on Kubernetes Slack Channel, or get involved in the OKD-WG by joining the OKD-WG google group. Get started with the Contributors Guide Fork the repository Read the documentation Read our charter Help Resolve an Open Issue Review our Apache 2 license","title":"Get Started"},{"location":"#connect-to-the-community","text":"Join the OKD Working Group","title":"Connect to the community"},{"location":"#talk-to-us","text":"Follow the public user or development mailing lists Chat with us on the #openshift-dev channel on Slack","title":"Talk to Us"},{"location":"#standardization-through-containerization","text":"Standards are powerful forces in the software industry. They can drive technology forward by bringing together the combined efforts of multiple developers, different communities, and even competing vendors. Open source container orchestration and cluster management at scale Standardized Linux container packaging for applications and their dependencies A container-focused OS that's designed for painless management in large clusters An open source project that provides developer and runtime Kubernetes tools, enabling you to accelerate the development of an Operator A lightweight container runtime for Kubernetes Prometheus is a systems and service monitoring toolkit that collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true","title":"Standardization through Containerization"},{"location":"#okd-end-user-community","text":"There is a large, vibrant end user community","title":"OKD End User Community"},{"location":"#become-a-part-of-something-bigger","text":"OpenShift Commons is open to all community participants: users, operators, enterprises, non-profits, educational institutions, partners, and service providers as well as other open source technology initiatives utilized under the hood or to extend the OpenShift platform If you are an OpenShift Online or an OpenShift Container Platform customer or have deployed OKD on premise or on a public cloud If you have contributed to the OKD project and want to connect with your peers and end users If you simply want to stay up-to-date on the roadmap and best practices for using, deploying and operating OpenShift ... then OpenShift Commons is the right place for you","title":"Become a part of something bigger"},{"location":"about/","text":"About OKD \u00b6 OKD is the community distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams. OKD is also referred to as Origin in github and in the documentation. OKD makes launching Kubernetes on any cloud or bare metal a snap, simplifies running and updating clusters, and provides all of the tools to make your containerized-applications succeed. Features \u00b6 A fully automated distribution of Kubernetes on all major clouds and bare metal, OpenStack, and other virtualization providers Easily build applications with integrated service discovery and persistent storage. Quickly and easily scale applications to handle periods of increased demand. Support for automatic high availability, load balancing, health checking, and failover. Access to the Operator Hub for extending Kubernetes with new, automated lifecycle capabilities Developer centric tooling and console for building containerized applications on Kubernetes Push source code to your Git repository and automatically deploy containerized applications. Web console and command-line client for building and monitoring applications. Centralized administration and management of an entire stack, team, or organization. Create reusable templates for components of your system, and iteratively deploy them over time. Roll out modifications to software stacks to your entire organization in a controlled fashion. Integration with your existing authentication mechanisms, including LDAP, Active Directory, and public OAuth providers such as GitHub. Multi-tenancy support, including team and user isolation of containers, builds, and network communication. Allow developers to run containers securely with fine-grained controls in production. Limit, track, and manage the developers and teams on the platform. Integrated container image registry, automatic edge load balancing, and full spectrum monitoring with Prometheus. What can I run on OKD? \u00b6 OKD is designed to run any Kubernetes workload. It also assists in building and developing containerized applications through the developer console. For an easier experience running your source code, Source-to-Image (S2I) allows developers to simply provide an application source repository containing code to build and run. It works by combining an existing S2I-enabled container image with application source to produce a new runnable image for your application. You can see the full list of Source-to-Image builder images and it's straightforward to create your own . Some of our available images include: Ruby Python Node.js PHP Perl WildFly MySQL MongoDB PostgreSQL MariaDB What sorts of security controls does OpenShift provide for containers? \u00b6 OKD runs with the following security policy by default: Containers run as a non-root unique user that is separate from other system users They cannot access host resources, run privileged, or become root They are given CPU and memory limits defined by the system administrator Any persistent storage they access will be under a unique SELinux label, which prevents others from seeing their content These settings are per project, so containers in different projects cannot see each other by default Regular users can run Docker, source, and custom builds By default, Docker builds can (and often do) run as root. You can control who can create Docker builds through the builds/docker and builds/custom policy resource. Regular users and project admins cannot change their security quotas. Many containers expect to run as root (and therefore edit all the contents of the filesystem). The Image Author's guide gives recommendations on making your image more secure by default: Don't run as root Make directories you want to write to group-writable and owned by group id 0 Set the net-bind capability on your executables if they need to bind to ports < 1024 If you are running your own cluster and want to run a container as root, you can grant that permission to the containers in your current project with the following command: # Gives the default service account in the current project access to run as UID 0 (root) oc adm add-scc-to-user anyuid -z default See the security documentation more on confining applications.","title":"About"},{"location":"about/#about-okd","text":"OKD is the community distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD adds developer and operations-centric tools on top of Kubernetes to enable rapid application development, easy deployment and scaling, and long-term lifecycle maintenance for small and large teams. OKD is also referred to as Origin in github and in the documentation. OKD makes launching Kubernetes on any cloud or bare metal a snap, simplifies running and updating clusters, and provides all of the tools to make your containerized-applications succeed.","title":"About OKD"},{"location":"about/#features","text":"A fully automated distribution of Kubernetes on all major clouds and bare metal, OpenStack, and other virtualization providers Easily build applications with integrated service discovery and persistent storage. Quickly and easily scale applications to handle periods of increased demand. Support for automatic high availability, load balancing, health checking, and failover. Access to the Operator Hub for extending Kubernetes with new, automated lifecycle capabilities Developer centric tooling and console for building containerized applications on Kubernetes Push source code to your Git repository and automatically deploy containerized applications. Web console and command-line client for building and monitoring applications. Centralized administration and management of an entire stack, team, or organization. Create reusable templates for components of your system, and iteratively deploy them over time. Roll out modifications to software stacks to your entire organization in a controlled fashion. Integration with your existing authentication mechanisms, including LDAP, Active Directory, and public OAuth providers such as GitHub. Multi-tenancy support, including team and user isolation of containers, builds, and network communication. Allow developers to run containers securely with fine-grained controls in production. Limit, track, and manage the developers and teams on the platform. Integrated container image registry, automatic edge load balancing, and full spectrum monitoring with Prometheus.","title":"Features"},{"location":"about/#what-can-i-run-on-okd","text":"OKD is designed to run any Kubernetes workload. It also assists in building and developing containerized applications through the developer console. For an easier experience running your source code, Source-to-Image (S2I) allows developers to simply provide an application source repository containing code to build and run. It works by combining an existing S2I-enabled container image with application source to produce a new runnable image for your application. You can see the full list of Source-to-Image builder images and it's straightforward to create your own . Some of our available images include: Ruby Python Node.js PHP Perl WildFly MySQL MongoDB PostgreSQL MariaDB","title":"What can I run on OKD?"},{"location":"about/#what-sorts-of-security-controls-does-openshift-provide-for-containers","text":"OKD runs with the following security policy by default: Containers run as a non-root unique user that is separate from other system users They cannot access host resources, run privileged, or become root They are given CPU and memory limits defined by the system administrator Any persistent storage they access will be under a unique SELinux label, which prevents others from seeing their content These settings are per project, so containers in different projects cannot see each other by default Regular users can run Docker, source, and custom builds By default, Docker builds can (and often do) run as root. You can control who can create Docker builds through the builds/docker and builds/custom policy resource. Regular users and project admins cannot change their security quotas. Many containers expect to run as root (and therefore edit all the contents of the filesystem). The Image Author's guide gives recommendations on making your image more secure by default: Don't run as root Make directories you want to write to group-writable and owned by group id 0 Set the net-bind capability on your executables if they need to bind to ports < 1024 If you are running your own cluster and want to run a container as root, you can grant that permission to the containers in your current project with the following command: # Gives the default service account in the current project access to run as UID 0 (root) oc adm add-scc-to-user anyuid -z default See the security documentation more on confining applications.","title":"What sorts of security controls does OpenShift provide for containers?"},{"location":"blog/","text":"okd.io Blog \u00b6 We look forward to sharing news and useful information about OKD in this blog. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io. Blogs \u00b6 2021 \u00b6 Date Title 2021-05-06 OKD Working Group Office Hours at KubeconEU on OpenShift.tv 2021-05-04 Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift 2021-03-22 Recap OKD Testing and Deployment Workshop - Videos and Additional Resources 2021-04-19 Please avoid using FCOS 33.20210301.3.1 for new OKD installs 2021-03-16 Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now! 2021-03-07 okd.io now has a blog 2021-02-18 Demo blog article for authors","title":"Blog"},{"location":"blog/#okdio-blog","text":"We look forward to sharing news and useful information about OKD in this blog. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io.","title":"okd.io Blog"},{"location":"blog/#blogs","text":"","title":"Blogs"},{"location":"blog/#2021","text":"Date Title 2021-05-06 OKD Working Group Office Hours at KubeconEU on OpenShift.tv 2021-05-04 Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift 2021-03-22 Recap OKD Testing and Deployment Workshop - Videos and Additional Resources 2021-04-19 Please avoid using FCOS 33.20210301.3.1 for new OKD installs 2021-03-16 Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now! 2021-03-07 okd.io now has a blog 2021-02-18 Demo blog article for authors","title":"2021"},{"location":"community/","text":"End User Community \u00b6 OKD has an active community of end-users, with many different use-cases. From enterprises, academic institutions or home hobbyists. In addition to the end-user community there is a smaller community of volunteers that contribute to the OKD project by helping other users resolve issues or by participating in one of the OKD working groups to enhance the OKD project. Code of Conduct \u00b6 We want the OKD community to be a welcoming community, where everyone is treat with respect. Todo Add code of conduct content or link to content Red Hat supports the Inclusive Naming Initiative and the OKD project follows the guidelines and recommendations from that project. All contributions to OKD must also follow their guidelines End-User community \u00b6 The community of OKD users is a self-supporting community. There is no official support for OKD, all help is community provided. The Help section provides details on how to get help for any issues you may be experiencing. We encourage all users to participate in discussions and to help fellow users where they can. Contributing to OKD \u00b6 The OKD project has a charter , setting out how the project is run. If you want to join the team of volunteers working on the OKD project then details of how to become a contributor are set out here .","title":"OKD Community"},{"location":"community/#end-user-community","text":"OKD has an active community of end-users, with many different use-cases. From enterprises, academic institutions or home hobbyists. In addition to the end-user community there is a smaller community of volunteers that contribute to the OKD project by helping other users resolve issues or by participating in one of the OKD working groups to enhance the OKD project.","title":"End User Community"},{"location":"community/#code-of-conduct","text":"We want the OKD community to be a welcoming community, where everyone is treat with respect. Todo Add code of conduct content or link to content Red Hat supports the Inclusive Naming Initiative and the OKD project follows the guidelines and recommendations from that project. All contributions to OKD must also follow their guidelines","title":"Code of Conduct"},{"location":"community/#end-user-community_1","text":"The community of OKD users is a self-supporting community. There is no official support for OKD, all help is community provided. The Help section provides details on how to get help for any issues you may be experiencing. We encourage all users to participate in discussions and to help fellow users where they can.","title":"End-User community"},{"location":"community/#contributing-to-okd","text":"The OKD project has a charter , setting out how the project is run. If you want to join the team of volunteers working on the OKD project then details of how to become a contributor are set out here .","title":"Contributing to OKD"},{"location":"contributor/","text":"Contributor Community \u00b6 OKD is built from many different open source projects - Fedora CoreOS, the CentOS and UBI RPM ecosystems, cri-o, Kubernetes, and many different extensions to Kubernetes. The openshift organization on GitHub holds active development of components on top of Kubernetes and references projects built elsewhere. Generally, you'll want to find the component that interests you and review their README.md for the processes for contributing. Community process and questions can be raised in our community repo and issues opened in this repository (Bugzilla locations coming soon). Our unified continuous integration system tests pull requests to the ecosystem and core images, then builds and promotes them after merge. To see the latest development releases of OKD visit our continuous release page . These releases are built continuously and expire after a few days. Long lived versions are pinned and then listed on our stable release page . All contributions are welcome - OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the OKD discussion forum , or get involved in the Kubernetes project at the container runtime layer. Becoming a contributor \u00b6 The easiest way to get involved in the community is to: watch replays of previous working group meetings attend one of the working group meetings (no invite needed, just use the link in the calendar to join the BlueJeans video conference). join the OKD Working Group Google Group join the #openshift-dev channel on Slack The OKD project has a charter , setting out how the project is run. Working Groups \u00b6 The project is managed by a bi-weekly working group video call: Calendar link : OKD working group calendar Agenda link : OKD working group agenda and meeting nodes The main working group is where are the major project decisions are made, but when a specific work item needs to be completed a sub-group may be formed, so a focussed set of volunteers can work on a specific area. Sub groups \u00b6 Documentation working group Code-Ready Containers on OKD working group OKD virtualization working group","title":"Contributor"},{"location":"contributor/#contributor-community","text":"OKD is built from many different open source projects - Fedora CoreOS, the CentOS and UBI RPM ecosystems, cri-o, Kubernetes, and many different extensions to Kubernetes. The openshift organization on GitHub holds active development of components on top of Kubernetes and references projects built elsewhere. Generally, you'll want to find the component that interests you and review their README.md for the processes for contributing. Community process and questions can be raised in our community repo and issues opened in this repository (Bugzilla locations coming soon). Our unified continuous integration system tests pull requests to the ecosystem and core images, then builds and promotes them after merge. To see the latest development releases of OKD visit our continuous release page . These releases are built continuously and expire after a few days. Long lived versions are pinned and then listed on our stable release page . All contributions are welcome - OKD uses the Apache 2 license and does not require any contributor agreement to submit patches. Please open issues for any bugs or problems you encounter, ask questions in the OKD discussion forum , or get involved in the Kubernetes project at the container runtime layer.","title":"Contributor Community"},{"location":"contributor/#becoming-a-contributor","text":"The easiest way to get involved in the community is to: watch replays of previous working group meetings attend one of the working group meetings (no invite needed, just use the link in the calendar to join the BlueJeans video conference). join the OKD Working Group Google Group join the #openshift-dev channel on Slack The OKD project has a charter , setting out how the project is run.","title":"Becoming a contributor"},{"location":"contributor/#working-groups","text":"The project is managed by a bi-weekly working group video call: Calendar link : OKD working group calendar Agenda link : OKD working group agenda and meeting nodes The main working group is where are the major project decisions are made, but when a specific work item needs to be completed a sub-group may be formed, so a focussed set of volunteers can work on a specific area.","title":"Working Groups"},{"location":"contributor/#sub-groups","text":"Documentation working group Code-Ready Containers on OKD working group OKD virtualization working group","title":"Sub groups"},{"location":"crc/","text":"CodeReady Containers for OkD \u00b6 CodeReady Containers brings a minimal, single node OKD 4 cluster to your local computer. This cluster provides a minimal environment for development and testing purposes. CodeReady Containers is mainly targeted at running on developers' laptops and desktops. Download CodeReady Containers for OKD \u00b6 Run a developer instance of OKD4 on your local workstation with CodeReady Containers built for OKD - >No Pull Secret Required! CodeReady Containers for OKD4 - Mac OS image CodeReady Containers for OKD4 - Linux image CodeReady Containers for OKD4 - Windows image The Getting Started Guide explains how to install and use CodeReady Containers. If you encounter any problems, please open a discussion item in the OKD GitHub Community ! CRC Working group \u00b6 There is a working group looking at automating the OKD CRC build process. If you want technical details on how to build OKD CRC see the working group section of this site","title":"CRC"},{"location":"crc/#codeready-containers-for-okd","text":"CodeReady Containers brings a minimal, single node OKD 4 cluster to your local computer. This cluster provides a minimal environment for development and testing purposes. CodeReady Containers is mainly targeted at running on developers' laptops and desktops.","title":"CodeReady Containers for OkD"},{"location":"crc/#download-codeready-containers-for-okd","text":"Run a developer instance of OKD4 on your local workstation with CodeReady Containers built for OKD - >No Pull Secret Required! CodeReady Containers for OKD4 - Mac OS image CodeReady Containers for OKD4 - Linux image CodeReady Containers for OKD4 - Windows image The Getting Started Guide explains how to install and use CodeReady Containers. If you encounter any problems, please open a discussion item in the OKD GitHub Community !","title":"Download CodeReady Containers for OKD"},{"location":"crc/#crc-working-group","text":"There is a working group looking at automating the OKD CRC build process. If you want technical details on how to build OKD CRC see the working group section of this site","title":"CRC Working group"},{"location":"docs/","text":"Documentation \u00b6 There are 2 primary sources of information for OKD: community documentation - https://okd.io (this site) product documentation - https://docs.okd.io Updates and Issues \u00b6 If you encounter an issue with the documentation or have an idea to improve the content or add new content then please follow the directions below to learn how you can get changes made. The source for the documentation is managed in github. There are different processes for requesting changes in the community and product documentation: Community documentation \u00b6 The OKD Documentation subgroup is responsible for the community documentation. The process for making changes is set out in the working group section of the documentation Product documentation \u00b6 The OKD docs are built off the openshift/openshift-docs repo. If you notice any problems in the OKD docs that need to be addressed, you can either create a pull request with those changes against the openshift/openshift-docs repo or create an issue to suggest the changes. Among the changes you could suggest are: errors typos missing information incorrect product name (OpenShift Container Platform instead of OKD) Incorrect operating system (RHEL or RHCOS instead of FCOS) incorrect code examples If you create an issue, please do the following: Add [OKD] to the title of the issue. Provide as much information as possible, including the problem, the exact location in the file, the versions of OKD that the error affects (if known), and the correction you would like to see. A link to the file with the problem is extremely helpful. If you have the appropriate permissions, assign the issue to Michael Burke (mburke5678) and Diane Mueller (dmueller2001) so that the issue gets our direct attention. You can assign an issue by including the following in the issue description: /assign @mburke5678 /assign @dmueller2001 If not, you can @ mention mburke5678 in a comment. If you have the permissions, add a kind/documentation label.","title":"Documentation"},{"location":"docs/#documentation","text":"There are 2 primary sources of information for OKD: community documentation - https://okd.io (this site) product documentation - https://docs.okd.io","title":"Documentation"},{"location":"docs/#updates-and-issues","text":"If you encounter an issue with the documentation or have an idea to improve the content or add new content then please follow the directions below to learn how you can get changes made. The source for the documentation is managed in github. There are different processes for requesting changes in the community and product documentation:","title":"Updates and Issues"},{"location":"docs/#community-documentation","text":"The OKD Documentation subgroup is responsible for the community documentation. The process for making changes is set out in the working group section of the documentation","title":"Community documentation"},{"location":"docs/#product-documentation","text":"The OKD docs are built off the openshift/openshift-docs repo. If you notice any problems in the OKD docs that need to be addressed, you can either create a pull request with those changes against the openshift/openshift-docs repo or create an issue to suggest the changes. Among the changes you could suggest are: errors typos missing information incorrect product name (OpenShift Container Platform instead of OKD) Incorrect operating system (RHEL or RHCOS instead of FCOS) incorrect code examples If you create an issue, please do the following: Add [OKD] to the title of the issue. Provide as much information as possible, including the problem, the exact location in the file, the versions of OKD that the error affects (if known), and the correction you would like to see. A link to the file with the problem is extremely helpful. If you have the appropriate permissions, assign the issue to Michael Burke (mburke5678) and Diane Mueller (dmueller2001) so that the issue gets our direct attention. You can assign an issue by including the following in the issue description: /assign @mburke5678 /assign @dmueller2001 If not, you can @ mention mburke5678 in a comment. If you have the permissions, add a kind/documentation label.","title":"Product documentation"},{"location":"faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Below are answers to common questions regarding OKD installation and administration. If you have a suggested question or a suggested improvement to an answer, please feel free to reach out. What are the relations with OCP project? Is OKD4 an upstream of OCP? \u00b6 In 3.x release time OKD was used as an upstream project for Openshift Container Platform. OKD could be installed on Fedora/CentOS/RHEL and used CentOS based images to install the cluster. OCP, however, could be installed only on RHEL and its images were rebuilt to be RHEL-based. Universal Base Image project has enabled us to run RHEL-based images on any platform, so the full image rebuild is no longer necessary, allowing OKD4 project to reuse most images from OCP4. There is another critical part of OCP - Red Hat Enterprise Linux CoreOS. Although RHCOS is an open source project (much like RHEL8) it's not a community-driven project. As a result, OKD workgroup has made a decision to use Fedora CoreOS - open source and community-driven project - as a base for OKD4. This decision allows end-users to modify all parts of the cluster using prepared instructions. It should be noted that OKD4 is being automatically built from OCP4 ci stream , so most of the tests are happening in OCP CI and being mirrored to OKD. As a result, OKD4 CI doesn't have to run a lot of tests to ensure the release is valid. These relationships are more complex than \"upstream/downstream\", so we use \"sibling distributions\" to describe its state. How stable is OKD4? \u00b6 OKD4 builds are being automatically tested by release-controller . Release is rejected if either installation, upgrade from previous version or conformance test fails. Test results determine the upgrade graph, so for instance, if upgrade tests passed for beta5->rc edge, clusters on beta5 can be directly updated to rc release, bypassing beta6. The OKD stable version is released bi-weekly, following Fedora CoreOS schedule, client tools are uploaded to Github and images are mirrored to Quay. Can I run a single node cluster? \u00b6 Currently, single-node cluster installations cannot be deployed directly by the 4.7 installer. This is a known issue . Single-node cluster installations do work with the 4.8 nightly installer builds. As an alternative, if OKD version 4.7 is needed, you may have luck with Charro Gruver's OKD 4 Single Node Cluster instructions . You can also use Code Ready Containers (CRC) to run a single-node cluster on your desktop. What to do in case of errors? \u00b6 If you experience problems during installation you must collect the bootstrap log bundle, see instructions If you experience problems post installation, collect data of your cluster with: oc adm must-gather See documentation for more information. Upload it to a file hoster and send the link to the developers (Slack channel, ...) During installation the SSH key is required. It can be used to SSH onto the nodes later on - ssh core@<node ip> Where do I seek support? \u00b6 OKD is a community-supported distribution, Red Hat does not provide commercial support of OKD installations. Contact us on Slack: Workspace: Kubernetes, Channel: #openshift-dev (for developer communication) Workspace: Kubernetes, Channel: #openshift-users (for users ) See https://openshift.tips/ for useful Openshift tips Where can I find upgrades? \u00b6 https://amd64.origin.releases.ci.openshift.org/ Warning Nightly builds (from 4.x.0-0.okd ) are pruned every 72 hours. If your cluster uses these images, consider mirroring these files to a local registry. Builds from the stable-4 stream are not removed. How can I upgrade my cluster to a new version? \u00b6 Find a version where a tested upgrade path is available from your version for on https://amd64.origin.releases.ci.openshift.org/ Upgrade options: Preferred ways: Web Console: Home -> Overview -> Tab: Cluster, Card: Overview -> View settings -> Update Status Shell: Upgrades to latest available version oc adm upgrade Last resort : Upgrade to a certain version (will ignore the update graph!) oc adm upgrade --force --allow-explicit-upgrade = true --to-image = registry.ci.openshift.org/origin/release:4.4.0-0.okd-2020-03-16-105308 This will take a while; the upgrade may take several hours. Throughout the upgrade, kubernetes API would still be accessible and user workloads would be evicted and rescheduled as nodes are updated. Interesting commands while an upgrade runs \u00b6 Check overall upgrade status: oc get clusterversion Check the status of your cluster operators: oc get co Check the status of your nodes (cluster upgrades may include base OS updates): oc get nodes How can I find out what's inside of a (CI) release and which commit id each component has? \u00b6 This one is very helpful if you want to know if a certain commit has landed in your current version: oc adm release info registry.ci.openshift.org/origin/release:4.4 --commit-urls Name: 4.4.0-0.okd-2020-04-10-020541 Digest: sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Created: 2020-04-10T02:14:15Z OS/Arch: linux/amd64 Manifests: 413 Pull From: registry.ci.openshift.org/origin/release@sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Release Metadata: Version: 4.4.0-0.okd-2020-04-10-020541 Upgrades: <none> Component Versions: kubernetes 1.17.1 machine-os 31.20200407.20 Fedora CoreOS Images: NAME URL aws-machine-controllers https://github.com/openshift/cluster-api-provider-aws/commit/5fa82204468e71b44f65a5f24e2675dbfa0f5c29 azure-machine-controllers https://github.com/openshift/cluster-api-provider-azure/commit/832a43a30d7f00cd6774c1f5cd117aeebbe1b730 baremetal-installer https://github.com/openshift/installer/commit/a58f24b0df7e3699b39d4ae1d23c45672706934d baremetal-machine-controllers baremetal-operator baremetal-runtimecfg https://github.com/openshift/baremetal-runtimecfg/commit/09850a724d9290ffb05db3dd7f4f4c748b982759 branding https://github.com/openshift/origin-branding/commit/068fa1eac9f31ffe13089dd3de2ec49c153b2a14 cli https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cli-artifacts https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cloud-credential-operator https://github.com/openshift/cloud-credential-operator/commit/446680ed10ac938e11626409acb0c076edd3fd52 ... How to use the official installation container? \u00b6 The official installer container is part of every release. # Find out the installer image. oc adm release info quay.io/openshift/okd:4.7.0-0.okd-2021-04-24-103438 --image-for = installer # Example output # quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 # Run the container via Podman or Docker to perform tasks. e.g. create ignition configurations docker run -v $( pwd ) :/output -ti quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 create ignition-configs","title":"FAQ"},{"location":"faq/#frequently-asked-questions-faq","text":"Below are answers to common questions regarding OKD installation and administration. If you have a suggested question or a suggested improvement to an answer, please feel free to reach out.","title":"Frequently Asked Questions (FAQ)"},{"location":"faq/#what-are-the-relations-with-ocp-project-is-okd4-an-upstream-of-ocp","text":"In 3.x release time OKD was used as an upstream project for Openshift Container Platform. OKD could be installed on Fedora/CentOS/RHEL and used CentOS based images to install the cluster. OCP, however, could be installed only on RHEL and its images were rebuilt to be RHEL-based. Universal Base Image project has enabled us to run RHEL-based images on any platform, so the full image rebuild is no longer necessary, allowing OKD4 project to reuse most images from OCP4. There is another critical part of OCP - Red Hat Enterprise Linux CoreOS. Although RHCOS is an open source project (much like RHEL8) it's not a community-driven project. As a result, OKD workgroup has made a decision to use Fedora CoreOS - open source and community-driven project - as a base for OKD4. This decision allows end-users to modify all parts of the cluster using prepared instructions. It should be noted that OKD4 is being automatically built from OCP4 ci stream , so most of the tests are happening in OCP CI and being mirrored to OKD. As a result, OKD4 CI doesn't have to run a lot of tests to ensure the release is valid. These relationships are more complex than \"upstream/downstream\", so we use \"sibling distributions\" to describe its state.","title":"What are the relations with OCP project? Is OKD4 an upstream of OCP?"},{"location":"faq/#how-stable-is-okd4","text":"OKD4 builds are being automatically tested by release-controller . Release is rejected if either installation, upgrade from previous version or conformance test fails. Test results determine the upgrade graph, so for instance, if upgrade tests passed for beta5->rc edge, clusters on beta5 can be directly updated to rc release, bypassing beta6. The OKD stable version is released bi-weekly, following Fedora CoreOS schedule, client tools are uploaded to Github and images are mirrored to Quay.","title":"How stable is OKD4?"},{"location":"faq/#can-i-run-a-single-node-cluster","text":"Currently, single-node cluster installations cannot be deployed directly by the 4.7 installer. This is a known issue . Single-node cluster installations do work with the 4.8 nightly installer builds. As an alternative, if OKD version 4.7 is needed, you may have luck with Charro Gruver's OKD 4 Single Node Cluster instructions . You can also use Code Ready Containers (CRC) to run a single-node cluster on your desktop.","title":"Can I run a single node cluster?"},{"location":"faq/#what-to-do-in-case-of-errors","text":"If you experience problems during installation you must collect the bootstrap log bundle, see instructions If you experience problems post installation, collect data of your cluster with: oc adm must-gather See documentation for more information. Upload it to a file hoster and send the link to the developers (Slack channel, ...) During installation the SSH key is required. It can be used to SSH onto the nodes later on - ssh core@<node ip>","title":"What to do in case of errors?"},{"location":"faq/#where-do-i-seek-support","text":"OKD is a community-supported distribution, Red Hat does not provide commercial support of OKD installations. Contact us on Slack: Workspace: Kubernetes, Channel: #openshift-dev (for developer communication) Workspace: Kubernetes, Channel: #openshift-users (for users ) See https://openshift.tips/ for useful Openshift tips","title":"Where do I seek support?"},{"location":"faq/#where-can-i-find-upgrades","text":"https://amd64.origin.releases.ci.openshift.org/ Warning Nightly builds (from 4.x.0-0.okd ) are pruned every 72 hours. If your cluster uses these images, consider mirroring these files to a local registry. Builds from the stable-4 stream are not removed.","title":"Where can I find upgrades?"},{"location":"faq/#how-can-i-upgrade-my-cluster-to-a-new-version","text":"Find a version where a tested upgrade path is available from your version for on https://amd64.origin.releases.ci.openshift.org/ Upgrade options: Preferred ways: Web Console: Home -> Overview -> Tab: Cluster, Card: Overview -> View settings -> Update Status Shell: Upgrades to latest available version oc adm upgrade Last resort : Upgrade to a certain version (will ignore the update graph!) oc adm upgrade --force --allow-explicit-upgrade = true --to-image = registry.ci.openshift.org/origin/release:4.4.0-0.okd-2020-03-16-105308 This will take a while; the upgrade may take several hours. Throughout the upgrade, kubernetes API would still be accessible and user workloads would be evicted and rescheduled as nodes are updated.","title":"How can I upgrade my cluster to a new version?"},{"location":"faq/#interesting-commands-while-an-upgrade-runs","text":"Check overall upgrade status: oc get clusterversion Check the status of your cluster operators: oc get co Check the status of your nodes (cluster upgrades may include base OS updates): oc get nodes","title":"Interesting commands while an upgrade runs"},{"location":"faq/#how-can-i-find-out-whats-inside-of-a-ci-release-and-which-commit-id-each-component-has","text":"This one is very helpful if you want to know if a certain commit has landed in your current version: oc adm release info registry.ci.openshift.org/origin/release:4.4 --commit-urls Name: 4.4.0-0.okd-2020-04-10-020541 Digest: sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Created: 2020-04-10T02:14:15Z OS/Arch: linux/amd64 Manifests: 413 Pull From: registry.ci.openshift.org/origin/release@sha256:79b82f237aad0c38b5cdaf386ce893ff86060a476a39a067b5178bb6451e713c Release Metadata: Version: 4.4.0-0.okd-2020-04-10-020541 Upgrades: <none> Component Versions: kubernetes 1.17.1 machine-os 31.20200407.20 Fedora CoreOS Images: NAME URL aws-machine-controllers https://github.com/openshift/cluster-api-provider-aws/commit/5fa82204468e71b44f65a5f24e2675dbfa0f5c29 azure-machine-controllers https://github.com/openshift/cluster-api-provider-azure/commit/832a43a30d7f00cd6774c1f5cd117aeebbe1b730 baremetal-installer https://github.com/openshift/installer/commit/a58f24b0df7e3699b39d4ae1d23c45672706934d baremetal-machine-controllers baremetal-operator baremetal-runtimecfg https://github.com/openshift/baremetal-runtimecfg/commit/09850a724d9290ffb05db3dd7f4f4c748b982759 branding https://github.com/openshift/origin-branding/commit/068fa1eac9f31ffe13089dd3de2ec49c153b2a14 cli https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cli-artifacts https://github.com/openshift/oc/commit/2576e482bf003e34e67ba3d69edcf5d411cfd6f3 cloud-credential-operator https://github.com/openshift/cloud-credential-operator/commit/446680ed10ac938e11626409acb0c076edd3fd52 ...","title":"How can I find out what's inside of a (CI) release and which commit id each component has?"},{"location":"faq/#how-to-use-the-official-installation-container","text":"The official installer container is part of every release. # Find out the installer image. oc adm release info quay.io/openshift/okd:4.7.0-0.okd-2021-04-24-103438 --image-for = installer # Example output # quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 # Run the container via Podman or Docker to perform tasks. e.g. create ignition configurations docker run -v $( pwd ) :/output -ti quay.io/openshift/okd-content@sha256:521cd3ac7d826749a085418f753f1f909579e1aedfda704dca939c5ea7e5b105 create ignition-configs","title":"How to use the official installation container?"},{"location":"help/","text":"Help \u00b6 There is no official product support for OKD as it is a community project. All assistance is provided by volunteers from the user community. How to ask for help \u00b6 For questions or feedback, start a discussion on the discussion forum or reach us on Kubernetes Slack on #openshift-users Community Etiquette \u00b6 As all assistance is provided by the community, you are reminded of the code-of-conduct when asking a question or replying to a question. Before starting a new discussion topic, do a search on the discussion forum to see if anyone else has already raised the same issue - then contribute to the existing discussion topic rather than starting a new topic. When seeking help you should provide all the information a community volunteer may need to assist you. The easier it is for a volunteer to understand your issue, the more likely they are to provide assistance. This information should include: the version of OKD you are using the platform you are running on (and type of install - IPI / UPI) if you are following instructions and are having issues, a link to the instructions the must gather logs - these should be uploaded to a sharing site, such as Google Drive then a public link added to the discussion topic a description of the steps that can be used to recreate your issue, if applicable Please do not tag people you see answering other questions to try to get a faster answer as it is anti-social. We have an active community and it is up to individuals which questions they feel they want to respond to. Raising bugs \u00b6 We are trying to do all the diagnostic work in the discussion forum rather than using issues for the OKD project. If you are certain you have discovered a bug, then please raise an issue , but if you are not sure if you have found a bug then use the discussion forum to discuss it. If it turns out to be a bug, then the discussion topic can be converted to an issue.","title":"Getting Help"},{"location":"help/#help","text":"There is no official product support for OKD as it is a community project. All assistance is provided by volunteers from the user community.","title":"Help"},{"location":"help/#how-to-ask-for-help","text":"For questions or feedback, start a discussion on the discussion forum or reach us on Kubernetes Slack on #openshift-users","title":"How to ask for help"},{"location":"help/#community-etiquette","text":"As all assistance is provided by the community, you are reminded of the code-of-conduct when asking a question or replying to a question. Before starting a new discussion topic, do a search on the discussion forum to see if anyone else has already raised the same issue - then contribute to the existing discussion topic rather than starting a new topic. When seeking help you should provide all the information a community volunteer may need to assist you. The easier it is for a volunteer to understand your issue, the more likely they are to provide assistance. This information should include: the version of OKD you are using the platform you are running on (and type of install - IPI / UPI) if you are following instructions and are having issues, a link to the instructions the must gather logs - these should be uploaded to a sharing site, such as Google Drive then a public link added to the discussion topic a description of the steps that can be used to recreate your issue, if applicable Please do not tag people you see answering other questions to try to get a faster answer as it is anti-social. We have an active community and it is up to individuals which questions they feel they want to respond to.","title":"Community Etiquette"},{"location":"help/#raising-bugs","text":"We are trying to do all the diagnostic work in the discussion forum rather than using issues for the OKD project. If you are certain you have discovered a bug, then please raise an issue , but if you are not sure if you have found a bug then use the discussion forum to discuss it. If it turns out to be a bug, then the discussion topic can be converted to an issue.","title":"Raising bugs"},{"location":"installation/","text":"Install OKD \u00b6 Plan your installation \u00b6 OKD supports 2 types of cluster install options: Installer-provisioned infrastructure (IPI) User-provisioned infrastructure (UPI) IPI is a largely automated install process, where the installer is responsible for setting up the infrastructure, where UPI requires you to set up the base infrastructure. You can find further details in the documentation OKD support installation on bare metal hardware, a number of virtualization platforms and a number of cloud platforms, so you need to decide where you want to install OKD and that your environment has sufficient resources for the cluster to operate. The documentation has more information to help you plan your installation. If you want to install on a typical developer workstation, then Code-Ready Containers may be a better options, as that is a cut-down installation designed to run on limited compute and memory resources. You can find examples of OKD installations, setup by OKD community members in the guides section. Getting Started \u00b6 To obtain the openshift installer and client, visit releases for stable versions or https://amd64.origin.releases.ci.openshift.org/ for nightlies. You can verify the downloads using: curl https://www.okd.io/vrutkovs.pub | gpg --import Output gpg: key 3D54B6723B20C69F: public key \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" imported gpg: Total number processed: 1 gpg: imported: 1 gpg --verify sha256sum.txt.asc sha256sum.txt Output gpg: Signature made Mon May 25 18:48:22 2020 CEST gpg: using RSA key DB861D01D4D1138A993ADC1A3D54B6723B20C69F gpg: Good signature from \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" [ultimate] gpg: aka \"Vadim Rutkovsky <vrutkovs@redhat.com>\" [ultimate] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: DB86 1D01 D4D1 138A 993A DC1A 3D54 B672 3B20 C69F sha256sum -c sha256sum.txt Output release.txt: OK openshift-client-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-windows-4.4.0-0.okd-2020-05-23-055148-beta5.zip: OK openshift-install-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-install-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK Please note that each nightly release is pruned after 72 hours. If the nightly that you installed was pruned, the cluster may be unable to pull necessary images and may show errors for various functionality (including updates). Alternatively, if you have the openshift client oc already installed, you can use it to download and extract the openshift installer and client from our container image: oc adm release extract --tools quay.io/openshift/okd:4.5.0-0.okd-2020-07-14-153706-ga Note You need a 4.x version of oc to extract the installer and the latest client. You can initially use the official Openshift client (mirror) There are full instructions in the OKD documentation for each supported platform, but the main steps for an IPU install are: extract the downloaded tarballs and copy the binaries into your PATH. run the following from an empty directory: openshift-install create cluster follow the prompts to create the install config you will need to have cloud credentials set in your shell properly before installation. you must have permission to configure the appropriate cloud resources from that account (such as VPCs, instances, and DNS records). you must have already configured a public DNS zone on your chosen cloud before the install starts. you will also be prompted for a pull-secret that will be made available to all of of your machines - for OKD4 you should either paste the pull-secret you use for your registry, or paste {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} to bypass the required value check (see bug #182 ). Once the install completes successfully the console URL and an admin username and password will be printed. If your DNS records were correct, you should be able to log in to your new OKD4 cluster! To undo the installation and delete any cloud resources created by the installer, run openshift-install destroy cluster Note The OpenShift client tools for your cluster can be downloaded from the help drop down menu at the top of the web console.","title":"Installation"},{"location":"installation/#install-okd","text":"","title":"Install OKD"},{"location":"installation/#plan-your-installation","text":"OKD supports 2 types of cluster install options: Installer-provisioned infrastructure (IPI) User-provisioned infrastructure (UPI) IPI is a largely automated install process, where the installer is responsible for setting up the infrastructure, where UPI requires you to set up the base infrastructure. You can find further details in the documentation OKD support installation on bare metal hardware, a number of virtualization platforms and a number of cloud platforms, so you need to decide where you want to install OKD and that your environment has sufficient resources for the cluster to operate. The documentation has more information to help you plan your installation. If you want to install on a typical developer workstation, then Code-Ready Containers may be a better options, as that is a cut-down installation designed to run on limited compute and memory resources. You can find examples of OKD installations, setup by OKD community members in the guides section.","title":"Plan your installation"},{"location":"installation/#getting-started","text":"To obtain the openshift installer and client, visit releases for stable versions or https://amd64.origin.releases.ci.openshift.org/ for nightlies. You can verify the downloads using: curl https://www.okd.io/vrutkovs.pub | gpg --import Output gpg: key 3D54B6723B20C69F: public key \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" imported gpg: Total number processed: 1 gpg: imported: 1 gpg --verify sha256sum.txt.asc sha256sum.txt Output gpg: Signature made Mon May 25 18:48:22 2020 CEST gpg: using RSA key DB861D01D4D1138A993ADC1A3D54B6723B20C69F gpg: Good signature from \"Vadim Rutkovsky <vadim@vrutkovs.eu>\" [ultimate] gpg: aka \"Vadim Rutkovsky <vrutkovs@redhat.com>\" [ultimate] gpg: WARNING: This key is not certified with a trusted signature! gpg: There is no indication that the signature belongs to the owner. Primary key fingerprint: DB86 1D01 D4D1 138A 993A DC1A 3D54 B672 3B20 C69F sha256sum -c sha256sum.txt Output release.txt: OK openshift-client-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-client-windows-4.4.0-0.okd-2020-05-23-055148-beta5.zip: OK openshift-install-linux-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK openshift-install-mac-4.4.0-0.okd-2020-05-23-055148-beta5.tar.gz: OK Please note that each nightly release is pruned after 72 hours. If the nightly that you installed was pruned, the cluster may be unable to pull necessary images and may show errors for various functionality (including updates). Alternatively, if you have the openshift client oc already installed, you can use it to download and extract the openshift installer and client from our container image: oc adm release extract --tools quay.io/openshift/okd:4.5.0-0.okd-2020-07-14-153706-ga Note You need a 4.x version of oc to extract the installer and the latest client. You can initially use the official Openshift client (mirror) There are full instructions in the OKD documentation for each supported platform, but the main steps for an IPU install are: extract the downloaded tarballs and copy the binaries into your PATH. run the following from an empty directory: openshift-install create cluster follow the prompts to create the install config you will need to have cloud credentials set in your shell properly before installation. you must have permission to configure the appropriate cloud resources from that account (such as VPCs, instances, and DNS records). you must have already configured a public DNS zone on your chosen cloud before the install starts. you will also be prompted for a pull-secret that will be made available to all of of your machines - for OKD4 you should either paste the pull-secret you use for your registry, or paste {\"auths\":{\"fake\":{\"auth\":\"aWQ6cGFzcwo=\"}}} to bypass the required value check (see bug #182 ). Once the install completes successfully the console URL and an admin username and password will be printed. If your DNS records were correct, you should be able to log in to your new OKD4 cluster! To undo the installation and delete any cloud resources created by the installer, run openshift-install destroy cluster Note The OpenShift client tools for your cluster can be downloaded from the help drop down menu at the top of the web console.","title":"Getting Started"},{"location":"blog/2021-02-18-demo-article-for-authors.html/","text":"Demo blog article for authors \u00b6 This is a demo article \u00b6 See this file in the okd.io Github repository if you want to learn how this article was written with Github markdown syntax. Here's a table \u00b6 OKD Version Initial FCOS Version Mirror Fix needed Repo Disable Fix needed 4.5.0-0.okd-2020-10-15-235428 32.20200629.3 4.6.0-0.okd-2020-11-27-200126 32.20200629.3 x 4.6.0-0.okd-2020-12-12-135354 33.20201124.10 -> 33.20201209.10 x Links \u00b6 Use the scripts from this repository to fix the broken images from quay.io/openshift/okd-content. Code snippets \u00b6 Disable all repositories on each OKD node: sudo grep enabled = 1 /etc/yum.repos.d/* sudo find /etc/yum.repos.d/ -type f -exec sudo sed -i 's/enabled=1/enabled=0/g' {} + sudo grep enabled = 1 /etc/yum.repos.d/* Images \u00b6","title":"Demo blog article for authors"},{"location":"blog/2021-02-18-demo-article-for-authors.html/#demo-blog-article-for-authors","text":"","title":"Demo blog article for authors"},{"location":"blog/2021-02-18-demo-article-for-authors.html/#this-is-a-demo-article","text":"See this file in the okd.io Github repository if you want to learn how this article was written with Github markdown syntax.","title":"This is a demo article"},{"location":"blog/2021-02-18-demo-article-for-authors.html/#heres-a-table","text":"OKD Version Initial FCOS Version Mirror Fix needed Repo Disable Fix needed 4.5.0-0.okd-2020-10-15-235428 32.20200629.3 4.6.0-0.okd-2020-11-27-200126 32.20200629.3 x 4.6.0-0.okd-2020-12-12-135354 33.20201124.10 -> 33.20201209.10 x","title":"Here's a table"},{"location":"blog/2021-02-18-demo-article-for-authors.html/#links","text":"Use the scripts from this repository to fix the broken images from quay.io/openshift/okd-content.","title":"Links"},{"location":"blog/2021-02-18-demo-article-for-authors.html/#code-snippets","text":"Disable all repositories on each OKD node: sudo grep enabled = 1 /etc/yum.repos.d/* sudo find /etc/yum.repos.d/ -type f -exec sudo sed -i 's/enabled=1/enabled=0/g' {} + sudo grep enabled = 1 /etc/yum.repos.d/*","title":"Code snippets"},{"location":"blog/2021-02-18-demo-article-for-authors.html/#images","text":"","title":"Images"},{"location":"blog/2021-03-07-new-blog.html/","text":"okd.io now has a blog \u00b6 Todo This content is for the current Middleman based OKD.io site Let's share news and useful information with each other \u00b6 We look forward to sharing news and useful information about OKD in this blog in the future. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io. Here's how to do it: Fork the repository https://github.com/openshift-cs/okd.io Create a new file in the source/blog directory The filename must have this format: yyyy-mm-dd-<title of your blog article (lowercase)>.html.markdown yyyy = Year (four digits) mm = Month (two digits) dd = Day (two digits) e.g.: source/blog/2021-03-07-my-first-article.html.markdown The date must not be in the future ! Each article must contain a header section like this one: --- title : <THIS TITLE WILL BE DISPLAYED IN THE BLOG> date : 2021-03-07 <- THIS DATE MUST EXACTLY MATCH THE DATE IN THE FILENAME tags : blog <- CHOOSE ONE OR MORE TAGS (COMMA SEPARATED) --- ... GITHUB MARKDOWN SYNTAX ... Test your changes locally. This README.md file will tell you how to do that. Create a pull request, if your article is ready to be published.","title":"okd.io now has a blog"},{"location":"blog/2021-03-07-new-blog.html/#okdio-now-has-a-blog","text":"Todo This content is for the current Middleman based OKD.io site","title":"okd.io now has a blog"},{"location":"blog/2021-03-07-new-blog.html/#lets-share-news-and-useful-information-with-each-other","text":"We look forward to sharing news and useful information about OKD in this blog in the future. You are also invited to participate: share your experiences and tips with the community by creating your own blog articles for okd.io. Here's how to do it: Fork the repository https://github.com/openshift-cs/okd.io Create a new file in the source/blog directory The filename must have this format: yyyy-mm-dd-<title of your blog article (lowercase)>.html.markdown yyyy = Year (four digits) mm = Month (two digits) dd = Day (two digits) e.g.: source/blog/2021-03-07-my-first-article.html.markdown The date must not be in the future ! Each article must contain a header section like this one: --- title : <THIS TITLE WILL BE DISPLAYED IN THE BLOG> date : 2021-03-07 <- THIS DATE MUST EXACTLY MATCH THE DATE IN THE FILENAME tags : blog <- CHOOSE ONE OR MORE TAGS (COMMA SEPARATED) --- ... GITHUB MARKDOWN SYNTAX ... Test your changes locally. This README.md file will tell you how to do that. Create a pull request, if your article is ready to be published.","title":"Let's share news and useful information with each other"},{"location":"blog/2021-03-16-save-the-date-okd-testing-deployment-workshop.html/","text":"Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now! \u00b6 The OKD Working Group is hosting a virtual workshop on testing and deploying OKD4 \u00b6 On March 20th, OKD-Working Group is hosting a one day event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day will start with all attendees together in the \u2018main stage\u2019 area for 2 hours where we will give an short welcome and describe the logistics for the day, give a brief introduction to OKD4 itself then walk thru a install deployment to vSphere using UPI approach along with a few other more universal best practices such as DNS/DHCP server configuration) that apply to all deployment targets. Then we will break into tracks specific to the deployment target platforms for deep dive demos with Q/A, try and answer any questions you have about your specific deployment target's configurations, identify any missing pieces in the documentation and triage the documentation as we go. There will be 4 track break-out rooms set-up for 3 hours of deployment walk throughs and Q/A with session leads: vSphere/UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat) and Sri Ramanujam (Datto) Our goal is to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release. This is community event NOT meant as a substitute for Red Hat technical support. There is no admission or ticket charge for OKD-Working Group events. However, you are required to complete a free hopin.to platform registration and watch the hopin site for updates about registration and schedule updates. We are committed to fostering an open and welcoming environment at our working group meetings and events. We set expectations for inclusive behavior through our code of conduct and media policies, and are prepared to enforce these. You can Register for the workshop here : https://hopin.com/events/okd-testing-and-deployment-workshop","title":"Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now!"},{"location":"blog/2021-03-16-save-the-date-okd-testing-deployment-workshop.html/#save-the-date-okd-testing-and-deployment-workshop-march-20-register-now","text":"","title":"Save The Date! OKD Testing and Deployment Workshop (March 20) Register Now!"},{"location":"blog/2021-03-16-save-the-date-okd-testing-deployment-workshop.html/#the-okd-working-group-is-hosting-a-virtual-workshop-on-testing-and-deploying-okd4","text":"On March 20th, OKD-Working Group is hosting a one day event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day will start with all attendees together in the \u2018main stage\u2019 area for 2 hours where we will give an short welcome and describe the logistics for the day, give a brief introduction to OKD4 itself then walk thru a install deployment to vSphere using UPI approach along with a few other more universal best practices such as DNS/DHCP server configuration) that apply to all deployment targets. Then we will break into tracks specific to the deployment target platforms for deep dive demos with Q/A, try and answer any questions you have about your specific deployment target's configurations, identify any missing pieces in the documentation and triage the documentation as we go. There will be 4 track break-out rooms set-up for 3 hours of deployment walk throughs and Q/A with session leads: vSphere/UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat) and Sri Ramanujam (Datto) Our goal is to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release. This is community event NOT meant as a substitute for Red Hat technical support. There is no admission or ticket charge for OKD-Working Group events. However, you are required to complete a free hopin.to platform registration and watch the hopin site for updates about registration and schedule updates. We are committed to fostering an open and welcoming environment at our working group meetings and events. We set expectations for inclusive behavior through our code of conduct and media policies, and are prepared to enforce these. You can Register for the workshop here : https://hopin.com/events/okd-testing-and-deployment-workshop","title":"The OKD Working Group is hosting a virtual workshop on testing and deploying OKD4"},{"location":"blog/2021-03-19-please-avoid-using-fcos-33.20210301.3.1.html/","text":"Please avoid using FCOS 33.20210301.3.1 for new OKD installs \u00b6 Hi, Due to several issues ([1] and [2]) fresh installations using FCOS 33.20210301.3.1 would fail. The fix is coming in Podman 3.1.0. Please use an older stable release - 33.20210217.3.0 - as a starting point instead. See download links at https://builds.coreos.fedoraproject.org/browser?stream=stable (might need some scrolling), Note, that only fresh installs are affected. Also, you won't be left with outdated packages, as OKD does update themselves to latest stable FCOS content during installation/update. https://bugzilla.redhat.com/show_bug.cgi?id=1936927 https://github.com/openshift/okd/issues/566 -- Cheers, Vadim","title":"Please avoid using FCOS 33.20210301.3.1 for new OKD installs"},{"location":"blog/2021-03-19-please-avoid-using-fcos-33.20210301.3.1.html/#please-avoid-using-fcos-332021030131-for-new-okd-installs","text":"Hi, Due to several issues ([1] and [2]) fresh installations using FCOS 33.20210301.3.1 would fail. The fix is coming in Podman 3.1.0. Please use an older stable release - 33.20210217.3.0 - as a starting point instead. See download links at https://builds.coreos.fedoraproject.org/browser?stream=stable (might need some scrolling), Note, that only fresh installs are affected. Also, you won't be left with outdated packages, as OKD does update themselves to latest stable FCOS content during installation/update. https://bugzilla.redhat.com/show_bug.cgi?id=1936927 https://github.com/openshift/okd/issues/566 -- Cheers, Vadim","title":"Please avoid using FCOS 33.20210301.3.1 for new OKD installs"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/","text":"Recap OKD Testing and Deployment Workshop - Videos and Additional Resources \u00b6 The OKD Working Group held a virtual community-hosted workshop on testing and deploying OKD4 on March 20th \u00b6 On March 20th, OKD-Working Group hosted a day-long event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day started with all attendees together in the \u2018main stage\u2019 area for 2 hours where community members gave an short welcome along with the following four presentations: What is OKD4 (with a Release Update) - by Charro Gruver (Red Hat) Walk Thru of the OKD Release and Build Processes - Vadim Rutkovsky (Red Hat) Walk Thru of the OKD Deployment and Configuration Guides - Jamie Magiera (UMich) Best Practices such as DNS/DHCP server and Load Balancer Configuration) - Josef Meier (Rohde and Schwarz) Then attendees then broke into track sessions specific to the deployment target platforms for deep dive demos with live Q/A, answered as many questions as possible about that specific deployment target's configurations, attempted to identify any missing pieces in the documentation and triage the documentation as we went along. The 4 track break-out rooms set-up for 2.5 hours of deployment walk throughs and Q/A with session leads: Automated Installation on vSphere UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat), Sri Ramanujam (Datto) and Vadim Rutkovsky(Red Hat) Our goal was to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release. Resources: \u00b6 Link to Playlist OKD Workshop Slides - Charro Gruver DNS DHCP Load Balancer Diagram - Josef Meier","title":"Recap OKD Testing and Deployment Workshop - Videos and Additional Resources"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/#recap-okd-testing-and-deployment-workshop-videos-and-additional-resources","text":"","title":"Recap OKD Testing and Deployment Workshop - Videos and Additional Resources"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/#the-okd-working-group-held-a-virtual-community-hosted-workshop-on-testing-and-deploying-okd4-on-march-20th","text":"On March 20th, OKD-Working Group hosted a day-long event to bring together people from the OKD and related Open Source project communities to collaborate on testing and documentation of the OKD 4 install and upgrade processes for the various platforms that people are deploying OKD 4 on as well to identify any issues with the current documentation for these processes and triage them together. The day started with all attendees together in the \u2018main stage\u2019 area for 2 hours where community members gave an short welcome along with the following four presentations: What is OKD4 (with a Release Update) - by Charro Gruver (Red Hat) Walk Thru of the OKD Release and Build Processes - Vadim Rutkovsky (Red Hat) Walk Thru of the OKD Deployment and Configuration Guides - Jamie Magiera (UMich) Best Practices such as DNS/DHCP server and Load Balancer Configuration) - Josef Meier (Rohde and Schwarz) Then attendees then broke into track sessions specific to the deployment target platforms for deep dive demos with live Q/A, answered as many questions as possible about that specific deployment target's configurations, attempted to identify any missing pieces in the documentation and triage the documentation as we went along. The 4 track break-out rooms set-up for 2.5 hours of deployment walk throughs and Q/A with session leads: Automated Installation on vSphere UPI - lead by Jaime Magiera (UMich) and Josef Meier (Rohde & Schwarz) Bare Metal/UPI - lead by Andrew Sullivan (Red Hat) and Jason Pittman (Red Hat) Single Node Cluster - lead by Charro Gruver (Red Hat) and Bruce Link (BCIT) Home Lab Setup - lead by Craig Robinson (Red Hat), Sri Ramanujam (Datto) and Vadim Rutkovsky(Red Hat) Our goal was to triage our existing community documentation, identify any short comings and encourage your participation in the OKD-Working Group 's testing of the installation and upgrade processes for each OKD release.","title":"The OKD Working Group held a virtual community-hosted workshop on testing and deploying OKD4 on March 20th"},{"location":"blog/2021-03-22-recap-okd-testing-deployment-workshop.html/#resources","text":"Link to Playlist OKD Workshop Slides - Charro Gruver DNS DHCP Load Balancer Diagram - Josef Meier","title":"Resources:"},{"location":"blog/2021-05-04-From-OKD-to-OpenShift-in-3-Years.html/","text":"Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift \u00b6 From OKD to OpenShift in 3 Years - talk by Josef Meier (Rohde & Schwarz) from OpenShift Commons Gathering at Kubecon \u00b6 On May 4th 2020, OKD-Working Group member Josef Meier gave a wonderful talk about Rohde & Schwarz's Journey to OpenShift 4 from OKD to ARO (Azure Red Hat OpenShift) and discussed benefits of participating in the OKD Working Group! Join the OKD-Working Group and add your voice to the conversation!","title":"Rohde & Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift"},{"location":"blog/2021-05-04-From-OKD-to-OpenShift-in-3-Years.html/#rohde-schwarzs-journey-to-openshift-4-from-okd-to-azure-red-hat-openshift","text":"","title":"Rohde &amp; Schwarz's Journey to OpenShift 4 From OKD to Azure Red Hat OpenShift"},{"location":"blog/2021-05-04-From-OKD-to-OpenShift-in-3-Years.html/#from-okd-to-openshift-in-3-years-talk-by-josef-meier-rohde-schwarz-from-openshift-commons-gathering-at-kubecon","text":"On May 4th 2020, OKD-Working Group member Josef Meier gave a wonderful talk about Rohde & Schwarz's Journey to OpenShift 4 from OKD to ARO (Azure Red Hat OpenShift) and discussed benefits of participating in the OKD Working Group! Join the OKD-Working Group and add your voice to the conversation!","title":"From OKD to OpenShift in 3 Years - talk by Josef Meier (Rohde &amp; Schwarz) from OpenShift Commons Gathering at Kubecon"},{"location":"blog/2021-05-06-OKD-Office-Hours-at-KubeconEU-on-OpenShiftTV.html/","text":"OKD Working Group Office Hours at KubeconEU on OpenShift.tv \u00b6 Video from OKD Working Group Office Hours at KubeconEU on OpenShift.tv \u00b6 On May 6th 2020, OKD-Working Group members hosted an hour long community led Office Hour with a brief introduction to the latest release by Red Hat's Charro Gruver then live Q/A! Join the OKD-Working Group and add your voice to the conversation!","title":"OKD Working Group Office Hours at KubeconEU on OpenShift.tv"},{"location":"blog/2021-05-06-OKD-Office-Hours-at-KubeconEU-on-OpenShiftTV.html/#okd-working-group-office-hours-at-kubeconeu-on-openshifttv","text":"","title":"OKD Working Group Office Hours at KubeconEU on OpenShift.tv"},{"location":"blog/2021-05-06-OKD-Office-Hours-at-KubeconEU-on-OpenShiftTV.html/#video-from-okd-working-group-office-hours-at-kubeconeu-on-openshifttv","text":"On May 6th 2020, OKD-Working Group members hosted an hour long community led Office Hour with a brief introduction to the latest release by Red Hat's Charro Gruver then live Q/A! Join the OKD-Working Group and add your voice to the conversation!","title":"Video from OKD Working Group Office Hours at KubeconEU on OpenShift.tv"},{"location":"guides/automated-vsphere-upi/","text":"Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI) \u00b6 Introduction \u00b6 It\u2019s possible to completely automate the process of installing OpenShift/OKD on vSphere with User Provisioned Infrastructure by chaining together the various functions of OCT via a wrapper script. Steps \u00b6 Deploy the DNS, DHCP, and load balancer infrastructure outlined in the Prerequisites section. Create an install-config.yaml.template file based on the format outlined in the section Sample install-config.yaml file for VMware vSphere of the OKD docs. Do not add a pull secret. The script will query you for one or it will insert a default one if you use the \u2013auto-secret flag. Create a wrapper script that: Installs the desired FCOS image Downloads the oc and openshift-installer binaries for your desired release version Generates and modifies the ignition files appropriately Builds the cluster nodes Triggers the installation process. Prerequisites \u00b6 DNS \u00b6 1 entry for the bootstrap node of the format bootstrap.[cluster].domain.tld 3 entries for the master nodes of the form master-[n].[cluster].domain.tld An entry for each of the desired worker nodes in the form worker-[n].[cluster].domain.tld 1 entry for the API endpoint in the form api.[cluster].domain.tld 1 entry for the API internal endpoint in the form api-int.[cluster].domain.tld 1 wildcard entry for the Ingress endpoint in the form *.apps.[cluster].domain.tld DHCP \u00b6 Load Balancer \u00b6 vSphere UPI requires the use of a load balancer. There needs to be two pools. API: This pool should contain your master nodes. Ingress: This pool should contain your worker nodes. Proxy (Optional) \u00b6 If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid Wrapper Script \u00b6 #!/bin/bash masters_count = 3 workers_count = 2 template_url = \"https://builds.coreos.fedoraproject.org/prod/streams/testing/builds/33.20210314.2.0/x86_64/fedora-coreos-33.20210314.2.0-vmware.x86_64.ova\" template_name = \"fedora-coreos-33.20210201.2.1-vmware.x86_64\" library = \"Linux ISOs\" cluster_name = \"mycluster\" cluster_folder = \"/MyVSPHERE/vm/Linux/OKD/mycluster\" network_name = \"VM Network\" install_folder = ` pwd ` # Import the template ./oct.sh --import-template --library \" ${ library } \" --template-url \" ${ template_url } \" # Install the desired OKD tools oct.sh --install-tools --release 4 .6 # Launch the prerun to generate and modify the ignition files oct.sh --prerun --auto-secret # Deploy the nodes for the cluster with the appropriate ignition data oct.sh --build --template-name \" ${ template_name } \" --library \" ${ library } \" --cluster-name \" ${ cluster_name } \" --cluster-folder \" ${ cluster_folder } \" --network-name \" ${ network_name } \" --installation-folder \" ${ install_folder } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Turn on the cluster nodes oct.sh --cluster-power on --cluster-name \" ${ cluster_name } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Run the OpenShift installer bin/openshift-install --dir = $( pwd ) wait-for bootstrap-complete --log-level = info Future Updates \u00b6 Generating the install-config template Pull directly from FCOS release feed","title":"Automated vSphere install"},{"location":"guides/automated-vsphere-upi/#implementing-an-automated-installation-solution-for-okd-on-vsphere-with-user-provisioned-infrastructure-upi","text":"","title":"Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI)"},{"location":"guides/automated-vsphere-upi/#introduction","text":"It\u2019s possible to completely automate the process of installing OpenShift/OKD on vSphere with User Provisioned Infrastructure by chaining together the various functions of OCT via a wrapper script.","title":"Introduction"},{"location":"guides/automated-vsphere-upi/#steps","text":"Deploy the DNS, DHCP, and load balancer infrastructure outlined in the Prerequisites section. Create an install-config.yaml.template file based on the format outlined in the section Sample install-config.yaml file for VMware vSphere of the OKD docs. Do not add a pull secret. The script will query you for one or it will insert a default one if you use the \u2013auto-secret flag. Create a wrapper script that: Installs the desired FCOS image Downloads the oc and openshift-installer binaries for your desired release version Generates and modifies the ignition files appropriately Builds the cluster nodes Triggers the installation process.","title":"Steps"},{"location":"guides/automated-vsphere-upi/#prerequisites","text":"","title":"Prerequisites"},{"location":"guides/automated-vsphere-upi/#dns","text":"1 entry for the bootstrap node of the format bootstrap.[cluster].domain.tld 3 entries for the master nodes of the form master-[n].[cluster].domain.tld An entry for each of the desired worker nodes in the form worker-[n].[cluster].domain.tld 1 entry for the API endpoint in the form api.[cluster].domain.tld 1 entry for the API internal endpoint in the form api-int.[cluster].domain.tld 1 wildcard entry for the Ingress endpoint in the form *.apps.[cluster].domain.tld","title":"DNS"},{"location":"guides/automated-vsphere-upi/#dhcp","text":"","title":"DHCP"},{"location":"guides/automated-vsphere-upi/#load-balancer","text":"vSphere UPI requires the use of a load balancer. There needs to be two pools. API: This pool should contain your master nodes. Ingress: This pool should contain your worker nodes.","title":"Load Balancer"},{"location":"guides/automated-vsphere-upi/#proxy-optional","text":"If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid","title":"Proxy (Optional)"},{"location":"guides/automated-vsphere-upi/#wrapper-script","text":"#!/bin/bash masters_count = 3 workers_count = 2 template_url = \"https://builds.coreos.fedoraproject.org/prod/streams/testing/builds/33.20210314.2.0/x86_64/fedora-coreos-33.20210314.2.0-vmware.x86_64.ova\" template_name = \"fedora-coreos-33.20210201.2.1-vmware.x86_64\" library = \"Linux ISOs\" cluster_name = \"mycluster\" cluster_folder = \"/MyVSPHERE/vm/Linux/OKD/mycluster\" network_name = \"VM Network\" install_folder = ` pwd ` # Import the template ./oct.sh --import-template --library \" ${ library } \" --template-url \" ${ template_url } \" # Install the desired OKD tools oct.sh --install-tools --release 4 .6 # Launch the prerun to generate and modify the ignition files oct.sh --prerun --auto-secret # Deploy the nodes for the cluster with the appropriate ignition data oct.sh --build --template-name \" ${ template_name } \" --library \" ${ library } \" --cluster-name \" ${ cluster_name } \" --cluster-folder \" ${ cluster_folder } \" --network-name \" ${ network_name } \" --installation-folder \" ${ install_folder } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Turn on the cluster nodes oct.sh --cluster-power on --cluster-name \" ${ cluster_name } \" --master-node-count ${ masters_count } --worker-node-count ${ workers_count } # Run the OpenShift installer bin/openshift-install --dir = $( pwd ) wait-for bootstrap-complete --log-level = info","title":"Wrapper Script"},{"location":"guides/automated-vsphere-upi/#future-updates","text":"Generating the install-config template Pull directly from FCOS release feed","title":"Future Updates"},{"location":"guides/aws-ipi/","text":"AWS IPI Default Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the default options for the installer. Infrastructure \u00b6 Compute \u00b6 3 control plane nodes instance type m4.xlarge , or m5.xlarge if previous not available in the region 3 compute nodes instance type m4.large , or m5.large if previous not available in the region Networking \u00b6 1 virtual private cloud 1 public subnet per availability zone in the region 1 private subnet per availability zone in the region 1 NAT gateway per availability zone 1 elastic IP address per NAT gateway 3 elastic load balancers 1 external network load balancer for the master API server 1 internal network load balancer for the master API server 1 classic load balancer for the router 21 elastic network interfaces, plus 1 interface per availability zone 1 virtual private cloud gateway 10 distinct security groups Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"AWS IPI Default Deployment"},{"location":"guides/aws-ipi/#aws-ipi-default-deployment","text":"This describes the resources used by OpenShift after performing an installation using the default options for the installer.","title":"AWS IPI Default Deployment"},{"location":"guides/aws-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/aws-ipi/#compute","text":"3 control plane nodes instance type m4.xlarge , or m5.xlarge if previous not available in the region 3 compute nodes instance type m4.large , or m5.large if previous not available in the region","title":"Compute"},{"location":"guides/aws-ipi/#networking","text":"1 virtual private cloud 1 public subnet per availability zone in the region 1 private subnet per availability zone in the region 1 NAT gateway per availability zone 1 elastic IP address per NAT gateway 3 elastic load balancers 1 external network load balancer for the master API server 1 internal network load balancer for the master API server 1 classic load balancer for the router 21 elastic network interfaces, plus 1 interface per availability zone 1 virtual private cloud gateway 10 distinct security groups","title":"Networking"},{"location":"guides/aws-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/azure-ipi/","text":"Azure IPI Default Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the default options for the installer. Infrastructure \u00b6 Compute \u00b6 3 control plane nodes instance type Standard_D8s_v3 3 compute nodes instance type Standard_D4s_v3 Networking \u00b6 1 virtual network (VNet) containing 2 subnets 6 network interfaces -3 network load balancers 1 public for compute node access 1 private for control plane access 1 public for control plane access 2 public IP addresses 1 for the public compute load balancer 1 for the public control plane load balancer 7 private IP addresses 1 per control plane node 1 per compute node 1 for the private control plane load balancer 2 network security groups 1 for control plane allowing traffic on port 6443 from anywhere 1 for compute allowing traffic on ports 80 and 443 from the internet Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"Azure IPI Default Deployment"},{"location":"guides/azure-ipi/#azure-ipi-default-deployment","text":"This describes the resources used by OpenShift after performing an installation using the default options for the installer.","title":"Azure IPI Default Deployment"},{"location":"guides/azure-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/azure-ipi/#compute","text":"3 control plane nodes instance type Standard_D8s_v3 3 compute nodes instance type Standard_D4s_v3","title":"Compute"},{"location":"guides/azure-ipi/#networking","text":"1 virtual network (VNet) containing 2 subnets 6 network interfaces -3 network load balancers 1 public for compute node access 1 private for control plane access 1 public for control plane access 2 public IP addresses 1 for the public compute load balancer 1 for the public control plane load balancer 7 private IP addresses 1 per control plane node 1 per compute node 1 for the private control plane load balancer 2 network security groups 1 for control plane allowing traffic on port 6443 from anywhere 1 for compute allowing traffic on ports 80 and 443 from the internet","title":"Networking"},{"location":"guides/azure-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/gcp-ipi/","text":"GCP IPI Default Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the default options for the installer. Infrastructure \u00b6 Compute \u00b6 3 control plane nodes instance type n1-standard-4 3 compute nodes instance type n1-standard-2 1 image Networking \u00b6 2 networks 2 subnetworks 3 static IP addresses 1 router 2 routes 3 target pools 10 firewall rules 2 forwarding rules 3 in-use global IP addresses 3 health checks Platform \u00b6 5 IAM service accounts Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"GCP IPI Default Deployment"},{"location":"guides/gcp-ipi/#gcp-ipi-default-deployment","text":"This describes the resources used by OpenShift after performing an installation using the default options for the installer.","title":"GCP IPI Default Deployment"},{"location":"guides/gcp-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/gcp-ipi/#compute","text":"3 control plane nodes instance type n1-standard-4 3 compute nodes instance type n1-standard-2 1 image","title":"Compute"},{"location":"guides/gcp-ipi/#networking","text":"2 networks 2 subnetworks 3 static IP addresses 1 router 2 routes 3 target pools 10 firewall rules 2 forwarding rules 3 in-use global IP addresses 3 health checks","title":"Networking"},{"location":"guides/gcp-ipi/#platform","text":"5 IAM service accounts","title":"Platform"},{"location":"guides/gcp-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/overview/","text":"Deployment Guides \u00b6 The guides linked below provide some examples of how community members are using OKD and provide details of the underlying hardware and platform configurations they are using. Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI) Prerequisites for vSphere UPI vSphere Installer Provisioned Infrastructure Deployment Single Node OKD Installation Vadim's homelab Sri's homelab Azure Installer Provisioned Infrastructure Default Deployment AWS Installer Provisioned Infrastructure Default Deployment GCP Installer Provisioned Infrastructure Default Deployment","title":"Overview"},{"location":"guides/overview/#deployment-guides","text":"The guides linked below provide some examples of how community members are using OKD and provide details of the underlying hardware and platform configurations they are using. Implementing an Automated Installation Solution for OKD on vSphere with User Provisioned Infrastructure (UPI) Prerequisites for vSphere UPI vSphere Installer Provisioned Infrastructure Deployment Single Node OKD Installation Vadim's homelab Sri's homelab Azure Installer Provisioned Infrastructure Default Deployment AWS Installer Provisioned Infrastructure Default Deployment GCP Installer Provisioned Infrastructure Default Deployment","title":"Deployment Guides"},{"location":"guides/sno/","text":"Single Node OKD Installation \u00b6 This document outlines how to deploy a single node OKD cluster using virt. Requirements \u00b6 Host with a minimal CentOS Stream, Fedora, or CentOS-8 installed ( do not create a /home filesystem ) Monitor, mouse, and keyboard attached to the host Static IP for the host The following packages installed: virt, wget, git, net-tools, bind, bind-utils, bash-completion, rsync, libguestfs-tools, virt-install, epel-release, libvirt-devel, httpd-tools, snf, nginx Procedure \u00b6 For the complete procedure, please see Building an OKD4 single node cluster with minimal resources","title":"Single Node OKD Installation"},{"location":"guides/sno/#single-node-okd-installation","text":"This document outlines how to deploy a single node OKD cluster using virt.","title":"Single Node OKD Installation"},{"location":"guides/sno/#requirements","text":"Host with a minimal CentOS Stream, Fedora, or CentOS-8 installed ( do not create a /home filesystem ) Monitor, mouse, and keyboard attached to the host Static IP for the host The following packages installed: virt, wget, git, net-tools, bind, bind-utils, bash-completion, rsync, libguestfs-tools, virt-install, epel-release, libvirt-devel, httpd-tools, snf, nginx","title":"Requirements"},{"location":"guides/sno/#procedure","text":"For the complete procedure, please see Building an OKD4 single node cluster with minimal resources","title":"Procedure"},{"location":"guides/sri/","text":"Sri's Overkill Homelab Setup \u00b6 This document lays out the resources used to create my completely-overkill homelab. This cluster provides all the compute and storage I think I'll need for the foreseeable future, and the CPU, RAM, and storage can all be scaled vertically independently of each other. Not that I think I'll need to do that for a while. More detail into the deployment and my homelab's Terraform configuration can be found here . Hardware \u00b6 3 hyper-converged hypervisors Ryzen 5 3600 64 GiB RAM 3x 4TiB HDD 2x 500GiB SSD in RAID1 1x 256GiB NVME for boot disk 1 NUC I had laying around gathering dust Intel Core i3-5010U 16 GiB RAM 500GiB SSD Main cluster \u00b6 My hypervisors each host an identical workload. The total size of this cluster is 3 control plane nodes, and 9 worker nodes. So it splits very nicely three ways. Each hypervisor hosts 1 control plane VM and 3 worker VMs. 3 control plane nodes 4x CPU 10 GiB RAM 50 GiB disk 9 worker nodes 8x CPU 16 GiB RAM 50 GiB root disk 4 TiB HDD for workload use 1 bootstrap node (temporary, taken down after initial setup is complete) 4 vCPU 8 GiB RAM 120 GiB root disk Supporting infrastructure \u00b6 Networking \u00b6 OKD, and especially baremetal UPI OKD, requires a very specific network setup. You will most likely need something more flexible than your ISP's router to get everything fully configured. The documentation is very clear on the various DNS records and DHCP static allocations you will need to make, so I won't go into them here. However, there are a couple extra things that you may want to set for best results. In particular, I make sure that I have PTR records set up for all my cluster nodes. This is extremely important as the nodes need a correct PTR record set up for them to auto-discover their hostname. Clusters typically do not set themselves up properly if there are hostname collisions! API load balancer \u00b6 I run a separate smaller VM on the NUC as a single-purpose load balancer appliance, running HAProxy. 1 load balancer VM 2x vCPU 256MiB RAM 10GiB disk The HAProxy config is straightforward. I adapted mine from the example config file created by the ocp4-helpernode playbook. Deployment \u00b6 I create the VMs on the hypervisors using Terraform. The Terraform Libvirt provider is very, very cool. It's also used by openshift-install for its Libvirt-based deployments, so it supports everything needed to deploy OKD nodes. Most importantly, I can use Terraform to supply the VMs with their Ignition configs, which means I don't have to worry about passing kernel args manually or setting up a PXE server to get things going like the official OKD docs would have you do. Terraform also makes it easy to tear down the cluster and reset in case something goes wrong. Post-Bootstrap One-Time Setup \u00b6 Storage with Rook and Ceph \u00b6 I deploy a Ceph cluster into OKD using Rook. The Rook configuration deploys OSDs on top of the 4TiB HDDs assigned to each worker. I deploy an erasure-coded CephFS pool (6+2) for RWX workloads and a 3x replica block pool for RWO workloads. Monitoring and Alerting \u00b6 OKD comes with a very comprehensive monitoring and alerting suite, and it would be a shame not to take advantage of it. I set up an Alertmanager webhook to send any alerts to a small program I wrote that posts the alerts to Discord . I also deploy a Prometheus + Grafana set up into the cluster that collects metrics from the various hypervisors and supporting infrastructure VMs. I use Grafana's built-in Discord alerting mechanism to post those alerts. LoadBalancer with MetalLB \u00b6 MetalLB is a piece of fantastic software that allows on-prem or otherwise non-public-cloud Kubernetes clusters to enjoy the luxury of LoadBalancer type services. It's dead simple to set up and makes you feel you're in a real datacenter. I deploy several workloads that don't use standard HTTP and so can't be deployed behind a Route . Without MetalLB, I wouldn't be able to deploy these workloads on OKD at all but with it, I can! Software I Run \u00b6 I maintain an ansible playbook that handles deploying my workloads into the cluster. I prefer Ansible over other tools like Helm because it has more robust capabilities to store secrets, I find its templating capabilities more flexible and powerful than Helm's (especially when it comes to inlining config files into config maps or creating templated Dockerfiles for BuildConfigs), and because I am already familiar with Ansible and know how it works. paperless-ng - A document organizer that uses machine learning to automatically classify and organize bitwarden_rs - Password manager Jellyfin - Media management Samba - I joined a StatefulSet to my AD domain and it serves an authenticated SMB share Netbox - Infrastructure management tool Quassel - IRC bouncer Ukulele - Bot that plays music into Discord channels RPM and deb package repos for internal packages","title":"Sri's homelab"},{"location":"guides/sri/#sris-overkill-homelab-setup","text":"This document lays out the resources used to create my completely-overkill homelab. This cluster provides all the compute and storage I think I'll need for the foreseeable future, and the CPU, RAM, and storage can all be scaled vertically independently of each other. Not that I think I'll need to do that for a while. More detail into the deployment and my homelab's Terraform configuration can be found here .","title":"Sri's Overkill Homelab Setup"},{"location":"guides/sri/#hardware","text":"3 hyper-converged hypervisors Ryzen 5 3600 64 GiB RAM 3x 4TiB HDD 2x 500GiB SSD in RAID1 1x 256GiB NVME for boot disk 1 NUC I had laying around gathering dust Intel Core i3-5010U 16 GiB RAM 500GiB SSD","title":"Hardware"},{"location":"guides/sri/#main-cluster","text":"My hypervisors each host an identical workload. The total size of this cluster is 3 control plane nodes, and 9 worker nodes. So it splits very nicely three ways. Each hypervisor hosts 1 control plane VM and 3 worker VMs. 3 control plane nodes 4x CPU 10 GiB RAM 50 GiB disk 9 worker nodes 8x CPU 16 GiB RAM 50 GiB root disk 4 TiB HDD for workload use 1 bootstrap node (temporary, taken down after initial setup is complete) 4 vCPU 8 GiB RAM 120 GiB root disk","title":"Main cluster"},{"location":"guides/sri/#supporting-infrastructure","text":"","title":"Supporting infrastructure"},{"location":"guides/sri/#networking","text":"OKD, and especially baremetal UPI OKD, requires a very specific network setup. You will most likely need something more flexible than your ISP's router to get everything fully configured. The documentation is very clear on the various DNS records and DHCP static allocations you will need to make, so I won't go into them here. However, there are a couple extra things that you may want to set for best results. In particular, I make sure that I have PTR records set up for all my cluster nodes. This is extremely important as the nodes need a correct PTR record set up for them to auto-discover their hostname. Clusters typically do not set themselves up properly if there are hostname collisions!","title":"Networking"},{"location":"guides/sri/#api-load-balancer","text":"I run a separate smaller VM on the NUC as a single-purpose load balancer appliance, running HAProxy. 1 load balancer VM 2x vCPU 256MiB RAM 10GiB disk The HAProxy config is straightforward. I adapted mine from the example config file created by the ocp4-helpernode playbook.","title":"API load balancer"},{"location":"guides/sri/#deployment","text":"I create the VMs on the hypervisors using Terraform. The Terraform Libvirt provider is very, very cool. It's also used by openshift-install for its Libvirt-based deployments, so it supports everything needed to deploy OKD nodes. Most importantly, I can use Terraform to supply the VMs with their Ignition configs, which means I don't have to worry about passing kernel args manually or setting up a PXE server to get things going like the official OKD docs would have you do. Terraform also makes it easy to tear down the cluster and reset in case something goes wrong.","title":"Deployment"},{"location":"guides/sri/#post-bootstrap-one-time-setup","text":"","title":"Post-Bootstrap One-Time Setup"},{"location":"guides/sri/#storage-with-rook-and-ceph","text":"I deploy a Ceph cluster into OKD using Rook. The Rook configuration deploys OSDs on top of the 4TiB HDDs assigned to each worker. I deploy an erasure-coded CephFS pool (6+2) for RWX workloads and a 3x replica block pool for RWO workloads.","title":"Storage with Rook and Ceph"},{"location":"guides/sri/#monitoring-and-alerting","text":"OKD comes with a very comprehensive monitoring and alerting suite, and it would be a shame not to take advantage of it. I set up an Alertmanager webhook to send any alerts to a small program I wrote that posts the alerts to Discord . I also deploy a Prometheus + Grafana set up into the cluster that collects metrics from the various hypervisors and supporting infrastructure VMs. I use Grafana's built-in Discord alerting mechanism to post those alerts.","title":"Monitoring and Alerting"},{"location":"guides/sri/#loadbalancer-with-metallb","text":"MetalLB is a piece of fantastic software that allows on-prem or otherwise non-public-cloud Kubernetes clusters to enjoy the luxury of LoadBalancer type services. It's dead simple to set up and makes you feel you're in a real datacenter. I deploy several workloads that don't use standard HTTP and so can't be deployed behind a Route . Without MetalLB, I wouldn't be able to deploy these workloads on OKD at all but with it, I can!","title":"LoadBalancer with MetalLB"},{"location":"guides/sri/#software-i-run","text":"I maintain an ansible playbook that handles deploying my workloads into the cluster. I prefer Ansible over other tools like Helm because it has more robust capabilities to store secrets, I find its templating capabilities more flexible and powerful than Helm's (especially when it comes to inlining config files into config maps or creating templated Dockerfiles for BuildConfigs), and because I am already familiar with Ansible and know how it works. paperless-ng - A document organizer that uses machine learning to automatically classify and organize bitwarden_rs - Password manager Jellyfin - Media management Samba - I joined a StatefulSet to my AD domain and it serves an authenticated SMB share Netbox - Infrastructure management tool Quassel - IRC bouncer Ukulele - Bot that plays music into Discord channels RPM and deb package repos for internal packages","title":"Software I Run"},{"location":"guides/vadim/","text":"Vadim's homelab \u00b6 This describes the resources used by OpenShift after performing an installation to make it similar to my homelab setup. Compute \u00b6 Ubiquity EdgeRouter ER-X runs DHCP (embedded), custom DNS server via AdGuard NAS/Bastion host haproxy for loadbalancer ceph cluster for PVs NFS server for shared data control plane Intel i5 CPU, 16+4 GB RAM 120 GB NVME disk compute nodes Lenovo X220 laptop Router setup \u00b6 Once nodes have booted assign static IPs using MAC pinning. EdgeRouter has dnsmasq to support custom DNS entries, but I wanted to have a network-wide ad filtering and DNS-over-TLS for free, so I followed this guide to install AdGuard Home on the router. This gives a fancy UI for DNS rewrites and gives a useful stats about the nodes on the network. NAS/Bastion setup \u00b6 HAProxy setup is fairly standard - see ocp4-helpernode for idea. Along with (fairly standard) NFS server I also run a single node Ceph cluster, so that I could benefit from CSI / autoprovision / snapshots etc. Installation \u00b6 Currently \"single node install\" requires a dedicated throwaway bootstrap node, so I used future compute node (x220 laptop) as a bootstrap node. Once master was installed, the laptop was re-provisioned to become a compute node. Upgrading \u00b6 Since I use a single master install, upgrades are bit complicated. Both nodes are labelled as workers, so upgrading those is not an issue. Upgrading single master is tricky, so I use this script to pivot the node into expected master ignition content, which runs rpm-ostree rebase <new content> . This script needs to be cancelled before it starts installing OS extensions (NetworkManager-ovs etc.) as its necessary. This issue as a class would be addressed in 4.8. Useful software \u00b6 Grafana operator is incredibly useful to setup monitoring. This operator helps me to define a configuration for various datasources (i.e. Promtail+Loki ) and control dashboard source code using CRs. SnapScheduler makes periodic snapshots of some PVs so that risky changes could be reverted. Tekton operator is helping me to run a few clean up jobs in cluster periodically. Most useful pipeline I've been using is running oc adm must-gather on this cluster, unpacking it and storing it in Git. This helps me keep track of changes in the cluster in a git repo - and, unlike gitops solution like ArgoCD - I can still tinker with things in the console. Other useful software running in my cluster: Gitea - git server HomeAssistant - controls smart home devices BitWarden_rs - password storage Minio - S3-like storage Nextcloud - file sync software Navidrome - music server MiniFlux - RSS reader Matrix Synapse - federated chat app Pleroma - federated microblogging app Wallabag - Read-It-Later app","title":"Vadim's homelab"},{"location":"guides/vadim/#vadims-homelab","text":"This describes the resources used by OpenShift after performing an installation to make it similar to my homelab setup.","title":"Vadim's homelab"},{"location":"guides/vadim/#compute","text":"Ubiquity EdgeRouter ER-X runs DHCP (embedded), custom DNS server via AdGuard NAS/Bastion host haproxy for loadbalancer ceph cluster for PVs NFS server for shared data control plane Intel i5 CPU, 16+4 GB RAM 120 GB NVME disk compute nodes Lenovo X220 laptop","title":"Compute"},{"location":"guides/vadim/#router-setup","text":"Once nodes have booted assign static IPs using MAC pinning. EdgeRouter has dnsmasq to support custom DNS entries, but I wanted to have a network-wide ad filtering and DNS-over-TLS for free, so I followed this guide to install AdGuard Home on the router. This gives a fancy UI for DNS rewrites and gives a useful stats about the nodes on the network.","title":"Router setup"},{"location":"guides/vadim/#nasbastion-setup","text":"HAProxy setup is fairly standard - see ocp4-helpernode for idea. Along with (fairly standard) NFS server I also run a single node Ceph cluster, so that I could benefit from CSI / autoprovision / snapshots etc.","title":"NAS/Bastion setup"},{"location":"guides/vadim/#installation","text":"Currently \"single node install\" requires a dedicated throwaway bootstrap node, so I used future compute node (x220 laptop) as a bootstrap node. Once master was installed, the laptop was re-provisioned to become a compute node.","title":"Installation"},{"location":"guides/vadim/#upgrading","text":"Since I use a single master install, upgrades are bit complicated. Both nodes are labelled as workers, so upgrading those is not an issue. Upgrading single master is tricky, so I use this script to pivot the node into expected master ignition content, which runs rpm-ostree rebase <new content> . This script needs to be cancelled before it starts installing OS extensions (NetworkManager-ovs etc.) as its necessary. This issue as a class would be addressed in 4.8.","title":"Upgrading"},{"location":"guides/vadim/#useful-software","text":"Grafana operator is incredibly useful to setup monitoring. This operator helps me to define a configuration for various datasources (i.e. Promtail+Loki ) and control dashboard source code using CRs. SnapScheduler makes periodic snapshots of some PVs so that risky changes could be reverted. Tekton operator is helping me to run a few clean up jobs in cluster periodically. Most useful pipeline I've been using is running oc adm must-gather on this cluster, unpacking it and storing it in Git. This helps me keep track of changes in the cluster in a git repo - and, unlike gitops solution like ArgoCD - I can still tinker with things in the console. Other useful software running in my cluster: Gitea - git server HomeAssistant - controls smart home devices BitWarden_rs - password storage Minio - S3-like storage Nextcloud - file sync software Navidrome - music server MiniFlux - RSS reader Matrix Synapse - federated chat app Pleroma - federated microblogging app Wallabag - Read-It-Later app","title":"Useful software"},{"location":"guides/vsphere-ipi/","text":"vSphere IPI Deployment \u00b6 This describes the resources used by OpenShift after performing an installation using the required options for the installer. Infrastructure \u00b6 Compute \u00b6 All vms stored within folder described above and tagged with tag created by installer. 3 control plane vms (name format: {cluster name}-{generated cluster id}-master-{0,1,2} ) 4 vCPU 16 GB RAM 120 GB storage 3 worker vms (name format: {cluster name}-{generated cluster id}-master-{generated worker id} ) 2 vCPU 8 GB RAM 120 GB storage Networking \u00b6 Should be set up by user. Installer doesn't create anything there. Network name should be provided as installer argument. Miscellaneous \u00b6 tag category with format openshift-{cluster name}-{generated cluster id} tag with format {cluster name}-{generated cluster id} folder with title format {cluster name}-{generated cluster id} disabled virtual machine with name {cluster name}-rhcos-{generated cluster id} which using as template for further scaling Deployment \u00b6 See the OKD documentation to proceed with deployment","title":"vSphere IPI Deployment"},{"location":"guides/vsphere-ipi/#vsphere-ipi-deployment","text":"This describes the resources used by OpenShift after performing an installation using the required options for the installer.","title":"vSphere IPI Deployment"},{"location":"guides/vsphere-ipi/#infrastructure","text":"","title":"Infrastructure"},{"location":"guides/vsphere-ipi/#compute","text":"All vms stored within folder described above and tagged with tag created by installer. 3 control plane vms (name format: {cluster name}-{generated cluster id}-master-{0,1,2} ) 4 vCPU 16 GB RAM 120 GB storage 3 worker vms (name format: {cluster name}-{generated cluster id}-master-{generated worker id} ) 2 vCPU 8 GB RAM 120 GB storage","title":"Compute"},{"location":"guides/vsphere-ipi/#networking","text":"Should be set up by user. Installer doesn't create anything there. Network name should be provided as installer argument.","title":"Networking"},{"location":"guides/vsphere-ipi/#miscellaneous","text":"tag category with format openshift-{cluster name}-{generated cluster id} tag with format {cluster name}-{generated cluster id} folder with title format {cluster name}-{generated cluster id} disabled virtual machine with name {cluster name}-rhcos-{generated cluster id} which using as template for further scaling","title":"Miscellaneous"},{"location":"guides/vsphere-ipi/#deployment","text":"See the OKD documentation to proceed with deployment","title":"Deployment"},{"location":"guides/vsphere-prereqs/","text":"Prerequisites for vSphere UPI \u00b6 In this example I describe the setup of a DNS/DHCP server and a Load Balancer on a Raspberry PI microcomputer. The instructions most certainly will also work for other environments. I use Raspberry Pi OS (debian based). IP Addresses of components in this example \u00b6 Homelab subnet: 192.168.178.0/24 DSL router/gateway: 192.168.178.1 IP address of Raspberry Pi (DHCP/DNS/Load Balancer): 192.168.178.5 local domain: homelab.net local cluster (name: c1) domain: c1.homelab.net DHCP range: 192.168.178.40 \u2026 192.168.178.199 Static IPs for OKD\u2019s bootstrap, masters and workers Upgrade Raspberry Pi \u00b6 sudo apt-get update sudo apt-get upgrade sudo reboot Set static IP address on Raspberry Pi \u00b6 Add this: interface eth0 static ip_address=192.168.178.5/24 static routers=192.168.178.1 static domain_name_servers=192.168.178.5 8.8.8.8 to /etc/dhcpcd.conf DHCP \u00b6 Ensure that no other DHCP servers are activated in the network of your homelab e.g. in your internet router. The DHCP server in this example is setup with DDNS (Dynamic DNS) enabled. Install \u00b6 sudo apt-get install isc-dhcp-server Configure \u00b6 Enable DHCP server for IPv4 on eth0: /etc/default/isc-dhcp-server INTERFACESv4=\"eth0\" INTERFACESv6=\"\" /etc/dhcp/dhcpd.conf # dhcpd.conf # #################################################################################### # Configuration for Dynamic DNS (DDNS) updates # # Clients requesting an IP and sending their hostname for domain *.homelab.net # # will be auto registered in the DNS server. # #################################################################################### ddns-updates on; ddns-update-style standard; # This option points to the copy rndc.key we created for bind9. include \"/etc/bind/rndc.key\"; allow unknown-clients; use-host-decl-names on; default-lease-time 300; # 5 minutes max-lease-time 300; # 5 minutes # homelab.net DNS zones zone homelab.net. { primary 192.168.178.5; # This server is the primary DNS server for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } zone 178.168.192.in-addr.arpa. { primary 192.168.178.5; # This server is the primary reverse DNS for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } ddns-domainname \"homelab.net.\"; ddns-rev-domainname \"in-addr.arpa.\"; #################################################################################### #################################################################################### # Basic configuration # #################################################################################### # option definitions common to all supported networks... default-lease-time 300; max-lease-time 300; # If this DHCP server is the official DHCP server for the local # network, the authoritative directive should be uncommented. authoritative; # Parts of this section will be put in the /etc/resolv.conf of your hosts later option domain-name \"homelab.net\"; option routers 192.168.178.1; option subnet-mask 255.255.255.0; option domain-name-servers 192.168.178.5; subnet 192.168.178.0 netmask 255.255.255.0 { range 192.168.178.40 192.168.178.199; } #################################################################################### #################################################################################### # Static IP addresses # # (Replace the MAC addresses here with the ones you set in vsphere for your vms) # #################################################################################### group { host bootstrap { hardware ethernet 00:1c:00:00:00:00; fixed-address 192.168.178.200; } host master0 { hardware ethernet 00:1c:00:00:00:10; fixed-address 192.168.178.210; } host master1 { hardware ethernet 00:1c:00:00:00:11; fixed-address 192.168.178.211; } host master2 { hardware ethernet 00:1c:00:00:00:12; fixed-address 192.168.178.212; } host worker0 { hardware ethernet 00:1c:00:00:00:20; fixed-address 192.168.178.220; } host worker1 { hardware ethernet 00:1c:00:00:00:21; fixed-address 192.168.178.221; } host worker2 { hardware ethernet 00:1c:00:00:00:22; fixed-address 192.168.178.222; } } DNS \u00b6 Install \u00b6 sudo apt install bind9 dnsutils Basic configuration \u00b6 /etc/bind/named.conf.options include \"/etc/bind/rndc.key\"; acl internals { // lo adapter 127.0.0.1; // CIDR for your homelab network 192.168.178.0/24; }; options { directory \"/var/cache/bind\"; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. forwarders { 8.8.8.8; 8.8.4.4; }; forward only; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation no; listen-on-v6 { none; }; auth-nxdomain no; listen-on port 53 { any; }; // Allow queries from my Homelab and also from Wireguard Clients. allow-query { internals; }; allow-query-cache { internals; }; allow-update { internals; }; recursion yes; allow-recursion { internals; }; allow-transfer { internals; }; dnssec-enable no; check-names master ignore; check-names slave ignore; check-names response ignore; }; /etc/bind/named.conf.local #include \"/etc/bind/rndc.key\"; // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; # All devices that don't belong to the OKD cluster will be maintained here. zone \"homelab.net\" { type master; file \"/etc/bind/forward.homelab.net\"; allow-update { key rndc-key; }; }; zone \"c1.homelab.net\" { type master; file \"/etc/bind/forward.c1.homelab.net\"; allow-update { key rndc-key; }; }; zone \"178.168.192.in-addr.arpa\" { type master; notify no; file \"/etc/bind/178.168.192.in-addr.arpa\"; allow-update { key rndc-key; }; }; Zone file for homlab.net: /etc/bind/forward.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA homelab.net. root.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 The name of the next file depends on the subnet that is used: /etc/bind/178.168.192.in-addr.arpa $TTL 1W @ IN SOA ns1.homelab.net. root.homelab.net. ( 2019070742 ; serial 10800 ; refresh (3 hours) 1800 ; retry (30 minutes) 1209600 ; expire (2 weeks) 604800 ; minimum (1 week) ) NS ns1.homelab.net. 200 PTR bootstrap.c1.homelab.net. 210 PTR master0.c1.homelab.net. 211 PTR master1.c1.homelab.net. 212 PTR master2.c1.homelab.net. 220 PTR worker0.c1.homelab.net. 221 PTR worker1.c1.homelab.net. 222 PTR worker2.c1.homelab.net. 5 PTR api.c1.homelab.net. 5 PTR api-int.c1.homelab.net. DNS records for OKD 4 \u00b6 Zone file for c1.homelab.net (our OKD 4 cluster will be in this domain): /etc/bind/forward.c1.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA c1.homelab.net. root.c1.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS c1.homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 load-balancer IN A 192.168.178.5 bootstrap IN A 192.168.178.200 master0 IN A 192.168.178.210 master1 IN A 192.168.178.211 master2 IN A 192.168.178.212 worker0 IN A 192.168.178.220 worker1 IN A 192.168.178.221 worker2 IN A 192.168.178.222 worker3 IN A 192.168.178.223 *.apps.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api-int.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. Set file permissions \u00b6 For dynamic DNS (ddns) to work you should do this: sudo chown -R bind:bind /etc/bind Load Balancer \u00b6 Install \u00b6 sudo apt-get install haproxy Configure \u00b6 /etc/haproxy/haproxy.cfg global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from: # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 20000 timeout client 10000 timeout server 10000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # You can see the stats and observe OKD's bootstrap process by opening # http://<IP>:4321/haproxy?stats listen stats bind :4321 mode http log global maxconn 10 timeout client 100s timeout server 100s timeout connect 100s timeout queue 100s stats enable stats hide-version stats refresh 30s stats show-node stats auth admin:password stats uri /haproxy?stats frontend openshift-api-server bind *:6443 default_backend openshift-api-server mode tcp option tcplog backend openshift-api-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:6443 check server master0 master0.c1.homelab.net:6443 check server master1 master1.c1.homelab.net:6443 check server master2 master2.c1.homelab.net:6443 check frontend machine-config-server bind *:22623 default_backend machine-config-server mode tcp option tcplog backend machine-config-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:22623 check server master0 master0.c1.homelab.net:22623 check server master1 master1.c1.homelab.net:22623 check server master2 master2.c1.homelab.net:22623 check frontend ingress-http bind *:80 default_backend ingress-http mode tcp option tcplog backend ingress-http balance source mode tcp server master0 master0.c1.homelab.net:80 check server master1 master1.c1.homelab.net:80 check server master2 master2.c1.homelab.net:80 check server worker0 worker0.c1.homelab.net:80 check server worker1 worker1.c1.homelab.net:80 check server worker2 worker2.c1.homelab.net:80 check server worker3 worker3.c1.homelab.net:80 check frontend ingress-https bind *:443 default_backend ingress-https mode tcp option tcplog backend ingress-https balance source mode tcp server master0 master0.c1.homelab.net:443 check server master1 master1.c1.homelab.net:443 check server master2 master2.c1.homelab.net:443 check server worker0 worker0.c1.homelab.net:443 check server worker1 worker1.c1.homelab.net:443 check server worker2 worker2.c1.homelab.net:443 check server worker3 worker3.c1.homelab.net:443 check Reboot and check status \u00b6 Reboot Raspberry Pi: sudo reboot Check status of DNS/DHCP server and Load Balancer: sudo systemctl status haproxy.service sudo systemctl status isc-dhcp-server.service sudo systemctl status bind9 Proxy (if on a private network) \u00b6 If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid","title":"Prerequites for vSphere UPI"},{"location":"guides/vsphere-prereqs/#prerequisites-for-vsphere-upi","text":"In this example I describe the setup of a DNS/DHCP server and a Load Balancer on a Raspberry PI microcomputer. The instructions most certainly will also work for other environments. I use Raspberry Pi OS (debian based).","title":"Prerequisites for vSphere UPI"},{"location":"guides/vsphere-prereqs/#ip-addresses-of-components-in-this-example","text":"Homelab subnet: 192.168.178.0/24 DSL router/gateway: 192.168.178.1 IP address of Raspberry Pi (DHCP/DNS/Load Balancer): 192.168.178.5 local domain: homelab.net local cluster (name: c1) domain: c1.homelab.net DHCP range: 192.168.178.40 \u2026 192.168.178.199 Static IPs for OKD\u2019s bootstrap, masters and workers","title":"IP Addresses of components in this example"},{"location":"guides/vsphere-prereqs/#upgrade-raspberry-pi","text":"sudo apt-get update sudo apt-get upgrade sudo reboot","title":"Upgrade Raspberry Pi"},{"location":"guides/vsphere-prereqs/#set-static-ip-address-on-raspberry-pi","text":"Add this: interface eth0 static ip_address=192.168.178.5/24 static routers=192.168.178.1 static domain_name_servers=192.168.178.5 8.8.8.8 to /etc/dhcpcd.conf","title":"Set static IP address on Raspberry Pi"},{"location":"guides/vsphere-prereqs/#dhcp","text":"Ensure that no other DHCP servers are activated in the network of your homelab e.g. in your internet router. The DHCP server in this example is setup with DDNS (Dynamic DNS) enabled.","title":"DHCP"},{"location":"guides/vsphere-prereqs/#install","text":"sudo apt-get install isc-dhcp-server","title":"Install"},{"location":"guides/vsphere-prereqs/#configure","text":"Enable DHCP server for IPv4 on eth0: /etc/default/isc-dhcp-server INTERFACESv4=\"eth0\" INTERFACESv6=\"\" /etc/dhcp/dhcpd.conf # dhcpd.conf # #################################################################################### # Configuration for Dynamic DNS (DDNS) updates # # Clients requesting an IP and sending their hostname for domain *.homelab.net # # will be auto registered in the DNS server. # #################################################################################### ddns-updates on; ddns-update-style standard; # This option points to the copy rndc.key we created for bind9. include \"/etc/bind/rndc.key\"; allow unknown-clients; use-host-decl-names on; default-lease-time 300; # 5 minutes max-lease-time 300; # 5 minutes # homelab.net DNS zones zone homelab.net. { primary 192.168.178.5; # This server is the primary DNS server for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } zone 178.168.192.in-addr.arpa. { primary 192.168.178.5; # This server is the primary reverse DNS for the zone key rndc-key; # Use the key we defined earlier for dynamic updates } ddns-domainname \"homelab.net.\"; ddns-rev-domainname \"in-addr.arpa.\"; #################################################################################### #################################################################################### # Basic configuration # #################################################################################### # option definitions common to all supported networks... default-lease-time 300; max-lease-time 300; # If this DHCP server is the official DHCP server for the local # network, the authoritative directive should be uncommented. authoritative; # Parts of this section will be put in the /etc/resolv.conf of your hosts later option domain-name \"homelab.net\"; option routers 192.168.178.1; option subnet-mask 255.255.255.0; option domain-name-servers 192.168.178.5; subnet 192.168.178.0 netmask 255.255.255.0 { range 192.168.178.40 192.168.178.199; } #################################################################################### #################################################################################### # Static IP addresses # # (Replace the MAC addresses here with the ones you set in vsphere for your vms) # #################################################################################### group { host bootstrap { hardware ethernet 00:1c:00:00:00:00; fixed-address 192.168.178.200; } host master0 { hardware ethernet 00:1c:00:00:00:10; fixed-address 192.168.178.210; } host master1 { hardware ethernet 00:1c:00:00:00:11; fixed-address 192.168.178.211; } host master2 { hardware ethernet 00:1c:00:00:00:12; fixed-address 192.168.178.212; } host worker0 { hardware ethernet 00:1c:00:00:00:20; fixed-address 192.168.178.220; } host worker1 { hardware ethernet 00:1c:00:00:00:21; fixed-address 192.168.178.221; } host worker2 { hardware ethernet 00:1c:00:00:00:22; fixed-address 192.168.178.222; } }","title":"Configure"},{"location":"guides/vsphere-prereqs/#dns","text":"","title":"DNS"},{"location":"guides/vsphere-prereqs/#install_1","text":"sudo apt install bind9 dnsutils","title":"Install"},{"location":"guides/vsphere-prereqs/#basic-configuration","text":"/etc/bind/named.conf.options include \"/etc/bind/rndc.key\"; acl internals { // lo adapter 127.0.0.1; // CIDR for your homelab network 192.168.178.0/24; }; options { directory \"/var/cache/bind\"; // If there is a firewall between you and nameservers you want // to talk to, you may need to fix the firewall to allow multiple // ports to talk. See http://www.kb.cert.org/vuls/id/800113 // If your ISP provided one or more IP addresses for stable // nameservers, you probably want to use them as forwarders. // Uncomment the following block, and insert the addresses replacing // the all-0's placeholder. forwarders { 8.8.8.8; 8.8.4.4; }; forward only; //======================================================================== // If BIND logs error messages about the root key being expired, // you will need to update your keys. See https://www.isc.org/bind-keys //======================================================================== dnssec-validation no; listen-on-v6 { none; }; auth-nxdomain no; listen-on port 53 { any; }; // Allow queries from my Homelab and also from Wireguard Clients. allow-query { internals; }; allow-query-cache { internals; }; allow-update { internals; }; recursion yes; allow-recursion { internals; }; allow-transfer { internals; }; dnssec-enable no; check-names master ignore; check-names slave ignore; check-names response ignore; }; /etc/bind/named.conf.local #include \"/etc/bind/rndc.key\"; // // Do any local configuration here // // Consider adding the 1918 zones here, if they are not used in your // organization //include \"/etc/bind/zones.rfc1918\"; # All devices that don't belong to the OKD cluster will be maintained here. zone \"homelab.net\" { type master; file \"/etc/bind/forward.homelab.net\"; allow-update { key rndc-key; }; }; zone \"c1.homelab.net\" { type master; file \"/etc/bind/forward.c1.homelab.net\"; allow-update { key rndc-key; }; }; zone \"178.168.192.in-addr.arpa\" { type master; notify no; file \"/etc/bind/178.168.192.in-addr.arpa\"; allow-update { key rndc-key; }; }; Zone file for homlab.net: /etc/bind/forward.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA homelab.net. root.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 The name of the next file depends on the subnet that is used: /etc/bind/178.168.192.in-addr.arpa $TTL 1W @ IN SOA ns1.homelab.net. root.homelab.net. ( 2019070742 ; serial 10800 ; refresh (3 hours) 1800 ; retry (30 minutes) 1209600 ; expire (2 weeks) 604800 ; minimum (1 week) ) NS ns1.homelab.net. 200 PTR bootstrap.c1.homelab.net. 210 PTR master0.c1.homelab.net. 211 PTR master1.c1.homelab.net. 212 PTR master2.c1.homelab.net. 220 PTR worker0.c1.homelab.net. 221 PTR worker1.c1.homelab.net. 222 PTR worker2.c1.homelab.net. 5 PTR api.c1.homelab.net. 5 PTR api-int.c1.homelab.net.","title":"Basic configuration"},{"location":"guides/vsphere-prereqs/#dns-records-for-okd-4","text":"Zone file for c1.homelab.net (our OKD 4 cluster will be in this domain): /etc/bind/forward.c1.homelab.net ; ; BIND data file for local loopback interface ; $TTL 604800 @ IN SOA c1.homelab.net. root.c1.homelab.net. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS c1.homelab.net. @ IN A 192.168.178.5 @ IN AAAA ::1 load-balancer IN A 192.168.178.5 bootstrap IN A 192.168.178.200 master0 IN A 192.168.178.210 master1 IN A 192.168.178.211 master2 IN A 192.168.178.212 worker0 IN A 192.168.178.220 worker1 IN A 192.168.178.221 worker2 IN A 192.168.178.222 worker3 IN A 192.168.178.223 *.apps.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api-int.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net. api.c1.homelab.net. IN CNAME load-balancer.c1.homelab.net.","title":"DNS records for OKD 4"},{"location":"guides/vsphere-prereqs/#set-file-permissions","text":"For dynamic DNS (ddns) to work you should do this: sudo chown -R bind:bind /etc/bind","title":"Set file permissions"},{"location":"guides/vsphere-prereqs/#load-balancer","text":"","title":"Load Balancer"},{"location":"guides/vsphere-prereqs/#install_2","text":"sudo apt-get install haproxy","title":"Install"},{"location":"guides/vsphere-prereqs/#configure_1","text":"/etc/haproxy/haproxy.cfg global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners stats timeout 30s user haproxy group haproxy daemon # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/ssl/private # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). This list is from: # https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/ # An alternative list with additional directives can be obtained from # https://mozilla.github.io/server-side-tls/ssl-config-generator/?server=haproxy ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS ssl-default-bind-options no-sslv3 defaults log global mode http option httplog option dontlognull timeout connect 20000 timeout client 10000 timeout server 10000 errorfile 400 /etc/haproxy/errors/400.http errorfile 403 /etc/haproxy/errors/403.http errorfile 408 /etc/haproxy/errors/408.http errorfile 500 /etc/haproxy/errors/500.http errorfile 502 /etc/haproxy/errors/502.http errorfile 503 /etc/haproxy/errors/503.http errorfile 504 /etc/haproxy/errors/504.http # You can see the stats and observe OKD's bootstrap process by opening # http://<IP>:4321/haproxy?stats listen stats bind :4321 mode http log global maxconn 10 timeout client 100s timeout server 100s timeout connect 100s timeout queue 100s stats enable stats hide-version stats refresh 30s stats show-node stats auth admin:password stats uri /haproxy?stats frontend openshift-api-server bind *:6443 default_backend openshift-api-server mode tcp option tcplog backend openshift-api-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:6443 check server master0 master0.c1.homelab.net:6443 check server master1 master1.c1.homelab.net:6443 check server master2 master2.c1.homelab.net:6443 check frontend machine-config-server bind *:22623 default_backend machine-config-server mode tcp option tcplog backend machine-config-server balance source mode tcp server bootstrap bootstrap.c1.homelab.net:22623 check server master0 master0.c1.homelab.net:22623 check server master1 master1.c1.homelab.net:22623 check server master2 master2.c1.homelab.net:22623 check frontend ingress-http bind *:80 default_backend ingress-http mode tcp option tcplog backend ingress-http balance source mode tcp server master0 master0.c1.homelab.net:80 check server master1 master1.c1.homelab.net:80 check server master2 master2.c1.homelab.net:80 check server worker0 worker0.c1.homelab.net:80 check server worker1 worker1.c1.homelab.net:80 check server worker2 worker2.c1.homelab.net:80 check server worker3 worker3.c1.homelab.net:80 check frontend ingress-https bind *:443 default_backend ingress-https mode tcp option tcplog backend ingress-https balance source mode tcp server master0 master0.c1.homelab.net:443 check server master1 master1.c1.homelab.net:443 check server master2 master2.c1.homelab.net:443 check server worker0 worker0.c1.homelab.net:443 check server worker1 worker1.c1.homelab.net:443 check server worker2 worker2.c1.homelab.net:443 check server worker3 worker3.c1.homelab.net:443 check","title":"Configure"},{"location":"guides/vsphere-prereqs/#reboot-and-check-status","text":"Reboot Raspberry Pi: sudo reboot Check status of DNS/DHCP server and Load Balancer: sudo systemctl status haproxy.service sudo systemctl status isc-dhcp-server.service sudo systemctl status bind9","title":"Reboot and check status"},{"location":"guides/vsphere-prereqs/#proxy-if-on-a-private-network","text":"If the cluster will sit on a private network, you\u2019ll need a proxy for outgoing traffic, both for the install process and for regular operation. In the case of the former, the installer needs to pull containers from the external registries. In the case of the latter, the proxy is needed when application containers need access to the outside world (e.g. yum installs, external code repositories like gitlab, etc.) The proxy should be configured to accept connections from the IP subnet for your cluster. A simple proxy to use for this purpose is squid","title":"Proxy (if on a private network)"},{"location":"wg_crc/overview/","text":"CRC Build Subgroup \u00b6 Code-Ready Containers is a cut down version of OKD, designed to run on a developer's machine, which would not have sufficient resources for a full installation of OKD. The working group was established after a live session where Red Hat's Charro Gruver walked through the build process for OKD CRC The build process is currently manual, so the working group was established to automate the process and investigate options for creating a continuous integration setup to build and test OKD CRC.","title":"Overview"},{"location":"wg_crc/overview/#crc-build-subgroup","text":"Code-Ready Containers is a cut down version of OKD, designed to run on a developer's machine, which would not have sufficient resources for a full installation of OKD. The working group was established after a live session where Red Hat's Charro Gruver walked through the build process for OKD CRC The build process is currently manual, so the working group was established to automate the process and investigate options for creating a continuous integration setup to build and test OKD CRC.","title":"CRC Build Subgroup"},{"location":"wg_docs/okd-io/","text":"Contributing to okd.io \u00b6 The source for okd.io is in a github repository . To update or add new content to the site you need to fork the repository in your own github account, make the changes in your local repository then create a pull request to deliver the updates to the primary repository. Todo Add more specific instructions to help those less familiar with git Setting up a documentation environment \u00b6 To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: Tooling within a container You can use a container and run MkDocs from the container, so no local installation is required: Todo Add instructions for building and using a container Local mkdocs and python tooling installation You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Install Node.js on your system Clone your fork of the okd.io repository cd into the local repo directory (./okd.io) Install the required python packages `pip install -r requirements.txt' Install the spell checker using command npm ci Note sudo command may be needed to install globally, depending on your system configuration You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command linkchecker -f linkcheckerrc --check-extern public . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling cspell docs/**/*.md should be run in the root folder of the project, containing the cspell.json file. There is also a convenience script ./build.sh in the root of the repository that will check spelling, build the site then run the link checker. The site is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website that can be accessed from a filesystem or served from a web server. A link checker tool is also used to validate all links in the MkDocs generated website. Updating the site \u00b6 To make your changes live, create a pull request to deliver the changes in your fork of the repo to the main branch of the okd.io repo. Github automation is used to generate the site then publish to github pages, which serves the site. If your changes contain spelling issues or broken links, then the automation will fail and the github pages site will not be updated, so please do a local test using the build.sh script before creating the pull request. Changing content \u00b6 MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the in-page navigation on the right of the page. Standard Markdown features \u00b6 The following markdown syntax is used within the documentation Syntax Result # Title heading - you can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic `code` inline code block ```shell ... ``` multi-line (Fenced) code block 1. list item ordered list - unordered list item unordered list --- horizontal break HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions. Indentation \u00b6 MkDocs uses 4 spaces for tabs, so when indenting code ensure you are working with tabs set to 4 spaces rather than 2, which is commonly used. When using some features of Markdown indentation is used to identify blocks. 1. Ubiquity EdgeRouter ER-X - runs DHCP (embedded), custom DNS server via AdGuard ![ pic ]( ./img/erx.jpg ){width=80%} In the code block above you will see the unordered list item is indented, so it aligns with the content of the ordered list (rather than aligning with the number of the ordered list). The image is also indented so it too aligns with the ordered list text. Many of the Markdown elements can be nested and indentation is used to define the nesting relationship. If you look down on this page at the Information boxes section, the example shows an example of nesting elements and the Markdown tab shows how indentation is being used to identify the nesting relationships. Links within MkDocs generated content \u00b6 MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information Markdown Extensions used in OKD.io \u00b6 There are a number of Markdown extensions being used to create the site. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here Link configuration \u00b6 Links on the page or embedded images can be annotated to control the links and also the appearance of the links: Image \u00b6 Images are embedded in a page using the standard Markdown syntax ![description](URL) , but the image can be formatted with Attribute Lists . This is most commonly used to scale an image or center an image, e.g. ![ GitHub repo url ]( images/github-repo-url.png ){style=\"width: 80%\" .center } External Links \u00b6 External links can also use attribute lists to control behaviors, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below: [ MkDocs ]( http://mkdocs.org ){: target=\"_blank\" .external } Info You can embed an image as the description of a link to create clickable images that launch to another site: [![Image description](Image URL)](target URL \"hover text\"){: target=_blank} YouTube videos \u00b6 It is not possible to embed a YouTube video and have it play in place using pure markdown. You can use HTML within the markdown file to embed a video: < iframe width = \"100%\" height = \"500\" src = \"https://www.youtube.com/watch?v=qh1zYW7BLxE&t=431s\" title = \"Building an OKD 4 Home Lab with special guest Craig Robinson\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></ iframe > Tabs \u00b6 Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World Information boxes \u00b6 The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note Supported Admonition Classes \u00b6 The Admonitions supported by the Material theme are : Note This is a note Abstract This is an abstract Info This is an info Tip This is a tip Success This is a success Question This is a question Warning This is a warning Failure This is a failure Danger This is a danger Bug This is a bug Example This is an example Quote This is a quote Code blocks \u00b6 Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ```shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content Advanced highlighting of code blocks \u00b6 There are some additional features available due to the highlight plugin installed in MkDocs. Full details can be found in the MkDocs Materials documentation . Line numbers \u00b6 You can add line numbers to a code block with the linenums directive. You must specify the starting line number, 1 in the example below: ``` javascript linenums=\"1\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 1 2 3 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Info The line numbers do not get included when the copy to clipboard link is selected Spell checking \u00b6 This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project. Adding local words \u00b6 You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment. Adding global words \u00b6 The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Modifying OKD.io"},{"location":"wg_docs/okd-io/#contributing-to-okdio","text":"The source for okd.io is in a github repository . To update or add new content to the site you need to fork the repository in your own github account, make the changes in your local repository then create a pull request to deliver the updates to the primary repository. Todo Add more specific instructions to help those less familiar with git","title":"Contributing to okd.io"},{"location":"wg_docs/okd-io/#setting-up-a-documentation-environment","text":"To work on documentation and be able to view the rendered web site you need to create an environment, which comprises of: Tooling within a container You can use a container and run MkDocs from the container, so no local installation is required: Todo Add instructions for building and using a container Local mkdocs and python tooling installation You can install MkDocs and associated plugins on your development system and run the tools locally: Install Python 3 on your system Install Node.js on your system Clone your fork of the okd.io repository cd into the local repo directory (./okd.io) Install the required python packages `pip install -r requirements.txt' Install the spell checker using command npm ci Note sudo command may be needed to install globally, depending on your system configuration You now have all the tools installed to be able to create the static HTML site from the markdown documents. The documentation for MkDocs provides full instructions for using MkDocs, but the important commands are: mkdocs build will build the static site. This must be run in the root directory of the repo, where mkdocs.yml is located mkdocs serve will build the static site and launch a test server on http://localhost:8000 . Every time a document is modified the website will automatically be updated and any browser open will be refreshed to the latest. To check links in the built site ( mkdocs build must be run first), use the linkchecker, with command linkchecker -f linkcheckerrc --check-extern public . This command should be run in the root folder of the project, containing the linkcheckerrc file. To check spelling cspell docs/**/*.md should be run in the root folder of the project, containing the cspell.json file. There is also a convenience script ./build.sh in the root of the repository that will check spelling, build the site then run the link checker. The site is created using MkDocs with the Materials theme theme. MkDocs takes Markdown documents and turns them into a static website that can be accessed from a filesystem or served from a web server. A link checker tool is also used to validate all links in the MkDocs generated website.","title":"Setting up a documentation environment"},{"location":"wg_docs/okd-io/#updating-the-site","text":"To make your changes live, create a pull request to deliver the changes in your fork of the repo to the main branch of the okd.io repo. Github automation is used to generate the site then publish to github pages, which serves the site. If your changes contain spelling issues or broken links, then the automation will fail and the github pages site will not be updated, so please do a local test using the build.sh script before creating the pull request.","title":"Updating the site"},{"location":"wg_docs/okd-io/#changing-content","text":"MkDocs supports standard Markdown syntax and a set Markdown extensions provided by plugins. The exact Markdown syntax supported is based on the python implementation . MkDocs is configured using the mkdocs.yml file in the root of the git repository. The mkdoc.yml file defines the top level navigation for the site. The level of indentation is configurable (this requires the theme to support this feature) with Markdown headings, levels 2 ( ## ) and 3 ( ### ) being used for the in-page navigation on the right of the page.","title":"Changing content"},{"location":"wg_docs/okd-io/#standard-markdown-features","text":"The following markdown syntax is used within the documentation Syntax Result # Title heading - you can create up to 6 levels of headings by adding additional # characters, so ### is a level 3 heading **text** will display the word text in bold *text* will display the word text in italic `code` inline code block ```shell ... ``` multi-line (Fenced) code block 1. list item ordered list - unordered list item unordered list --- horizontal break HTML can be embedded in Markdown, but in the documentation it is preferred to stick with pure Markdown with the installed extensions.","title":"Standard Markdown features"},{"location":"wg_docs/okd-io/#indentation","text":"MkDocs uses 4 spaces for tabs, so when indenting code ensure you are working with tabs set to 4 spaces rather than 2, which is commonly used. When using some features of Markdown indentation is used to identify blocks. 1. Ubiquity EdgeRouter ER-X - runs DHCP (embedded), custom DNS server via AdGuard ![ pic ]( ./img/erx.jpg ){width=80%} In the code block above you will see the unordered list item is indented, so it aligns with the content of the ordered list (rather than aligning with the number of the ordered list). The image is also indented so it too aligns with the ordered list text. Many of the Markdown elements can be nested and indentation is used to define the nesting relationship. If you look down on this page at the Information boxes section, the example shows an example of nesting elements and the Markdown tab shows how indentation is being used to identify the nesting relationships.","title":"Indentation"},{"location":"wg_docs/okd-io/#links-within-mkdocs-generated-content","text":"MkDocs will warn of any internal broken links, so it is important that links within the documentation are recognized as internal links. a link starting with a protocol name, such as http or https, is an external link a link starting with / is an external link. This is because MkDocs generated content can be embedded into another web application, so links can point outside of the MkDocs generated site but hosted on the same website a link starting with a file name (including the .md extension) or relative directory (../directory/filename.md) is an internal link and will be verified by MkDocs Information Internal links should be to the Markdown file (with .md extension). When the site is generated the filename will be automatically converted to the correct URL As part of the build process a linkchecker application will check the generated html site for any broken links. You can run this linkchecker locally using the instructions. If any links in the documentation should be excluded from the link checker, such as links to localhost, then they should be added as a regex to the linkcheckerrc file, located in the root folder of the project - see linkchecker documentation for additional information","title":"Links within MkDocs generated content"},{"location":"wg_docs/okd-io/#markdown-extensions-used-in-okdio","text":"There are a number of Markdown extensions being used to create the site. See the mkdocs.yml file to see which extensions are configured. The documentation for the extensions can be found here","title":"Markdown Extensions used in OKD.io"},{"location":"wg_docs/okd-io/#link-configuration","text":"Links on the page or embedded images can be annotated to control the links and also the appearance of the links:","title":"Link configuration"},{"location":"wg_docs/okd-io/#image","text":"Images are embedded in a page using the standard Markdown syntax ![description](URL) , but the image can be formatted with Attribute Lists . This is most commonly used to scale an image or center an image, e.g. ![ GitHub repo url ]( images/github-repo-url.png ){style=\"width: 80%\" .center }","title":"Image"},{"location":"wg_docs/okd-io/#external-links","text":"External links can also use attribute lists to control behaviors, such as open in new tab or add a css class attribute to the generated HTML, such as external in the example below: [ MkDocs ]( http://mkdocs.org ){: target=\"_blank\" .external } Info You can embed an image as the description of a link to create clickable images that launch to another site: [![Image description](Image URL)](target URL \"hover text\"){: target=_blank}","title":"External Links"},{"location":"wg_docs/okd-io/#youtube-videos","text":"It is not possible to embed a YouTube video and have it play in place using pure markdown. You can use HTML within the markdown file to embed a video: < iframe width = \"100%\" height = \"500\" src = \"https://www.youtube.com/watch?v=qh1zYW7BLxE&t=431s\" title = \"Building an OKD 4 Home Lab with special guest Craig Robinson\" frameborder = \"0\" allow = \"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen ></ iframe >","title":"YouTube videos"},{"location":"wg_docs/okd-io/#tabs","text":"Content can be organized into a set of horizontal tabs. === \"Tab 1\" Hello === \"Tab 2\" World produces : Tab 1 Hello Tab 2 World","title":"Tabs"},{"location":"wg_docs/okd-io/#information-boxes","text":"The Admonition extension allows you to add themed information boxes using the !!! and ??? syntax: !!! note This is a note produces: Note This is a note and ??? note This is a note You can add a `+` character to force the box to be initially open `???+` produces a collapsible box: Note This is a collapsible note You can add a + character to force the box to be initially open ???+ You can override the title of the box by providing a title after the Admonition type. Example You can also nest different components as required note Note This is a note collapsible note Note This is a note custom title note Sample Title This is a note Markdown !!!Example You can also nest different components as required === \"note\" !!!note This is a note === \"collapsible note\" ???+note This is a note === \"custom title note\" !!!note \"Sample Title\" This is a note","title":"Information boxes"},{"location":"wg_docs/okd-io/#supported-admonition-classes","text":"The Admonitions supported by the Material theme are : Note This is a note Abstract This is an abstract Info This is an info Tip This is a tip Success This is a success Question This is a question Warning This is a warning Failure This is a failure Danger This is a danger Bug This is a bug Example This is an example Quote This is a quote","title":"Supported Admonition Classes"},{"location":"wg_docs/okd-io/#code-blocks","text":"Code blocks allow you to insert code or blocks of text in line or as a block. To use inline you simply enclose the text using a single back quote ` character. So a command can be included using `oc get pods` and will create oc get pods When you want to include a block of code you use a fence , which is 3 back quote character at the start and end of the block. After the opening quotes you should also specify the content type contained in the block. ```shell oc get pods ``` which will produce: oc get pods Notice that the block automatically gets the copy to clipboard link to allow easy copy and paste. Every code block needs to identify the content. Where there is no content type, then text should be used to identify the content as plain text. Some of the common content types are shown in the table below. However, a full link of supported content types can be found here , where the short name in the documentation should be used. type Content shell Shell script content powershell Windows Power Shell content bat Windows batch file (.bat or .cmd files) json JSON content yaml YAML content markdown or md Markdown content java Java programming language javascript or js JavaScript programming language typescript or ts TypeScript programming language text Plain text content","title":"Code blocks"},{"location":"wg_docs/okd-io/#advanced-highlighting-of-code-blocks","text":"There are some additional features available due to the highlight plugin installed in MkDocs. Full details can be found in the MkDocs Materials documentation .","title":"Advanced highlighting of code blocks"},{"location":"wg_docs/okd-io/#line-numbers","text":"You can add line numbers to a code block with the linenums directive. You must specify the starting line number, 1 in the example below: ``` javascript linenums=\"1\" <script> document.getElementById(\"demo\").innerHTML = \"My First JavaScript\"; </script> ``` creates 1 2 3 < script > document . getElementById ( \"demo\" ). innerHTML = \"My First JavaScript\" ; < /script> Info The line numbers do not get included when the copy to clipboard link is selected","title":"Line numbers"},{"location":"wg_docs/okd-io/#spell-checking","text":"This project uses cSpell to check spelling within the markdown. The configuration included in the project automatically excludes content in a code block, enclosed in triple back quotes ```. The configuration file also specifies that US English is the language used in the documentation, so only US English spellings should be used for words where alternate international English spellings exist. You can add words to be considered valid either within a markdown document or within the cspell configuration file, cspell.json , in the root folder of the documentation repository. Words defined within a page only apply to that page, but words added to the configuration file apply to the entire project.","title":"Spell checking"},{"location":"wg_docs/okd-io/#adding-local-words","text":"You can add a list of words to be considered valid for spell checking purposes as a comment in a Markdown file. The comment has a specific format to be picked up by the cSpell tool: <!--- cSpell:ignore linkchecker linkcheckerrc mkdocs mkdoc --> here the words linkchecker , linkcheckerrc , mkdocs and mkdoc are specified as words to be accepted by the spell checker within the file containing the comment.","title":"Adding local words"},{"location":"wg_docs/okd-io/#adding-global-words","text":"The cSpell configuration file cspell.json contains a list of words that should always be considered valid when spell checking. The list of words applies to all files being checked.","title":"Adding global words"},{"location":"wg_docs/overview/","text":"Documentation Subgroup \u00b6 The Documentation working group is responsible for improving the OKD documentation. Both the community documentation (this site) and the product documentation. Joining the group \u00b6 The Documentation Subgroup is open to all. You don't need to be invited to join, just attend on of the bi-weekly video calls: Calendar link : OKD working group calendar Agenda link : Documentation working group agenda and meeting nodes Product Documentation \u00b6 The OKD product documentation is maintained in the same git repository as Red Hat OpenShift product documentation, as they are sibling projects and largely share the same source code. The process for making changes to the documentation is outlined in the documentation section Community Documentation \u00b6 This site is the community documentation. It is hosted on github and uses a static site generator to convert the Markdown documents in the git repo into this website. Details of how to modify the site content is contained on the page Modifying OKD.io .","title":"Overview"},{"location":"wg_docs/overview/#documentation-subgroup","text":"The Documentation working group is responsible for improving the OKD documentation. Both the community documentation (this site) and the product documentation.","title":"Documentation Subgroup"},{"location":"wg_docs/overview/#joining-the-group","text":"The Documentation Subgroup is open to all. You don't need to be invited to join, just attend on of the bi-weekly video calls: Calendar link : OKD working group calendar Agenda link : Documentation working group agenda and meeting nodes","title":"Joining the group"},{"location":"wg_docs/overview/#product-documentation","text":"The OKD product documentation is maintained in the same git repository as Red Hat OpenShift product documentation, as they are sibling projects and largely share the same source code. The process for making changes to the documentation is outlined in the documentation section","title":"Product Documentation"},{"location":"wg_docs/overview/#community-documentation","text":"This site is the community documentation. It is hosted on github and uses a static site generator to convert the Markdown documents in the git repo into this website. Details of how to modify the site content is contained on the page Modifying OKD.io .","title":"Community Documentation"},{"location":"wg_virt/overview/","text":"OKD Virtualization Subgroup \u00b6 Todo Move content from https://okd-virtualization.github.io to here?","title":"Overview"},{"location":"wg_virt/overview/#okd-virtualization-subgroup","text":"Todo Move content from https://okd-virtualization.github.io to here?","title":"OKD Virtualization Subgroup"}]}